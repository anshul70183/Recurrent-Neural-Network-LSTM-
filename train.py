# -*- coding: utf-8 -*-
"""RNN_NEW_NEW_NEW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7Hp6GMDAer2n9ZNQ_GcSgRMUz3wYvOo
"""

import re
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from collections import OrderedDict

from itertools import chain
import collections
from tensorflow.contrib import rnn

from google.colab import drive
drive.mount('/content/drive')

"""building encoder"""

lr = 0.001
batch_size =128
init = 1
attention = 0
dropout_prob = 0.2
decode_method=1
# beam_width = 10
save_dir = 'drive/My Drive/data_PA3/'
num_epochs = 15
train_path = 'drive/My Drive/data_PA3/train.csv'
valid_path = 'drive/My Drive/data_PA3/valid.csv'
test_path = 'drive/My Drive/data_PA3/partial_test_400.csv'

train = pd.read_csv(train_path)
valid= pd.read_csv(valid_path)
test = pd.read_csv(test_path)

inembsize = 256
outembsize = 256
num_batches = len(train)//batch_size
encoder_layers = 1
decoder_layers = 2
state_size = 512

train_eng = train['ENG']
train_hin = train["HIN"]

valid_eng = valid["ENG"]
valid_hin = valid["HIN"]
test_eng = test['ENG']

def one_hot(batch_word, max_length, word_to_idx):
    vec_word = np.zeros((len(batch_word), max_length, len(word_to_idx)), dtype=float)
    for i, word in enumerate(batch_word):
        for j, char in enumerate(word):
            vec_word[i, j, char] = 1.
    return vec_word

def generate_text(prediction, batch_size, length, vocab_size, idx_to_word):

    batch_softmax = np.reshape(prediction, [batch_size, length, vocab_size])
    batch_word = []

    for word in batch_softmax:
        new_word = ''
        for char in word:
            vector_position = np.argmax(char)
            y_word = idx_to_word[vector_position]
            if y_word != 'PAD':
                new_word = new_word + y_word + ' '
            else:
                new_word = new_word + ''
        batch_word.append(new_word)

    return batch_word

def load_data(train_eng, train_hin):
    
    # Feed input data in backwards for better translation performance.
    x_data = [(tf.compat.as_str(train_eng[_]).lower().split(' ')) for _ in range(len(train_eng))]
    x_data = [list(x_data[_]) for _ in range(len(x_data))]
    X_DATA_SIZE = len(x_data)

    y_data = [(tf.compat.as_str(train_hin[_]).lower().split(' ')) for _ in range(len(train_hin))]
    y_data = [list(y_data[_]) for _ in range(len(y_data))]
    Y_DATA_SIZE = len(y_data)


    x_word_list = []
    y_word_list = []
    [x_word_list.extend(x_data[_]) for _ in range(len(x_data))]
    [y_word_list.extend(y_data[_]) for _ in range(len(y_data))]

    x_word_dictionary = []
    y_word_dictionary = []
    x_word_dictionary.extend(collections.Counter(x_word_list).most_common(44))
    y_word_dictionary.extend(collections.Counter(y_word_list).most_common(84))
    

    x_idx_to_word = [word[0] for idx, word in enumerate(x_word_dictionary)]
    x_idx_to_word.insert(0, 'GO')
    x_idx_to_word.insert(1, 'EOS')
    x_idx_to_word.insert(2, 'PAD')
    x_idx_to_word.insert(3, 'UK')


    y_idx_to_word = [word[0] for idx, word in enumerate(y_word_dictionary)]
    y_idx_to_word.insert(0, 'GO')
    y_idx_to_word.insert(1, 'EOS')
    y_idx_to_word.insert(2, 'PAD')
    y_idx_to_word.insert(3, 'UK')

    x_word_to_idx = {word:ix for ix, word in enumerate(x_idx_to_word)}
    y_word_to_idx = {word: ix for ix, word in enumerate(y_idx_to_word)}

    X_VOCAB_SIZE = len(x_word_dictionary) + 4
    Y_VOCAB_SIZE = len(y_word_dictionary) + 4

    X_MAX_LENGTH = max([len(x_data[_]) for _ in range(len(x_data))])
    Y_MAX_LENGTH = max([len(y_data[_]) for _ in range(len(y_data))])

    return X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE

def load_data_test(train_eng):
    
    # Feed input data in backwards for better translation performance.
    x_data = [(tf.compat.as_str(train_eng[_]).lower().split(' ')) for _ in range(len(train_eng))]
    x_data = [list(x_data[_]) for _ in range(len(x_data))]
    X_DATA_SIZE = len(x_data)

#     y_data = [(tf.compat.as_str(train_hin[_]).lower().split(' ')) for _ in range(len(train_hin))]
#     y_data = [list(y_data[_]) for _ in range(len(y_data))]
#     Y_DATA_SIZE = len(y_data)


    x_word_list = []
#     y_word_list = []
    [x_word_list.extend(x_data[_]) for _ in range(len(x_data))]
#     [y_word_list.extend(y_data[_]) for _ in range(len(y_data))]

    x_word_dictionary = []
#     y_word_dictionary = []
    x_word_dictionary.extend(collections.Counter(x_word_list).most_common(44))
#     y_word_dictionary.extend(collections.Counter(y_word_list).most_common(84))
    

    x_idx_to_word = [word[0] for idx, word in enumerate(x_word_dictionary)]
    x_idx_to_word.insert(0, 'GO')
    x_idx_to_word.insert(1, 'EOS')
    x_idx_to_word.insert(2, 'PAD')
    x_idx_to_word.insert(3, 'UK')


#     y_idx_to_word = [word[0] for idx, word in enumerate(y_word_dictionary)]
#     y_idx_to_word.insert(0, 'GO')
#     y_idx_to_word.insert(1, 'EOS')
#     y_idx_to_word.insert(2, 'PAD')
#     y_idx_to_word.insert(3, 'UK')

    x_word_to_idx = {word:ix for ix, word in enumerate(x_idx_to_word)}
#     y_word_to_idx = {word: ix for ix, word in enumerate(y_idx_to_word)}

#     X_VOCAB_SIZE = len(x_word_dictionary) + 4
#     Y_VOCAB_SIZE = len(y_word_dictionary) + 4

    X_MAX_LENGTH = max([len(x_data[_]) for _ in range(len(x_data))])
#     Y_MAX_LENGTH = max([len(y_data[_]) for _ in range(len(y_data))])

    return  x_idx_to_word, x_word_to_idx, X_MAX_LENGTH,  X_DATA_SIZE

X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE = load_data(train_eng, train_hin)                                                                                                                   

X_VOCAB_SIZE_VALID, Y_VOCAB_SIZE_VALID, x_idx_to_word_VALID, x_word_to_idx_VALID, y_idx_to_word_VALID, y_word_to_idx_VALID, X_MAX_LENGTH_VALID, Y_MAX_LENGTH_VALID, X_DATA_SIZE_VALID = load_data(valid_eng, valid_hin)                                                                                                                   

x_word_to_idx_test, x_word_to_idx_test, X_MAX_LENGTH_TEST, X_DATA_SIZE_TEST = load_data_test(test_eng)

def get_data(x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin):


    x_data = inputs
    y_data = labels

    x_data = [(tf.compat.as_str(x_data[_]).lower().split(' ')) for _ in range(len(x_data))]
    x_data = [list(x_data[_]) for _ in range(len(x_data))]
       
    y_data = [(tf.compat.as_str(y_data[_]).lower().split(' ')) for _ in range(len(y_data))]
    y_data = [list(y_data[_]) for _ in range(len(y_data))]

    for i, word in enumerate(x_data):
        for j, charac in enumerate(word):
            if charac in x_word_to_idx:
                x_data[i][j] = x_word_to_idx[charac]
            else:
                x_data[i][j] = x_word_to_idx['UK']

    for i, word in enumerate(y_data):
        for j, charac in enumerate(word):
            if charac in y_word_to_idx:
                y_data[i][j] = y_word_to_idx[charac]
            else:
                y_data[i][j] = y_word_to_idx['UK']
                
    return x_data, y_data

def get_data_test(x_word_to_idx, inputs = test_eng):


    x_data = inputs
#     y_data = labels

    x_data = [(tf.compat.as_str(x_data[_]).lower().split(' ')) for _ in range(len(x_data))]
    x_data = [list(x_data[_]) for _ in range(len(x_data))]
       
#     y_data = [(tf.compat.as_str(y_data[_]).lower().split(' ')) for _ in range(len(y_data))]
#     y_data = [list(y_data[_]) for _ in range(len(y_data))]

    for i, word in enumerate(x_data):
        for j, charac in enumerate(word):
            if charac in x_word_to_idx:
                x_data[i][j] = x_word_to_idx[charac]
            else:
                x_data[i][j] = x_word_to_idx['UK']

#     for i, word in enumerate(y_data):
#         for j, charac in enumerate(word):
#             if charac in y_word_to_idx:
#                 y_data[i][j] = y_word_to_idx[charac]
#             else:
#                 y_data[i][j] = y_word_to_idx['UK']
                
    return x_data

x_data , y_data = get_data( x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin)

x_data_val , y_data_val = get_data(x_word_to_idx, y_word_to_idx, inputs = valid_eng, labels = valid_hin)

x_data_test = get_data_test(x_word_to_idx_test)

def encoding(encoder_x):
  
      if (init ==1 ):
          init_f = tf.contrib.layers.xavier_initializer
      else :
          init_f = tf.contrib.layers.variance_scaling_initializer
    
      word_embeddings = tf.get_variable('encoder_word_embeddings', [X_VOCAB_SIZE, inembsize], initializer = init_f(dtype = tf.float32))
      encoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, encoder_x)
      encoder_embedded_word_ids = tf.reshape(encoder_embedded_word_ids, [-1, X_MAX_LENGTH, inembsize])
      
      encoder_cell = tf.nn.rnn_cell.LSTMCell(state_size//2, state_is_tuple=True)
      
      encoder_cell = tf.contrib.rnn.DropoutWrapper(encoder_cell, 
                                            output_keep_prob=keep_prob)
      
#       def attention():
#             attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units = state_size, 
#                                                                     memory = encoder_embedded_word_ids)
          
#             return tf.contrib.seq2seq.AttentionWrapper(cell = encoder_cell, 
#                                                         attention_mechanism = attention_mechanism,
#                                                         attention_layer_size = size_layer//2)
      
      for n in range(encoder_layers): 

                (out_fw, out_bw), encoder_state  = tf.nn.bidirectional_dynamic_rnn(
                cell_fw = encoder_cell,  ## initializer, reuse to be set
                cell_bw = encoder_cell,  ## initializer, reuse to be set
                inputs = encoder_embedded_word_ids,
                dtype = tf.float32)
        
#       print(tf.shape(encoder_out))
#       tf.concat([encoder_out[0],encoder_out[1]], 1)  
#       print(tf.shape(encoder_out))
      
#       tf.concat([encoder_out[0], encoder_out[1]],0)
      
#       print(tf.shape(encoder_out))

#       tf.reshape(encoder_state, [-1, batch_size,Y_MAX_LENGTH, state_size])
      
        
      bi_state_c = tf.concat((encoder_state[0].c, encoder_state[1].c), -1)
      bi_state_h = tf.concat((encoder_state[0].h, encoder_state[1].h), -1)
      bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)
      encoder_state = tuple([bi_lstm_state] * decoder_layers)
      encoder_output = tf.concat((out_fw, out_bw), 2)
      print(out_fw)
      print(out_bw)  
      print(encoder_output)
      print(encoder_state)
      
#       encoder_output = tf.concat([encoder_output,encoder_output],0)     
      return encoder_state, encoder_output

def process_decoder_input(y_data):

    id = y_word_to_idx['GO']
    
    after_slice = tf.strided_slice(y_data, [0, 0], [batch_size, -1], [1, 1])
    y_data_new = tf.concat( [tf.fill([batch_size, 1], id), after_slice], 1)
    
    return y_data_new



def decoding_train(decoder_embedded_word_ids, decoder_cell, encoder_state, encoder_output, dense_layer, train_sequence_length):
  
      decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, 
                                           output_keep_prob=keep_prob)
    
      training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embedded_word_ids, sequence_length = train_sequence_length)

      if attention == 1:
        # Define attention mechanism
#         attention_states = tf.transpose(encoder_output, [1,0, 2])
#         print(encoder_state)
        attn_mech = tf.contrib.seq2seq.LuongAttention(
        num_units = state_size, memory = encoder_output,
           memory_sequence_length= train_sequence_length)
    
#         input()
#     attention_mechanism = tf.contrib.seq2seq.LuongAttention(decSize, attention_states, memory_sequence_length=None)
#             decoder1_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=decSize)
#             initial_state = decoder1_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)
#             decoder1 = tf.contrib.seq2seq.BasicDecoder(decoder1_cell, helper, initial_state, output_layer=projection_layer)
#             decoder1_outputs, decoder1_state, decoder1_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(decoder1)
          
    
        # Define attention cell (Attention wrapper)
        attn_cell = tf.contrib.seq2seq.AttentionWrapper(
        cell = decoder_cell, attention_mechanism = attn_mech, attention_layer_size =state_size,
        alignment_history=True)
        
        #define initial state of the attention layer
        initial_state = attn_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)
        
        #define decoder and feed in the cell as the attention wrapped cell
        training_decoder = tf.contrib.seq2seq.BasicDecoder(
              cell = attn_cell,
              helper = training_helper,
              initial_state = initial_state,
              output_layer = dense_layer)

        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
              decoder = training_decoder,
              impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further
              maximum_iterations = Y_MAX_LENGTH)   
      
        training_logits = training_decoder_output.rnn_output
        training_logits_max = training_decoder_output.sample_id
        
      else:
        training_decoder = tf.contrib.seq2seq.BasicDecoder(
              cell = decoder_cell,
              helper = training_helper,
              initial_state = encoder_state,
              output_layer = dense_layer)

        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
              decoder = training_decoder,
              impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further
              maximum_iterations = Y_MAX_LENGTH)   
      
        training_logits = training_decoder_output.rnn_output
        training_logits_max = training_decoder_output.sample_id
      
      return training_logits, training_logits_max



def decoding_infer(word_embeddings, decoder_cell, encoder_state, encoder_output, dense_layer, train_sequence_length):
          
          decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, 
                                            output_keep_prob=keep_prob)
    
      #decoder for test data
          predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding = word_embeddings,
                  start_tokens = tf.fill([batch_size], y_word_to_idx['GO']), end_token = y_word_to_idx['EOS'])
            
          
          if attention == 1:
            # Define attention mechanism
    #         attention_states = tf.transpose(encoder_output, [1,0, 2])
    #         print(encoder_state)
            attn_mech = tf.contrib.seq2seq.LuongAttention(
            num_units = state_size, memory = encoder_output,
               memory_sequence_length= None)


   
            attn_cell = tf.contrib.seq2seq.AttentionWrapper(
            cell = decoder_cell, attention_mechanism = attn_mech, attention_layer_size =state_size,
            alignment_history=True)

            #define initial state of the attention layer
            initial_state = attn_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)

            #define decoder and feed in the cell as the attention wrapped cell
            predicting_decoder  = tf.contrib.seq2seq.BasicDecoder(
                  cell = attn_cell,
                  helper = predicting_helper,
                  initial_state = initial_state,
                  output_layer = dense_layer)

            predicting_decoder_output , _, _ = tf.contrib.seq2seq.dynamic_decode(
                  decoder = predicting_decoder,
                  impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further
                  maximum_iterations = Y_MAX_LENGTH)   

            predicting_ids = predicting_decoder_output.sample_id

          else:
          
            predicting_decoder = tf.contrib.seq2seq.BasicDecoder(
                    cell = decoder_cell,
                    helper = predicting_helper,
                    initial_state = encoder_state,
                    output_layer = dense_layer)

            predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
                    decoder = predicting_decoder,
                    impute_finished = True,
                    maximum_iterations = Y_MAX_LENGTH)


            predicting_ids = predicting_decoder_output.sample_id
          
          return predicting_ids

def decoding(decoder_x, encoder_state,encoder_output, train_sequence_length):
      word_embeddings = tf.get_variable('decoder_word_embeddings', [Y_VOCAB_SIZE, outembsize])
      decoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, decoder_x)
      decoder_embedded_word_ids = tf.reshape(decoder_embedded_word_ids,  [-1, Y_MAX_LENGTH,  outembsize])
      
      decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(state_size) for _ in range(decoder_layers)])
      dense_layer = tf.layers.Dense(Y_VOCAB_SIZE)
      
      with tf.variable_scope("decode"):
            train_output, train_output_max = decoding_train(decoder_embedded_word_ids, decoder_cell, encoder_state, encoder_output, dense_layer, train_sequence_length)
#             train_output = tf.identity(train_output.rnn_output, name='logits')
          
      
      with tf.variable_scope("decode", reuse=True):       
            infer_output = decoding_infer(word_embeddings, decoder_cell, encoder_state,  encoder_output, dense_layer, train_sequence_length)
#             infer_output = tf.identity(infer_output.sample_id, name='prediction')
        
      return train_output, train_output_max,  infer_output

def get_accuracy(logits, labels, flag):
  
    max_seq = Y_MAX_LENGTH
    if flag == 1 :
        logits = logits[0]
    
    for i in range(len(logits)):
       if len(logits[i])<max_seq:
          logits[i] = np.pad(
            logits[i],
            [(0,0),(0,max_seq - len(logits[i]))],
            'constant')
          
    for i in range(len(labels)):
       if len(labels[i])<max_seq:
          labels[i] = np.pad(
            labels[i],
            [(0,0),(0,max_seq - len(labels[i]))],
            'constant')
   
        
    return np.mean(np.sum((logits==labels),axis=1) == np.ones(batch_size)*max_seq)

def pad(sentence_batch, pad_int, max_sentence):
    
    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]

def get_batches(x_data, y_data, batch_size, x_pad_int, y_pad_int):
    """Batch targets, sources, and the lengths of their sentences together"""
    for batch_i in range(0, len(x_data)//batch_size):
        start_i = batch_i * batch_size

        # Slice the right amount for the batch
        x_data_batch = x_data[start_i:start_i + batch_size]
        y_data_batch = y_data[start_i:start_i + batch_size]
        
       
        # Pad
        pad_x_batch = np.array(pad(x_data_batch, x_pad_int, X_MAX_LENGTH))
        pad_y_batch = np.array(pad(y_data_batch, y_pad_int, Y_MAX_LENGTH))

        # Need the lengths for the _lengths parameters
        pad_y_lengths = []
        for y in pad_y_batch:
            pad_y_lengths.append(len(y))

        pad_x_lengths = []
        for x in pad_x_batch:
            pad_x_lengths.append(len(x))

        yield pad_x_batch, pad_y_batch, pad_x_lengths, pad_y_lengths

Y_MAX_LENGTH

tf_graph = tf.Graph()
with tf_graph.as_default():
        
#       input_data = loaded_graph.get_tensor_by_name('input:0')
#       logits = loaded_graph.get_tensor_by_name('predictions:0')
#       target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')
#       keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')  
      
      encoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None], name = 'input') 
      decoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None], name = 'target') 
      
      y_sequence_length =  tf.placeholder(dtype=tf.int32, shape=[None], name = 'y_sequence_length') 
      keep_prob = tf.placeholder(tf.float32, name='keep_prob')
      
      encoder_state, encoder_output =  encoding(encoder_x)      
      _decoder_x  = process_decoder_input(decoder_x)
      print(_decoder_x)
      train_logits,train_logits_max,infer_output = decoding(_decoder_x, encoder_state, encoder_output, y_sequence_length)
      
     
      masks = tf.sequence_mask(y_sequence_length, Y_MAX_LENGTH, dtype=tf.float32)
      
      softmax_out = tf.nn.softmax(train_logits)
      
      #cost =  tf.nn.softmax_cross_entropy_with_logits(logits=train_logits, labels=y)
      cost = tf.contrib.seq2seq.sequence_loss(
            train_logits,
            decoder_x,
            masks)
      

      total_cost = tf.reduce_mean(cost)
      optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost)

train_loss = []
val_loss = []
train_accuracy = []
val_accuracy = []

with tf.Session(graph=tf_graph) as sess:
    
    merged = tf.summary.merge_all()
    saver = tf.train.Saver(max_to_keep=num_epochs, save_relative_paths=True)
    
    batch_x_val, batch_y_val, batch_x_sequence_length_val, batch_y_sequence_length_val = next(get_batches(x_data_val, y_data_val, batch_size, 2,2))
 
    sess.run(tf.global_variables_initializer())

    epoch = 0
   
#     print(process_decoder_input(batch_y))
    

    while epoch< num_epochs: 
        epoch_loss = 0
        epoch_loss_val =0
        epoch_acc_agg =0
        epoch_acc_agg_val =0
        n_steps=0
        
        for batch, (batch_x, batch_y, batch_x_sequence_length, batch_y_sequence_length) in enumerate(get_batches(x_data, y_data, batch_size, 2,2)):
                
            opt = sess.run(optimizer, feed_dict={encoder_x:batch_x,  decoder_x:batch_y,   y_sequence_length: batch_y_sequence_length, keep_prob : (1- dropout_prob) })
            batch_loss  = sess.run([total_cost], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,   y_sequence_length: batch_y_sequence_length,  keep_prob : 1- dropout_prob })     
            logits, logits_max = sess.run([train_logits, train_logits_max], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,   y_sequence_length: batch_y_sequence_length,  keep_prob : 1- dropout_prob  })    
            #_logits = np.argmax(logits, axis=2)
            accuracy = get_accuracy(logits_max, batch_y,0)
            #print(accuracy)
                            
                
            #opt = sess.run(optimizer, feed_dict={encoder_x: x_data_valid, decoder_x: y_data_valid, y: y_one_hot_valid})
            batch_loss_val = sess.run([total_cost], feed_dict={encoder_x:  batch_x_val, decoder_x:  batch_y_val, y_sequence_length: batch_y_sequence_length_val, keep_prob :1.0 })     
            logits_val = sess.run([infer_output], feed_dict={encoder_x:  batch_x_val, decoder_x:  batch_y_val, y_sequence_length: batch_y_sequence_length_val, keep_prob :1.0 })
            #print(np.shape(logits_val))
            accuracy_val = get_accuracy(logits_val, batch_y_val, 1)
            
           
            
            if batch % 10 == 0:
    
                save_output_path = save_dir + 'TRAINING_TRANSLATION_OUTPUT_EPOCH_{}_STATE-SIZE_{}_NUM-LAYERS_{}_LEARNING-RATE{}_EMBEDDING-SIZE_{}.txt'.format(epoch, state_size, decoder_layers, lr, inembsize)
                text = generate_text(logits, batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE, y_idx_to_word)
                for _ in range(batch_size):
                    print(text[_])
                print('Epoch {} Batch {}/{} Training Loss: {}, Training Accuracy: {} '.format(epoch+1, batch+1, num_batches, batch_loss[0], accuracy))
                print('Epoch {} Batch {}/{} Validation Loss: {}, Validation Accuracy: {} '.format(epoch+1, batch+1, num_batches, batch_loss_val[0], accuracy_val))
      
            
            epoch_loss += batch_loss[0]
            epoch_loss_val += batch_loss_val[0]
            epoch_acc_agg += accuracy
            epoch_acc_agg_val += accuracy_val
            n_steps+=1
#             break    
                
        train_loss.append(epoch_loss)   
        val_loss.append(epoch_loss_val)
        train_accuracy.append(epoch_acc_agg)
        val_accuracy.append(epoch_acc_agg_val)
        
        print('Epoch {} Training Loss: {}, Training Accuracy: {} '.format(epoch+1, epoch_loss, epoch_acc_agg/n_steps))
        print('Epoch {} Validation Loss: {}, Validation Accuracy: {} '.format(epoch+1, epoch_loss_val, epoch_acc_agg_val/n_steps))
            
        path = saver.save(sess, save_dir + 'dlpa3', epoch)
    
        print("Saved in: %s" % path)
        epoch+=1
        
        
    #prediction
    
    #prediction
    pred_text = []
    temp_vec = [6]

    for i in range(batch_size - (len(x_data_test)%batch_size)):
        x_data_test.append(temp_vec)


    for batch, (batch_x, batch_x_sequence_length) in enumerate(get_batches_test(x_data_test,2)):
 

        pred_logits = sess.run([infer_output], feed_dict={encoder_x: batch_x, y_sequence_length: batch_x_sequence_length,  keep_prob : 1- dropout_prob  })    
        pred_logits = pred_logits[0]
        

        pred_vec_one_hot = one_hot(pred_logits, Y_MAX_LENGTH, y_word_to_idx)
        text = generate_text_test(pred_vec_one_hot, Y_MAX_LENGTH, Y_VOCAB_SIZE, y_idx_to_word, batch_size)
        pred_text.append(text)
        print(text)
    
    
    pred_test = np.array(pred_text)
    pred_test = pred_test.reshape(-1)
#     pred_test = np.transpose(pred_test)
    pred_test = pred_test[:1000]
    print(pred_test.shape)
    prediction_df = pd.DataFrame(pred_test)
#     pred_vec = np.array(pred_vec)
    
    
#     prediction_df.index.names = ['id']
#     prediction_df.columns = ['HIN']
    prediction_df.to_csv(save_dir + 'prediction_1.csv')