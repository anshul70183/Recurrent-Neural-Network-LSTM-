{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_NEW_NEW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "L9ixfY61Bege",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "imXc7RCu-at0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from collections import OrderedDict\n",
        "from itertools import chain\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3Uyb4HA-qKs",
        "colab_type": "code",
        "outputId": "13cf9a08-f866-4df5-9259-66748c9ffb47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EPkld2qwnGP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "building encoder\n"
      ]
    },
    {
      "metadata": {
        "id": "GW0tXdQqntUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "batch_size = 128\n",
        "init = 1\n",
        "dropout_prob= 0\n",
        "decode_method=1\n",
        "beam_width = 10\n",
        "save_dir = 'drive/My Drive/data_PA3/'\n",
        "num_epochs = 10\n",
        "train_path = 'drive/My Drive/data_PA3/train.csv'\n",
        "valid_path = 'drive/My Drive/data_PA3/valid.csv'\n",
        "test_path = 'drive/My Drive/data_PA3/partial_test_400.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qiGRxGP-r_R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(train_path)\n",
        "valid= pd.read_csv(valid_path)\n",
        "test = pd.read_csv(test_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zAg2bE-P4dXH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inembsize = 256\n",
        "outembsize = 256\n",
        "\n",
        "num_batches = len(train)//batch_size\n",
        "encoder_layers = 1\n",
        "decoder_layers = 2\n",
        "state_size = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GSxI6R1Zdc2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_eng = train['ENG']\n",
        "train_hin = train[\"HIN\"]\n",
        "\n",
        "valid_eng = valid[\"ENG\"]\n",
        "valid_hin = valid[\"HIN\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtmkDkmzyWoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot(batch_word, max_length, word_to_idx):\n",
        "    vec_word = np.zeros((len(batch_word), max_length, len(word_to_idx)), dtype=float)\n",
        "    for i, word in enumerate(batch_word):\n",
        "        for j, char in enumerate(word):\n",
        "            vec_word[i, j, char] = 1.\n",
        "    return vec_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SeZ4DeG9yaf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(prediction, batch_size, length, vocab_size, idx_to_word):\n",
        "\n",
        "    batch_softmax = np.reshape(prediction, [batch_size, length, vocab_size])\n",
        "    batch_word = []\n",
        "\n",
        "    for word in batch_softmax:\n",
        "        new_word = ''\n",
        "        for char in word:\n",
        "            vector_position = np.argmax(char)\n",
        "            y_word = idx_to_word[vector_position]\n",
        "            if y_word != '#':\n",
        "                new_word = new_word + y_word + ' '\n",
        "            else:\n",
        "                new_word = new_word + ''\n",
        "        batch_word.append(new_word)\n",
        "\n",
        "    return batch_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HvqFl_c8Buw2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## To remove later\n",
        "\n",
        "# def variable_summaries(var):\n",
        "#   \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "#   with tf.name_scope('summaries'):\n",
        "#     mean = tf.reduce_mean(var)\n",
        "#     tf.summary.scalar('mean', mean)\n",
        "#     with tf.name_scope('stddev'):\n",
        "#       stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "#     tf.summary.scalar('stddev', stddev)\n",
        "#     tf.summary.scalar('max', tf.reduce_max(var))\n",
        "#     tf.summary.scalar('min', tf.reduce_min(var))\n",
        "#     tf.summary.histogram('histogram', var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bn9keAIChKrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(train_eng, train_hin):\n",
        "    \n",
        "    # Feed input data in backwards for better translation performance.\n",
        "    x_data = [(tf.compat.as_str(train_eng[_]).lower().split(' ')) for _ in range(len(train_eng))]\n",
        "    x_data = [list(x_data[_]) for _ in range(len(x_data))]\n",
        "    X_DATA_SIZE = len(x_data)\n",
        "\n",
        "    y_data = [(tf.compat.as_str(train_hin[_]).lower().split(' ')) for _ in range(len(train_hin))]\n",
        "    y_data = [list(y_data[_]) for _ in range(len(y_data))]\n",
        "    Y_DATA_SIZE = len(y_data)\n",
        "\n",
        "\n",
        "    x_word_list = []\n",
        "    y_word_list = []\n",
        "    [x_word_list.extend(x_data[_]) for _ in range(len(x_data))]\n",
        "    [y_word_list.extend(y_data[_]) for _ in range(len(y_data))]\n",
        "\n",
        "    x_word_dictionary = []\n",
        "    y_word_dictionary = []\n",
        "    x_word_dictionary.extend(collections.Counter(x_word_list).most_common(44))\n",
        "    y_word_dictionary.extend(collections.Counter(y_word_list).most_common(84))\n",
        "    \n",
        "\n",
        "    x_idx_to_word = [word[0] for idx, word in enumerate(x_word_dictionary)]\n",
        "    x_idx_to_word.insert(0, 'GO')\n",
        "    x_idx_to_word.insert(1, 'EOS')\n",
        "    x_idx_to_word.insert(2, 'PAD')\n",
        "    x_idx_to_word.insert(3, 'UK')\n",
        "\n",
        "\n",
        "    y_idx_to_word = [word[0] for idx, word in enumerate(y_word_dictionary)]\n",
        "    y_idx_to_word.insert(0, 'GO')\n",
        "    y_idx_to_word.insert(1, 'EOS')\n",
        "    y_idx_to_word.insert(2, 'PAD')\n",
        "    y_idx_to_word.insert(3, 'UK')\n",
        "\n",
        "    x_word_to_idx = {word:ix for ix, word in enumerate(x_idx_to_word)}\n",
        "    y_word_to_idx = {word: ix for ix, word in enumerate(y_idx_to_word)}\n",
        "\n",
        "    X_VOCAB_SIZE = len(x_word_dictionary) + 4\n",
        "    Y_VOCAB_SIZE = len(y_word_dictionary) + 4\n",
        "\n",
        "    X_MAX_LENGTH = max([len(x_data[_]) for _ in range(len(x_data))])\n",
        "    Y_MAX_LENGTH = max([len(y_data[_]) for _ in range(len(y_data))])\n",
        "\n",
        "    return X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOPM5vzBQtTF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE = load_data(train_eng, train_hin)                                                                                                                   \n",
        "\n",
        "X_VOCAB_SIZE_VALID, Y_VOCAB_SIZE_VALID, x_idx_to_word_VALID, x_word_to_idx_VALID, y_idx_to_word_VALID, y_word_to_idx_VALID, X_MAX_LENGTH_VALID, Y_MAX_LENGTH_VALID, X_DATA_SIZE_VALID = load_data(valid_eng, valid_hin)                                                                                                                   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpmDEaRYxFcZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin):\n",
        "\n",
        "\n",
        "    x_data = inputs\n",
        "    y_data = labels\n",
        "\n",
        "    x_data = [(tf.compat.as_str(x_data[_]).lower().split(' ')) for _ in range(len(x_data))]\n",
        "    x_data = [list(x_data[_]) for _ in range(len(x_data))]\n",
        "       \n",
        "    y_data = [(tf.compat.as_str(y_data[_]).lower().split(' ')) for _ in range(len(y_data))]\n",
        "    y_data = [list(y_data[_]) for _ in range(len(y_data))]\n",
        "\n",
        "    for i, word in enumerate(x_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in x_word_to_idx:\n",
        "                x_data[i][j] = x_word_to_idx[charac]\n",
        "            else:\n",
        "                x_data[i][j] = x_word_to_idx['pad']\n",
        "\n",
        "    for i, word in enumerate(y_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in y_word_to_idx:\n",
        "                y_data[i][j] = y_word_to_idx[charac]\n",
        "            else:\n",
        "                y_data[i][j] = y_word_to_idx['pad']\n",
        "                \n",
        "    for _ in range(len(x_data)):\n",
        "      x_length = len(x_data[_])\n",
        "      y_length = len(y_data[_])\n",
        "\n",
        "      #x_data[_].extend(np.zeros([X_MAX_LENGTH - x_length], dtype=int))\n",
        "      #y_data[_].extend(np.zeros([Y_MAX_LENGTH - y_length], dtype=int))\n",
        "\n",
        "    return x_data, y_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZFb7VoMHoI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data , y_data = get_data( x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin)                                              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFNbGZXhgJ-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data_val , y_data_val = get_data(x_word_to_idx, y_word_to_idx, inputs = valid_eng, labels = valid_hin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a13EGvTatGim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding(encoder_x):\n",
        "  \n",
        "      word_embeddings = tf.get_variable('encoder_word_embeddings', [X_VOCAB_SIZE, inembsize])\n",
        "      encoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, encoder_x)\n",
        "      encoder_embedded_word_ids = tf.reshape(encoder_embedded_word_ids, [-1, X_MAX_LENGTH, inembsize])\n",
        "      \n",
        "      for n in range(encoder_layers): \n",
        "                encoder_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "                encoder_out, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = encoder_cell,  ## initializer, reuse to be set\n",
        "                cell_bw = encoder_cell,  ## initializer, reuse to be set\n",
        "                inputs = encoder_embedded_word_ids,\n",
        "                dtype = tf.float32)\n",
        "            \n",
        "      return encoder_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rqsPIfaLuq4G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_train(decoder_embedded_word_ids, decoder_cell, encoder_state, dense_layer, train_sequence_length):\n",
        "  \n",
        "      training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embedded_word_ids, sequence_length = train_sequence_length)\n",
        "\n",
        "      training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "              cell = decoder_cell,\n",
        "              helper = training_helper,\n",
        "              initial_state = encoder_state,\n",
        "              output_layer = dense_layer)\n",
        "\n",
        "      training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "              decoder = training_decoder,\n",
        "              impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further\n",
        "              maximum_iterations = Y_MAX_LENGTH)   \n",
        "      \n",
        "      training_logits = training_decoder_output.rnn_output\n",
        "      \n",
        "      return training_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jooQgWLFvtTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_infer(word_embeddings, decoder_cell, encoder_state, dense_layer):\n",
        "     \n",
        "      #decoder for test data\n",
        "          predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding = word_embeddings,\n",
        "                  start_tokens = tf.fill([batch_size], y_word_to_idx['GO']), end_token = y_word_to_idx['EOS'])\n",
        "\n",
        "          predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                  cell = decoder_cell,\n",
        "                  helper = predicting_helper,\n",
        "                  initial_state = encoder_state,\n",
        "                  output_layer = dense_layer)\n",
        "\n",
        "          predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                  decoder = predicting_decoder,\n",
        "                  impute_finished = True,\n",
        "                  maximum_iterations = 2 * (X_MAX_LENGTH))\n",
        "\n",
        "          predicting_ids = predicting_decoder_output.sample_id\n",
        "          \n",
        "          return predicting_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zt7y4-GYv-hL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding(decoder_x, encoder_state, train_sequence_length):\n",
        "      word_embeddings = tf.get_variable('decoder_word_embeddings', [Y_VOCAB_SIZE, outembsize])\n",
        "      decoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, decoder_x)\n",
        "      decoder_embedded_word_ids = tf.reshape(decoder_embedded_word_ids,  [-1, Y_MAX_LENGTH,  outembsize])\n",
        "      \n",
        "      decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(state_size) for _ in range(decoder_layers)])\n",
        "      dense_layer = tf.layers.Dense(Y_VOCAB_SIZE)\n",
        "      \n",
        "      with tf.variable_scope(\"decode\"):\n",
        "            train_output = decoding_train(decoder_embedded_word_ids, decoder_cell, encoder_state, dense_layer, train_sequence_length)\n",
        "          \n",
        "      \n",
        "      with tf.variable_scope(\"decode\", reuse=True):       \n",
        "            infer_output = decoding_infer(word_embeddings, decoder_cell, encoder_state, dense_layer)\n",
        "        \n",
        "      return train_output, infer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mjDQz_ZQodIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(logits, labels):\n",
        "  \n",
        "    max_seq = Y_MAX_LENGTH\n",
        "    \n",
        "    for i in range(len(logits)):\n",
        "       if len(logits[i])<max_seq:\n",
        "          logits[i] = np.pad(\n",
        "            logits[i],\n",
        "            [(0,0),(0,max_seq - len(logits[i]))],\n",
        "            'constant')\n",
        "          \n",
        "    for i in range(len(labels)):\n",
        "       if len(labels[i])<max_seq:\n",
        "          labels[i] = np.pad(\n",
        "            labels[i],\n",
        "            [(0,0),(0,max_seq - len(labels[i]))],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(logits, labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T99fng0AzbnE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf_graph = tf.Graph()\n",
        "with tf_graph.as_default():\n",
        "  \n",
        "      encoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) \n",
        "      decoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) \n",
        "      #y = tf.placeholder(dtype=tf.float32, shape=[None, None, None])\n",
        "      y_sequence_length =  tf.placeholder(dtype=tf.int32, shape=[None]) \n",
        "      \n",
        "      encoder_state=  encoding(encoder_x)\n",
        "      train_logits, infer_output = decoding(decoder_x, encoder_state, y_sequence_length)\n",
        "      \n",
        "      \n",
        "      masks = tf.sequence_mask(y_sequence_length, Y_MAX_LENGTH, dtype=tf.float32)\n",
        "      \n",
        "      softmax_out = tf.nn.softmax(train_logits)\n",
        "      \n",
        "      #cost =  tf.nn.softmax_cross_entropy_with_logits(logits=train_logits, labels=y)\n",
        "      cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            train_logits,\n",
        "            decoder_x,\n",
        "            masks)\n",
        "      \n",
        "\n",
        "      total_cost = tf.reduce_mean(cost)\n",
        "      optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost)\n",
        "      \n",
        "    \n",
        "     # accuracy = get_accuracy(train_logits, decoder_x)\n",
        "      \n",
        "#       y_t = tf.argmax(softmax_out,axis=2)\n",
        "#       y_t = tf.cast(y_t, tf.int32)\n",
        "\n",
        "#       prediction = y_t\n",
        "#       mask_label = decoder_x\n",
        "\n",
        "#       correct_pred = tf.equal(prediction, mask_label)\n",
        "#       accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hKX06qd1BCNV",
        "colab_type": "code",
        "outputId": "33a40a87-2860-464f-843f-657b5700ef25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2242
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=tf_graph) as sess:\n",
        "\n",
        "    merged = tf.summary.merge_all()\n",
        "    saver = tf.train.Saver(max_to_keep=30)\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch< num_epochs: \n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch in range(num_batches):\n",
        "            start_batch = batch * batch_size\n",
        "            end_batch = start_batch + batch_size\n",
        "\n",
        "            batch_loss = 0\n",
        "            batch_y_sequence_length=[]\n",
        "            \n",
        "            batch_x = x_data[start_batch:end_batch]\n",
        "            batch_y = y_data[start_batch:end_batch]\n",
        "            \n",
        "            for y in batch_y:\n",
        "                batch_y_sequence_length.append(len(y))\n",
        "            \n",
        "            batch_y_sequence_length = np.array(batch_y_sequence_length)\n",
        "            \n",
        "            for _ in range(len(batch_x)):\n",
        "                x_length = len(batch_x[_])\n",
        "                y_length = len(batch_y[_])\n",
        "\n",
        "                batch_x[_].extend(np.zeros([X_MAX_LENGTH - x_length], dtype=int))\n",
        "                batch_y[_].extend(np.zeros([Y_MAX_LENGTH - y_length], dtype=int))\n",
        "            \n",
        "   \n",
        "#             batch_x_valid = x_data_val[start_batch:end_batch]\n",
        "#             batch_y_valid = y_data_val[start_batch:end_batch]\n",
        "        \n",
        "#             y_one_hot = one_hot(batch_y, Y_MAX_LENGTH, y_word_to_idx)\n",
        "#             y_one_hot_valid = one_hot(batch_y_valid, Y_MAX_LENGTH, y_word_to_idx)\n",
        "            \n",
        "            opt = sess.run(optimizer, feed_dict={encoder_x:batch_x,  decoder_x:batch_y,   y_sequence_length: batch_y_sequence_length })\n",
        "            batch_loss,  = sess.run([total_cost], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,   y_sequence_length: batch_y_sequence_length })     \n",
        "            #_train_logits = sess.run([train_logits], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,   y_sequence_length: batch_y_sequence_length })    \n",
        "            \n",
        "            #accuracy = get_accuracy(_train_logits, softmax_out)\n",
        "            \n",
        "            \n",
        "            #opt = sess.run(optimizer, feed_dict={encoder_x: x_data_valid, decoder_x: y_data_valid, y: y_one_hot_valid})\n",
        "            #batch_loss_valid, _accuracy_valid, = sess.run([total_cost_val, accuracy_val], feed_dict={encoder_x:  batch_x_valid, decoder_x:  batch_y_valid, y: y_one_hot_valid})     \n",
        "            #logits_val = sess.run([infer_output], feed_dict={encoder_x:  batch_x_valid}) \n",
        "            \n",
        "            if batch % 10 == 0:\n",
        "\n",
        "\n",
        "                if epoch % 5 == 0:\n",
        "#                    save_output_path = save_dir + 'TRAINING_TRANSLATION_OUTPUT_EPOCH_{}_STATE-SIZE_{}_NUM-LAYERS_{}_LEARNING-RATE{}_EMBEDDING-SIZE_{}.txt'.format(epoch, state_size, decoder_layers, lr, inembsize)\n",
        "\n",
        "#                     with open(save_output_path,'w') as f:\n",
        "#                        for item in text:\n",
        "#                            f.write(\"%s\\n\" % item)\n",
        "                      print('Batch {}/{} Training Loss: {}, Training Accuracy: {}'.format(batch+1, num_batches, batch_loss,  accuracy))\n",
        "\n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "            \n",
        "        if epoch == 0:\n",
        "            path = saver.save(sess, save_dir, epoch)\n",
        "        else:\n",
        "            path = saver.save(sess, save_dir, epoch, write_meta_graph=False)\n",
        "\n",
        "        print(\"Saved in: %s\" % path)\n",
        "        epoch+=1"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1/102 Training Loss: 3.4963207244873047, Training Accuracy: 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [4480,87] and labels shape [7936]\n\t [[{{node sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2aab7efc67aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#             y_one_hot_valid = one_hot(batch_y_valid, Y_MAX_LENGTH, y_word_to_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mencoder_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdecoder_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0my_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y_sequence_length\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_cost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mencoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdecoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0my_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y_sequence_length\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m#_train_logits = sess.run([train_logits], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,   y_sequence_length: batch_y_sequence_length })\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [4480,87] and labels shape [7936]\n\t [[node sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-24-da03d1b5b36e>:21) ]]\n\nCaused by op 'sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-da03d1b5b36e>\", line 21, in <module>\n    masks)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/loss.py\", line 92, in sequence_loss\n    labels=targets, logits=logits_flat)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2657, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 8215, in sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [4480,87] and labels shape [7936]\n\t [[node sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-24-da03d1b5b36e>:21) ]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N8EeqiOjE2Rs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tf_graph = tf.Graph()\n",
        "# with tf_graph.as_default():\n",
        "  \n",
        "#   encoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) #[batch_size, X_MAX_LENGTH]\n",
        "#   decoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) #[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "#   y = tf.placeholder(dtype=tf.float32, shape=[None, None, None])#[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "  \n",
        "#   with tf.variable_scope('encoder_word_embeddings'):\n",
        "\n",
        "#       word_embeddings = tf.get_variable('encoder_word_embeddings', [X_VOCAB_SIZE, inembsize])\n",
        "#       encoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, encoder_x)\n",
        "#       encoder_embedded_word_ids = tf.reshape(encoder_embedded_word_ids, [-1, X_MAX_LENGTH, inembsize])\n",
        "\n",
        "#   with tf.variable_scope('encoder'):\n",
        "\n",
        "#       for n in range(encoder_layers): \n",
        "#                 encoder_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "#                 encoder_out, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
        "#                 cell_fw = encoder_cell,  ## initializer, reuse to be set\n",
        "#                 cell_bw = encoder_cell,  ## initializer, reuse to be set\n",
        "#                 inputs = encoder_embedded_word_ids,\n",
        "#                 dtype = tf.float32)\n",
        "\n",
        "#   #           (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
        "#   #               cell_fw = tf.nn.rnn_cell.LSTMCell(state_size//2, state_is_tuple=True),  ## initializer, reuse to be set\n",
        "#   #               cell_bw = tf.nn.rnn_cell.LSTMCell(state_size//2, state_is_tuple=True),  ## initializer, reuse to be set\n",
        "#   #               inputs = encoder_embedded_word_ids,\n",
        "\n",
        "#   #               sequence_length =  [61]* batch_size,\n",
        "#   #               dtype = tf.float32)\n",
        "\n",
        "#   #           encoder_embedded_word_ids = tf.concat((out_fw, out_bw), 2)\n",
        "\n",
        "#   #           bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n",
        "#   #           bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n",
        "#   #           bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
        "#   #           encoder_state = tuple([bi_lstm_state] * encoder_layers)\n",
        "\n",
        "#   #           encoder_state = tuple(encoder_state[-1] for _ in range(encoder_layers))\n",
        "\n",
        "\n",
        "#   with tf.variable_scope('decoder_word_embeddings'):\n",
        "\n",
        "#       word_embeddings = tf.get_variable('decoder_word_embeddings', [Y_VOCAB_SIZE, outembsize])\n",
        "#       decoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, decoder_x)\n",
        "#       decoder_embedded_word_ids = tf.reshape(decoder_embedded_word_ids,  [-1, Y_MAX_LENGTH,  outembsize])\n",
        "\n",
        "\n",
        "\n",
        "#   with tf.variable_scope('decoder'):\n",
        "\n",
        "#       decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(state_size) for _ in range(decoder_layers)])\n",
        "#       dense_layer = tf.layers.Dense(Y_VOCAB_SIZE)\n",
        "\n",
        "#       #decoder for training data\n",
        "#       training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embedded_word_ids, sequence_length = [62]* batch_size)\n",
        "\n",
        "\n",
        "\n",
        "#       training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "#               cell = decoder_cell,\n",
        "#               helper = training_helper,\n",
        "#               initial_state = encoder_state,\n",
        "#               output_layer = dense_layer)\n",
        "\n",
        "#       training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "#               decoder = training_decoder,\n",
        "#               impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further\n",
        "#               maximum_iterations = Y_MAX_LENGTH)   \n",
        "      \n",
        "#       training_logits = training_decoder_output.rnn_output\n",
        "#       softmax_out = tf.nn.softmax(training_logits)\n",
        "      \n",
        "#       cost =  tf.nn.softmax_cross_entropy_with_logits(logits=training_logits, labels=y)\n",
        "\n",
        "#       total_cost = tf.reduce_mean(cost)\n",
        "#       optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost)\n",
        "\n",
        "#       y_t = tf.argmax(softmax_out,axis=2)\n",
        "#       y_t = tf.cast(y_t, tf.int32)\n",
        "\n",
        "#       #prediction = tf.boolean_mask(y_t, masks)\n",
        "#       prediction = y_t\n",
        "#       #mask_label = tf.boolean_mask(decoder_x, masks)\n",
        "#       mask_label = decoder_x\n",
        "\n",
        "#       correct_pred = tf.equal(prediction, mask_label)\n",
        "#       accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "           \n",
        "#   with tf.variable_scope('decoder', reuse=True):   \n",
        "#       #decoder for test data\n",
        "#       predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding = word_embeddings,\n",
        "#               start_tokens = tf.fill([batch_size], y_word_to_idx['GO']), end_token = y_word_to_idx['EOS'])\n",
        "                           \n",
        "#       predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "#               cell = decoder_cell,\n",
        "#               helper = predicting_helper,\n",
        "#               initial_state = encoder_state,\n",
        "#               output_layer = dense_layer)\n",
        "\n",
        "#       predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "#               decoder = predicting_decoder,\n",
        "#               impute_finished = True,\n",
        "#               maximum_iterations = 2 * (X_MAX_LENGTH))\n",
        "\n",
        "#       predicting_ids = predicting_decoder_output.sample_id\n",
        "      \n",
        "#       #predicting_ids = tf.cast(predicting_ids, tf.float32)\n",
        "#       #softmax_out_val = tf.nn.softmax(predicting_ids)\n",
        "# #       print(predicting_ids)\n",
        "      \n",
        "# #       cost_val =  tf.nn.softmax_cross_entropy_with_logits(logits=predicting_ids, labels=y)\n",
        "\n",
        "# #       total_cost_val = tf.reduce_mean(cost_val)\n",
        "#       #optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost_val)\n",
        "      \n",
        "#       #y_t_val = tf.argmax(predicting_ids,axis=2)\n",
        "#       #y_t_val = tf.cast(y_t_val, tf.int32)\n",
        "\n",
        "#       #prediction = tf.boolean_mask(y_t, masks)\n",
        "# #       prediction_val = predicting_ids\n",
        "      \n",
        "# #       prediction_val = tf.cast(prediction_val, tf.int32)\n",
        "# #       #mask_label = tf.boolean_mask(decoder_x, masks)\n",
        "      \n",
        "# #       mask_label_val = decoder_x\n",
        "\n",
        "# #       correct_pred_val= tf.equal(prediction_val, mask_label_val)\n",
        "# #       accuracy_val = tf.reduce_mean(tf.cast(correct_pred_val, tf.float32))\n",
        "      \n",
        "      \n",
        "#       #masks = tf.sequence_mask(tf.count_nonzero(decoder_x, 1, dtype=tf.int32), Y_MAX_LENGTH, dtype=tf.float32)  \n",
        "\n",
        "#       #cost = tf.contrib.seq2seq.sequence_loss(logits = softmax_out,targets = decoder_x)\n",
        "\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJenXzc7e901",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}