{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_PA3_NEW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "L9ixfY61Bege",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "imXc7RCu-at0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from collections import OrderedDict\n",
        "from itertools import chain\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3Uyb4HA-qKs",
        "colab_type": "code",
        "outputId": "d2c78004-f0b2-40db-b3df-7389e414261b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EPkld2qwnGP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "building encoder\n"
      ]
    },
    {
      "metadata": {
        "id": "GW0tXdQqntUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "batch_size = 20\n",
        "init = 1\n",
        "dropout_prob= 0\n",
        "decode_method=1\n",
        "beam_width = 10\n",
        "save_dir = 'drive/My Drive/data_PA3/'\n",
        "num_epochs = 10\n",
        "train_path = 'drive/My Drive/data_PA3/train.csv'\n",
        "valid_path = 'drive/My Drive/data_PA3/valid.csv'\n",
        "test_path = 'drive/My Drive/data_PA3/partial_test_400.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qiGRxGP-r_R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(train_path)\n",
        "valid= pd.read_csv(valid_path)\n",
        "test = pd.read_csv(test_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zAg2bE-P4dXH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inembsize = 256\n",
        "outembsize = 256\n",
        "\n",
        "num_batches = len(train)//batch_size\n",
        "encoder_layers = 1\n",
        "decoder_layers = 2\n",
        "state_size = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GSxI6R1Zdc2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_eng = train['ENG']\n",
        "train_hin = train[\"HIN\"]\n",
        "\n",
        "valid_eng = valid[\"ENG\"]\n",
        "valid_hin = valid[\"HIN\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtmkDkmzyWoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot(batch_word, max_length, word_to_idx):\n",
        "    vec_word = np.zeros((len(batch_word), max_length, len(word_to_idx)), dtype=float)\n",
        "    for i, word in enumerate(batch_word):\n",
        "        for j, char in enumerate(word):\n",
        "            vec_word[i, j, char] = 1.\n",
        "    return vec_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SeZ4DeG9yaf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(prediction, batch_size, length, vocab_size, idx_to_word):\n",
        "\n",
        "    batch_softmax = np.reshape(prediction, [batch_size, length, vocab_size])\n",
        "    batch_word = []\n",
        "\n",
        "    for word in batch_softmax:\n",
        "        new_word = ''\n",
        "        for char in word:\n",
        "            vector_position = np.argmax(char)\n",
        "            y_word = idx_to_word[vector_position]\n",
        "            if y_word != '#':\n",
        "                new_word = new_word + y_word + ' '\n",
        "            else:\n",
        "                new_word = new_word + ''\n",
        "        batch_word.append(new_word)\n",
        "\n",
        "    return batch_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HvqFl_c8Buw2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## To remove later\n",
        "\n",
        "# def variable_summaries(var):\n",
        "#   \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "#   with tf.name_scope('summaries'):\n",
        "#     mean = tf.reduce_mean(var)\n",
        "#     tf.summary.scalar('mean', mean)\n",
        "#     with tf.name_scope('stddev'):\n",
        "#       stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "#     tf.summary.scalar('stddev', stddev)\n",
        "#     tf.summary.scalar('max', tf.reduce_max(var))\n",
        "#     tf.summary.scalar('min', tf.reduce_min(var))\n",
        "#     tf.summary.histogram('histogram', var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bn9keAIChKrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(train_eng, train_hin):\n",
        "    \n",
        "    # Feed input data in backwards for better translation performance.\n",
        "    x_data = [(tf.compat.as_str(train_eng[_]).lower().split(' ')) for _ in range(len(train_eng))]\n",
        "    x_data = [list(x_data[_]) for _ in range(len(x_data))]\n",
        "    X_DATA_SIZE = len(x_data)\n",
        "\n",
        "    y_data = [(tf.compat.as_str(train_hin[_]).lower().split(' ')) for _ in range(len(train_hin))]\n",
        "    y_data = [list(y_data[_]) for _ in range(len(y_data))]\n",
        "    Y_DATA_SIZE = len(y_data)\n",
        "\n",
        "\n",
        "    x_word_list = []\n",
        "    y_word_list = []\n",
        "    [x_word_list.extend(x_data[_]) for _ in range(len(x_data))]\n",
        "    [y_word_list.extend(y_data[_]) for _ in range(len(y_data))]\n",
        "\n",
        "    x_word_dictionary = []\n",
        "    y_word_dictionary = []\n",
        "    x_word_dictionary.extend(collections.Counter(x_word_list).most_common(44))\n",
        "    y_word_dictionary.extend(collections.Counter(y_word_list).most_common(84))\n",
        "    \n",
        "\n",
        "    x_idx_to_word = [word[0] for idx, word in enumerate(x_word_dictionary)]\n",
        "    x_idx_to_word.insert(0, 'GO')\n",
        "    x_idx_to_word.insert(1, 'EOS')\n",
        "    x_idx_to_word.insert(2, 'PAD')\n",
        "    x_idx_to_word.insert(3, 'UK')\n",
        "\n",
        "\n",
        "    y_idx_to_word = [word[0] for idx, word in enumerate(y_word_dictionary)]\n",
        "    y_idx_to_word.insert(0, 'GO')\n",
        "    y_idx_to_word.insert(1, 'EOS')\n",
        "    y_idx_to_word.insert(2, 'PAD')\n",
        "    y_idx_to_word.insert(3, 'UK')\n",
        "\n",
        "    x_word_to_idx = {word:ix for ix, word in enumerate(x_idx_to_word)}\n",
        "    y_word_to_idx = {word: ix for ix, word in enumerate(y_idx_to_word)}\n",
        "\n",
        "    X_VOCAB_SIZE = len(x_word_dictionary) + 4\n",
        "    Y_VOCAB_SIZE = len(y_word_dictionary) + 4\n",
        "\n",
        "    X_MAX_LENGTH = max([len(x_data[_]) for _ in range(len(x_data))])\n",
        "    Y_MAX_LENGTH = max([len(y_data[_]) for _ in range(len(y_data))])\n",
        "\n",
        "    return X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOPM5vzBQtTF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE = load_data(train_eng, train_hin)                                                                                                                   \n",
        "\n",
        "X_VOCAB_SIZE_VALID, Y_VOCAB_SIZE_VALID, x_idx_to_word_VALID, x_word_to_idx_VALID, y_idx_to_word_VALID, y_word_to_idx_VALID, X_MAX_LENGTH_VALID, Y_MAX_LENGTH_VALID, X_DATA_SIZE_VALID = load_data(valid_eng, valid_hin)                                                                                                                   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpmDEaRYxFcZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin):\n",
        "\n",
        "\n",
        "    x_data = inputs\n",
        "    y_data = labels\n",
        "\n",
        "    x_data = [(tf.compat.as_str(x_data[_]).lower().split(' ')) for _ in range(len(x_data))]\n",
        "    x_data = [list(x_data[_]) for _ in range(len(x_data))]\n",
        "       \n",
        "    y_data = [(tf.compat.as_str(y_data[_]).lower().split(' ')) for _ in range(len(y_data))]\n",
        "    y_data = [list(y_data[_]) for _ in range(len(y_data))]\n",
        "\n",
        "    for i, word in enumerate(x_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in x_word_to_idx:\n",
        "                x_data[i][j] = x_word_to_idx[charac]\n",
        "            else:\n",
        "                x_data[i][j] = x_word_to_idx['pad']\n",
        "\n",
        "    for i, word in enumerate(y_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in y_word_to_idx:\n",
        "                y_data[i][j] = y_word_to_idx[charac]\n",
        "            else:\n",
        "                y_data[i][j] = y_word_to_idx['pad']\n",
        "                \n",
        "    for _ in range(len(x_data)):\n",
        "      x_length = len(x_data[_])\n",
        "      y_length = len(y_data[_])\n",
        "\n",
        "      x_data[_].extend(np.zeros([X_MAX_LENGTH - x_length], dtype=int))\n",
        "      y_data[_].extend(np.zeros([Y_MAX_LENGTH - y_length], dtype=int))\n",
        "\n",
        "    return x_data, y_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZFb7VoMHoI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data , y_data = get_data( x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin)                                              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFNbGZXhgJ-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data_valid , y_data_valid = get_data(x_word_to_idx, y_word_to_idx, inputs = valid_eng, labels = valid_hin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zBczf1yEvULF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N8EeqiOjE2Rs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "602c9779-0cb1-4f23-860d-8f3be3c5fc73"
      },
      "cell_type": "code",
      "source": [
        "tf_graph = tf.Graph()\n",
        "with tf_graph.as_default():\n",
        "  \n",
        "  encoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) #[batch_size, X_MAX_LENGTH]\n",
        "  decoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) #[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "  y = tf.placeholder(dtype=tf.float32, shape=[None, None, None])#[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "  \n",
        "  with tf.variable_scope('encoder_word_embeddings'):\n",
        "\n",
        "      word_embeddings = tf.get_variable('encoder_word_embeddings', [X_VOCAB_SIZE, inembsize])\n",
        "      encoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, encoder_x)\n",
        "      encoder_embedded_word_ids = tf.reshape(encoder_embedded_word_ids, [-1, X_MAX_LENGTH, inembsize])\n",
        "\n",
        "  with tf.variable_scope('encoder'):\n",
        "\n",
        "      for n in range(encoder_layers): \n",
        "                encoder_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "                encoder_out, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = encoder_cell,  ## initializer, reuse to be set\n",
        "                cell_bw = encoder_cell,  ## initializer, reuse to be set\n",
        "                inputs = encoder_embedded_word_ids,\n",
        "                dtype = tf.float32)\n",
        "\n",
        "  #           (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
        "  #               cell_fw = tf.nn.rnn_cell.LSTMCell(state_size//2, state_is_tuple=True),  ## initializer, reuse to be set\n",
        "  #               cell_bw = tf.nn.rnn_cell.LSTMCell(state_size//2, state_is_tuple=True),  ## initializer, reuse to be set\n",
        "  #               inputs = encoder_embedded_word_ids,\n",
        "\n",
        "  #               sequence_length =  [61]* batch_size,\n",
        "  #               dtype = tf.float32)\n",
        "\n",
        "  #           encoder_embedded_word_ids = tf.concat((out_fw, out_bw), 2)\n",
        "\n",
        "  #           bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n",
        "  #           bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n",
        "  #           bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
        "  #           encoder_state = tuple([bi_lstm_state] * encoder_layers)\n",
        "\n",
        "  #           encoder_state = tuple(encoder_state[-1] for _ in range(encoder_layers))\n",
        "\n",
        "\n",
        "  with tf.variable_scope('decoder_word_embeddings'):\n",
        "\n",
        "      word_embeddings = tf.get_variable('decoder_word_embeddings', [Y_VOCAB_SIZE, outembsize])\n",
        "      decoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, decoder_x)\n",
        "      decoder_embedded_word_ids = tf.reshape(decoder_embedded_word_ids,  [-1, Y_MAX_LENGTH,  outembsize])\n",
        "\n",
        "\n",
        "\n",
        "  with tf.variable_scope('decoder'):\n",
        "\n",
        "      decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(state_size) for _ in range(decoder_layers)])\n",
        "      dense_layer = tf.layers.Dense(Y_VOCAB_SIZE)\n",
        "\n",
        "      #decoder for training data\n",
        "      training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embedded_word_ids, sequence_length = [62]* batch_size)\n",
        "\n",
        "\n",
        "\n",
        "      training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "              cell = decoder_cell,\n",
        "              helper = training_helper,\n",
        "              initial_state = encoder_state,\n",
        "              output_layer = dense_layer)\n",
        "\n",
        "      training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "              decoder = training_decoder,\n",
        "              impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further\n",
        "              maximum_iterations = Y_MAX_LENGTH)   \n",
        "      \n",
        "      training_logits = training_decoder_output.rnn_output\n",
        "      softmax_out = tf.nn.softmax(training_logits)\n",
        "      \n",
        "      cost =  tf.nn.softmax_cross_entropy_with_logits(logits=training_logits, labels=y)\n",
        "\n",
        "      total_cost = tf.reduce_mean(cost)\n",
        "      optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost)\n",
        "\n",
        "      y_t = tf.argmax(softmax_out,axis=2)\n",
        "      y_t = tf.cast(y_t, tf.int32)\n",
        "\n",
        "      #prediction = tf.boolean_mask(y_t, masks)\n",
        "      prediction = y_t\n",
        "      #mask_label = tf.boolean_mask(decoder_x, masks)\n",
        "      mask_label = decoder_x\n",
        "\n",
        "      correct_pred = tf.equal(prediction, mask_label)\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "           \n",
        "  with tf.variable_scope('decoder', reuse=True):   \n",
        "      #decoder for test data\n",
        "      predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding = word_embeddings,\n",
        "              start_tokens = tf.fill([batch_size], y_word_to_idx['GO']), end_token = y_word_to_idx['EOS'])\n",
        "                           \n",
        "      predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "              cell = decoder_cell,\n",
        "              helper = predicting_helper,\n",
        "              initial_state = encoder_state,\n",
        "              output_layer = dense_layer)\n",
        "\n",
        "      predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "              decoder = predicting_decoder,\n",
        "              impute_finished = True,\n",
        "              maximum_iterations = 2 * (X_MAX_LENGTH))\n",
        "\n",
        "      predicting_ids = predicting_decoder_output.sample_id\n",
        "      #softmax_out_val = tf.nn.softmax(predicting_ids)\n",
        "#       cost_val =  tf.nn.softmax_cross_entropy_with_logits(logits=predicting_ids, labels=y)\n",
        "\n",
        "#       total_cost_val = tf.reduce_mean(cost_val)\n",
        "#       optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost_val)\n",
        "      \n",
        "      #prediction_val = tf.argmax(predicting_ids,axis=2)\n",
        "      #y_t_val = tf.cast(y_t_val, tf.int32)\n",
        "\n",
        "#       #prediction = tf.boolean_mask(y_t, masks)\n",
        "#       prediction_val = y_t_val\n",
        "#       #mask_label = tf.boolean_mask(decoder_x, masks)\n",
        "#       mask_label_val = decoder_x\n",
        "\n",
        "#       correct_pred_val= tf.equal(prediction_val, mask_label_val)\n",
        "#       accuracy_val = tf.reduce_mean(tf.cast(correct_pred_val, tf.float32))\n",
        "      \n",
        "      \n",
        "      #masks = tf.sequence_mask(tf.count_nonzero(decoder_x, 1, dtype=tf.int32), Y_MAX_LENGTH, dtype=tf.float32)  \n",
        "\n",
        "      #cost = tf.contrib.seq2seq.sequence_loss(logits = softmax_out,targets = decoder_x)\n",
        "\n",
        "      \n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-30-4462dc3aad9d>:17: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-30-4462dc3aad9d>:22: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From <ipython-input-30-4462dc3aad9d>:52: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-30-4462dc3aad9d>:74: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hKX06qd1BCNV",
        "colab_type": "code",
        "outputId": "b96e239f-be2f-4666-c5b1-8f8019c590fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1261
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=tf_graph) as sess:\n",
        "\n",
        "    merged = tf.summary.merge_all()\n",
        "    saver = tf.train.Saver(max_to_keep=30)\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch< num_epochs: \n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch in range(num_batches):\n",
        "            start_batch = batch * batch_size\n",
        "            end_batch = start_batch + batch_size\n",
        "\n",
        "            batch_loss = 0\n",
        "\n",
        "            batch_x = x_data[start_batch:end_batch]\n",
        "            batch_y = y_data[start_batch:end_batch]\n",
        "\n",
        "            y_one_hot = one_hot(batch_y, Y_MAX_LENGTH, y_word_to_idx)\n",
        "            #y_one_hot_valid = one_hot(y_data_valid, Y_MAX_LENGTH, y_word_to_idx)\n",
        "            \n",
        "            opt = sess.run(optimizer, feed_dict={encoder_x:batch_x,  decoder_x:batch_y,  y: y_one_hot})\n",
        "            batch_loss, _accuracy, = sess.run([total_cost, accuracy], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,  y: y_one_hot})     \n",
        "            logits, _prediction_series = sess.run([training_logits, softmax_out], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,  y: y_one_hot})    \n",
        "            \n",
        "            #opt = sess.run(optimizer, feed_dict={encoder_x: x_data_valid, decoder_x: y_data_valid, y: y_one_hot_valid})\n",
        "            #batch_loss_valid, _accuracy_valid, = sess.run([total_cost, accuracy], feed_dict={encoder_x: x_data_valid, decoder_x: y_data_valid, y: y_one_hot_valid})     \n",
        "            #logits, _prediction_series = sess.run([training_logits, softmax_out], feed_dict={encoder_x: x_data_valid, decoder_x: y_data_valid, y: y_one_hot_valid}) \n",
        "            \n",
        "            if batch % 1 == 0:\n",
        "\n",
        "                text = generate_text(_prediction_series, batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE, y_idx_to_word)\n",
        "\n",
        "                if epoch % 5 == 0:\n",
        "                    save_output_path = save_dir + 'TRAINING_TRANSLATION_OUTPUT_EPOCH_{}_STATE-SIZE_{}_NUM-LAYERS_{}_LEARNING-RATE{}_EMBEDDING-SIZE_{}.txt'.format(epoch, state_size, decoder_layers, lr, inembsize)\n",
        "\n",
        "                    with open(save_output_path,'w') as f:\n",
        "                       for item in text:\n",
        "                           f.write(\"%s\\n\" % item)\n",
        "                 \n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "            print('Batch {}/{} Training Loss: {}, Training Accuracy: {}'.format(batch+1, num_batches, batch_loss,  _accuracy))\n",
        "\n",
        "    \n",
        "        if epoch == 0:\n",
        "            path = saver.save(sess, save_dir, epoch)\n",
        "        else:\n",
        "            path = saver.save(sess, save_dir, epoch, write_meta_graph=False)\n",
        "\n",
        "        print(\"Saved in: %s\" % path)\n",
        "        epoch+=1"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1/656 Training Loss: 3.403665542602539, Training Accuracy: 0.8500000238418579\n",
            "Batch 2/656 Training Loss: 0.8618399500846863, Training Accuracy: 0.8475806713104248\n",
            "Batch 3/656 Training Loss: 1.39096200466156, Training Accuracy: 0.8411290049552917\n",
            "Batch 4/656 Training Loss: 1.4326122999191284, Training Accuracy: 0.8209677338600159\n",
            "Batch 5/656 Training Loss: 0.969719648361206, Training Accuracy: 0.8379032015800476\n",
            "Batch 6/656 Training Loss: 1.2629585266113281, Training Accuracy: 0.7661290168762207\n",
            "Batch 7/656 Training Loss: 0.7144237160682678, Training Accuracy: 0.8556451797485352\n",
            "Batch 8/656 Training Loss: 0.6609979867935181, Training Accuracy: 0.8709677457809448\n",
            "Batch 9/656 Training Loss: 0.6751545667648315, Training Accuracy: 0.8717741966247559\n",
            "Batch 10/656 Training Loss: 0.5096594095230103, Training Accuracy: 0.8919354677200317\n",
            "Batch 11/656 Training Loss: 0.7747747302055359, Training Accuracy: 0.8491935729980469\n",
            "Batch 12/656 Training Loss: 0.9304915070533752, Training Accuracy: 0.823387086391449\n",
            "Batch 13/656 Training Loss: 0.6098727583885193, Training Accuracy: 0.8669354915618896\n",
            "Batch 14/656 Training Loss: 2.4415385723114014, Training Accuracy: 0.6733871102333069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c30b36f470ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#y_one_hot_valid = one_hot(y_data_valid, Y_MAX_LENGTH, y_word_to_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mencoder_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdecoder_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mencoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdecoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_prediction_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mencoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdecoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "VRB64kR7YKcO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}