{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_NEW_NEW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "L9ixfY61Bege",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "imXc7RCu-at0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from collections import OrderedDict\n",
        "from itertools import chain\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3Uyb4HA-qKs",
        "colab_type": "code",
        "outputId": "67409e67-d6fc-474b-faf5-c4b6814ad899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EPkld2qwnGP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "building encoder\n"
      ]
    },
    {
      "metadata": {
        "id": "GW0tXdQqntUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "batch_size = 128\n",
        "init = 1\n",
        "dropout_prob= 0\n",
        "decode_method=1\n",
        "beam_width = 10\n",
        "save_dir = 'drive/My Drive/data_PA3/'\n",
        "num_epochs = 10\n",
        "train_path = 'drive/My Drive/data_PA3/train.csv'\n",
        "valid_path = 'drive/My Drive/data_PA3/valid.csv'\n",
        "test_path = 'drive/My Drive/data_PA3/partial_test_400.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qiGRxGP-r_R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(train_path)\n",
        "valid= pd.read_csv(valid_path)\n",
        "test = pd.read_csv(test_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zAg2bE-P4dXH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inembsize = 256\n",
        "outembsize = 256\n",
        "\n",
        "num_batches = len(train)//batch_size\n",
        "encoder_layers = 1\n",
        "decoder_layers = 2\n",
        "state_size = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GSxI6R1Zdc2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_eng = train['ENG']\n",
        "train_hin = train[\"HIN\"]\n",
        "\n",
        "valid_eng = valid[\"ENG\"]\n",
        "valid_hin = valid[\"HIN\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtmkDkmzyWoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot(batch_word, max_length, word_to_idx):\n",
        "    vec_word = np.zeros((len(batch_word), max_length, len(word_to_idx)), dtype=float)\n",
        "    for i, word in enumerate(batch_word):\n",
        "        for j, char in enumerate(word):\n",
        "            vec_word[i, j, char] = 1.\n",
        "    return vec_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SeZ4DeG9yaf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(prediction, batch_size, length, vocab_size, idx_to_word):\n",
        "\n",
        "    batch_softmax = np.reshape(prediction, [batch_size, length, vocab_size])\n",
        "    batch_word = []\n",
        "\n",
        "    for word in batch_softmax:\n",
        "        new_word = ''\n",
        "        for char in word:\n",
        "            vector_position = np.argmax(char)\n",
        "            y_word = idx_to_word[vector_position]\n",
        "            if y_word != 'PAD':\n",
        "                new_word = new_word + y_word + ' '\n",
        "            else:\n",
        "                new_word = new_word + ''\n",
        "        batch_word.append(new_word)\n",
        "\n",
        "    return batch_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HvqFl_c8Buw2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## To remove later\n",
        "\n",
        "# def variable_summaries(var):\n",
        "#   \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "#   with tf.name_scope('summaries'):\n",
        "#     mean = tf.reduce_mean(var)\n",
        "#     tf.summary.scalar('mean', mean)\n",
        "#     with tf.name_scope('stddev'):\n",
        "#       stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "#     tf.summary.scalar('stddev', stddev)\n",
        "#     tf.summary.scalar('max', tf.reduce_max(var))\n",
        "#     tf.summary.scalar('min', tf.reduce_min(var))\n",
        "#     tf.summary.histogram('histogram', var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bn9keAIChKrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(train_eng, train_hin):\n",
        "    \n",
        "    # Feed input data in backwards for better translation performance.\n",
        "    x_data = [(tf.compat.as_str(train_eng[_]).lower().split(' ')) for _ in range(len(train_eng))]\n",
        "    x_data = [list(x_data[_]) for _ in range(len(x_data))]\n",
        "    X_DATA_SIZE = len(x_data)\n",
        "\n",
        "    y_data = [(tf.compat.as_str(train_hin[_]).lower().split(' ')) for _ in range(len(train_hin))]\n",
        "    y_data = [list(y_data[_]) for _ in range(len(y_data))]\n",
        "    Y_DATA_SIZE = len(y_data)\n",
        "\n",
        "\n",
        "    x_word_list = []\n",
        "    y_word_list = []\n",
        "    [x_word_list.extend(x_data[_]) for _ in range(len(x_data))]\n",
        "    [y_word_list.extend(y_data[_]) for _ in range(len(y_data))]\n",
        "\n",
        "    x_word_dictionary = []\n",
        "    y_word_dictionary = []\n",
        "    x_word_dictionary.extend(collections.Counter(x_word_list).most_common(44))\n",
        "    y_word_dictionary.extend(collections.Counter(y_word_list).most_common(84))\n",
        "    \n",
        "\n",
        "    x_idx_to_word = [word[0] for idx, word in enumerate(x_word_dictionary)]\n",
        "    x_idx_to_word.insert(0, 'GO')\n",
        "    x_idx_to_word.insert(1, 'EOS')\n",
        "    x_idx_to_word.insert(2, 'PAD')\n",
        "    x_idx_to_word.insert(3, 'UK')\n",
        "\n",
        "\n",
        "    y_idx_to_word = [word[0] for idx, word in enumerate(y_word_dictionary)]\n",
        "    y_idx_to_word.insert(0, 'GO')\n",
        "    y_idx_to_word.insert(1, 'EOS')\n",
        "    y_idx_to_word.insert(2, 'PAD')\n",
        "    y_idx_to_word.insert(3, 'UK')\n",
        "\n",
        "    x_word_to_idx = {word:ix for ix, word in enumerate(x_idx_to_word)}\n",
        "    y_word_to_idx = {word: ix for ix, word in enumerate(y_idx_to_word)}\n",
        "\n",
        "    X_VOCAB_SIZE = len(x_word_dictionary) + 4\n",
        "    Y_VOCAB_SIZE = len(y_word_dictionary) + 4\n",
        "\n",
        "    X_MAX_LENGTH = max([len(x_data[_]) for _ in range(len(x_data))])\n",
        "    Y_MAX_LENGTH = max([len(y_data[_]) for _ in range(len(y_data))])\n",
        "\n",
        "    return X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOPM5vzBQtTF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE = load_data(train_eng, train_hin)                                                                                                                   \n",
        "\n",
        "X_VOCAB_SIZE_VALID, Y_VOCAB_SIZE_VALID, x_idx_to_word_VALID, x_word_to_idx_VALID, y_idx_to_word_VALID, y_word_to_idx_VALID, X_MAX_LENGTH_VALID, Y_MAX_LENGTH_VALID, X_DATA_SIZE_VALID = load_data(valid_eng, valid_hin)                                                                                                                   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpmDEaRYxFcZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin):\n",
        "\n",
        "\n",
        "    x_data = inputs\n",
        "    y_data = labels\n",
        "\n",
        "    x_data = [(tf.compat.as_str(x_data[_]).lower().split(' ')) for _ in range(len(x_data))]\n",
        "    x_data = [list(x_data[_]) for _ in range(len(x_data))]\n",
        "       \n",
        "    y_data = [(tf.compat.as_str(y_data[_]).lower().split(' ')) for _ in range(len(y_data))]\n",
        "    y_data = [list(y_data[_]) for _ in range(len(y_data))]\n",
        "\n",
        "    for i, word in enumerate(x_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in x_word_to_idx:\n",
        "                x_data[i][j] = x_word_to_idx[charac]\n",
        "            else:\n",
        "                x_data[i][j] = x_word_to_idx['UK']\n",
        "\n",
        "    for i, word in enumerate(y_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in y_word_to_idx:\n",
        "                y_data[i][j] = y_word_to_idx[charac]\n",
        "            else:\n",
        "                y_data[i][j] = y_word_to_idx['UK']\n",
        "                \n",
        "    return x_data, y_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZFb7VoMHoI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data , y_data = get_data( x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin)                                              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFNbGZXhgJ-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data_val , y_data_val = get_data(x_word_to_idx, y_word_to_idx, inputs = valid_eng, labels = valid_hin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a13EGvTatGim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding(encoder_x):\n",
        "  \n",
        "      word_embeddings = tf.get_variable('encoder_word_embeddings', [X_VOCAB_SIZE, inembsize])\n",
        "      encoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, encoder_x)\n",
        "      encoder_embedded_word_ids = tf.reshape(encoder_embedded_word_ids, [-1, X_MAX_LENGTH, inembsize])\n",
        "      \n",
        "      for n in range(encoder_layers): \n",
        "                encoder_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "                encoder_out, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = encoder_cell,  ## initializer, reuse to be set\n",
        "                cell_bw = encoder_cell,  ## initializer, reuse to be set\n",
        "                inputs = encoder_embedded_word_ids,\n",
        "                dtype = tf.float32)\n",
        "            \n",
        "      return encoder_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rqsPIfaLuq4G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_train(decoder_embedded_word_ids, decoder_cell, encoder_state, dense_layer, train_sequence_length):\n",
        "  \n",
        "      training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embedded_word_ids, sequence_length = train_sequence_length)\n",
        "\n",
        "      training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "              cell = decoder_cell,\n",
        "              helper = training_helper,\n",
        "              initial_state = encoder_state,\n",
        "              output_layer = dense_layer)\n",
        "\n",
        "      training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "              decoder = training_decoder,\n",
        "              impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further\n",
        "              maximum_iterations = Y_MAX_LENGTH)   \n",
        "      \n",
        "      training_logits = training_decoder_output.rnn_output\n",
        "      training_logits_max = training_decoder_output.sample_id\n",
        "      \n",
        "      return training_logits, training_logits_max"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jooQgWLFvtTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_infer(word_embeddings, decoder_cell, encoder_state, dense_layer):\n",
        "     \n",
        "      #decoder for test data\n",
        "          predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding = word_embeddings,\n",
        "                  start_tokens = tf.fill([batch_size], y_word_to_idx['GO']), end_token = y_word_to_idx['EOS'])\n",
        "\n",
        "          predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                  cell = decoder_cell,\n",
        "                  helper = predicting_helper,\n",
        "                  initial_state = encoder_state,\n",
        "                  output_layer = dense_layer)\n",
        "\n",
        "          predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                  decoder = predicting_decoder,\n",
        "                  impute_finished = True,\n",
        "                  maximum_iterations = Y_MAX_LENGTH)\n",
        "          \n",
        "          \n",
        "          predicting_ids = predicting_decoder_output.sample_id\n",
        "          \n",
        "          return predicting_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zt7y4-GYv-hL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding(decoder_x, encoder_state, train_sequence_length):\n",
        "      word_embeddings = tf.get_variable('decoder_word_embeddings', [Y_VOCAB_SIZE, outembsize])\n",
        "      decoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, decoder_x)\n",
        "      decoder_embedded_word_ids = tf.reshape(decoder_embedded_word_ids,  [-1, Y_MAX_LENGTH,  outembsize])\n",
        "      \n",
        "      decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(state_size) for _ in range(decoder_layers)])\n",
        "      dense_layer = tf.layers.Dense(Y_VOCAB_SIZE)\n",
        "      \n",
        "      with tf.variable_scope(\"decode\"):\n",
        "            train_output, train_output_max = decoding_train(decoder_embedded_word_ids, decoder_cell, encoder_state, dense_layer, train_sequence_length)\n",
        "          \n",
        "      \n",
        "      with tf.variable_scope(\"decode\", reuse=True):       \n",
        "            infer_output = decoding_infer(word_embeddings, decoder_cell, encoder_state, dense_layer)\n",
        "        \n",
        "      return train_output,  train_output_max, infer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mjDQz_ZQodIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(logits, labels):\n",
        "  \n",
        "    max_seq = Y_MAX_LENGTH\n",
        "    \n",
        "    for i in range(len(logits)):\n",
        "       if len(logits[i])<max_seq:\n",
        "          logits[i] = np.pad(\n",
        "            logits[i],\n",
        "            [(0,0),(0,max_seq - len(logits[i]))],\n",
        "            'constant')\n",
        "          \n",
        "    for i in range(len(labels)):\n",
        "       if len(labels[i])<max_seq:\n",
        "          labels[i] = np.pad(\n",
        "            labels[i],\n",
        "            [(0,0),(0,max_seq - len(labels[i]))],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(logits, labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gfjzdjVLUbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad(sentence_batch, pad_int, max_sentence):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    \n",
        "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5il21DoqK03y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(x_data, y_data, batch_size, x_pad_int, y_pad_int):\n",
        "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(x_data)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "\n",
        "        # Slice the right amount for the batch\n",
        "        x_data_batch = x_data[start_i:start_i + batch_size]\n",
        "        y_data_batch = y_data[start_i:start_i + batch_size]\n",
        "\n",
        "        # Pad\n",
        "        pad_x_batch = np.array(pad(x_data_batch, x_pad_int, X_MAX_LENGTH))\n",
        "        pad_y_batch = np.array(pad(y_data_batch, y_pad_int, Y_MAX_LENGTH))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_y_lengths = []\n",
        "        for y in pad_y_batch:\n",
        "            pad_y_lengths.append(len(y))\n",
        "\n",
        "        pad_x_lengths = []\n",
        "        for x in pad_x_batch:\n",
        "            pad_x_lengths.append(len(x))\n",
        "\n",
        "        yield pad_x_batch, pad_y_batch, pad_x_lengths, pad_y_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T99fng0AzbnE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf_graph = tf.Graph()\n",
        "with tf_graph.as_default():\n",
        "  \n",
        "      encoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) \n",
        "      decoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) \n",
        "      #y = tf.placeholder(dtype=tf.float32, shape=[None, None, None])\n",
        "      y_sequence_length =  tf.placeholder(dtype=tf.int32, shape=[None]) \n",
        "      \n",
        "      encoder_state=  encoding(encoder_x)\n",
        "      train_logits, train_logits_max,  infer_output = decoding(decoder_x, encoder_state, y_sequence_length)\n",
        "      \n",
        "      \n",
        "      masks = tf.sequence_mask(y_sequence_length, Y_MAX_LENGTH, dtype=tf.float32)\n",
        "      \n",
        "      softmax_out = tf.nn.softmax(train_logits)\n",
        "      \n",
        "      #cost =  tf.nn.softmax_cross_entropy_with_logits(logits=train_logits, labels=y)\n",
        "      cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            train_logits,\n",
        "            decoder_x,\n",
        "            masks)\n",
        "      \n",
        "\n",
        "      total_cost = tf.reduce_mean(cost)\n",
        "      optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost)\n",
        "      \n",
        "    \n",
        "     # accuracy = get_accuracy(train_logits, decoder_x)\n",
        "      \n",
        "#       y_t = tf.argmax(softmax_out,axis=2)\n",
        "#       y_t = tf.cast(y_t, tf.int32)\n",
        "\n",
        "#       prediction = y_t\n",
        "#       mask_label = decoder_x\n",
        "\n",
        "#       correct_pred = tf.equal(prediction, mask_label)\n",
        "#       accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hKX06qd1BCNV",
        "colab_type": "code",
        "outputId": "be04abe4-6bc3-48ee-89b5-21ee5e279a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2080
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=tf_graph) as sess:\n",
        "\n",
        "    merged = tf.summary.merge_all()\n",
        "    saver = tf.train.Saver(max_to_keep=30)\n",
        "    \n",
        "    batch_x_val, batch_y_val, batch_x_sequence_length_val, batch_y_sequence_length_val = next(get_batches(x_data_val, y_data_val, batch_size, 2,2))\n",
        " \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch< num_epochs: \n",
        "        epoch_loss = 0\n",
        "        epoch_loss_val =0\n",
        "        epoch_acc_agg =0\n",
        "        epoch_acc_agg_val =0\n",
        "        n_steps=0\n",
        "        \n",
        "        for batch, (batch_x, batch_y, batch_x_sequence_length, batch_y_sequence_length) in enumerate(get_batches(x_data, y_data, batch_size, 2,2)):\n",
        " \n",
        "#             start_batch = batch * batch_size\n",
        "#             end_batch = start_batch + batch_size\n",
        "\n",
        "#             batch_loss = 0\n",
        "#             batch_y_sequence_length=[]\n",
        "            \n",
        "#             batch_x = x_data[start_batch:end_batch]\n",
        "#             batch_y = y_data[start_batch:end_batch]\n",
        "            \n",
        "#             for y in batch_y:\n",
        "#                 batch_y_sequence_length.append(len(y))\n",
        "            \n",
        "#             batch_y_sequence_length = np.array(batch_y_sequence_length)\n",
        "            \n",
        "#             for _ in range(len(batch_x)):\n",
        "#                 x_length = len(batch_x[_])\n",
        "#                 y_length = len(batch_y[_])\n",
        "\n",
        "#                 batch_x[_].extend(np.zeros([X_MAX_LENGTH - x_length], dtype=int))\n",
        "#                 batch_y[_].extend(np.zeros([Y_MAX_LENGTH - y_length], dtype=int))\n",
        "            \n",
        "   \n",
        "#             batch_x_valid = x_data_val[start_batch:end_batch]\n",
        "#             batch_y_valid = y_data_val[start_batch:end_batch]\n",
        "        \n",
        "#             y_one_hot = one_hot(batch_y, Y_MAX_LENGTH, y_word_to_idx)\n",
        "#             y_one_hot_valid = one_hot(batch_y_valid, Y_MAX_LENGTH, y_word_to_idx)\n",
        "            \n",
        "  \n",
        "            \n",
        "            opt = sess.run(optimizer, feed_dict={encoder_x:batch_x,  decoder_x:batch_y,   y_sequence_length: batch_y_sequence_length })\n",
        "            batch_loss  = sess.run([total_cost], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,   y_sequence_length: batch_y_sequence_length })     \n",
        "            logits, logits_max = sess.run([train_logits, train_logits_max], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,   y_sequence_length: batch_y_sequence_length })    \n",
        "            #_logits = np.argmax(logits, axis=2)\n",
        "            \n",
        "            accuracy = get_accuracy(logits_max, batch_y)\n",
        "            #print(accuracy)\n",
        "            \n",
        "            \n",
        "            \n",
        "            #opt = sess.run(optimizer, feed_dict={encoder_x: x_data_valid, decoder_x: y_data_valid, y: y_one_hot_valid})\n",
        "            batch_loss_val = sess.run([total_cost], feed_dict={encoder_x:  batch_x_val, decoder_x:  batch_y_val, y_sequence_length: batch_y_sequence_length_val })     \n",
        "            logits_val = sess.run([infer_output], feed_dict={encoder_x:  batch_x_val, decoder_x:  batch_y_val, y_sequence_length: batch_y_sequence_length_val })\n",
        "            #print(np.shape(logits_val))\n",
        "            \n",
        "            accuracy_val = get_accuracy(logits_val, batch_y_val)\n",
        "            \n",
        "            if batch % 10 == 0:\n",
        "\n",
        "#                    save_output_path = save_dir + 'TRAINING_TRANSLATION_OUTPUT_EPOCH_{}_STATE-SIZE_{}_NUM-LAYERS_{}_LEARNING-RATE{}_EMBEDDING-SIZE_{}.txt'.format(epoch, state_size, decoder_layers, lr, inembsize)\n",
        "                      text = lstm_support.generate_text(_prediction_series, batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE, y_idx_to_word)\n",
        "  \n",
        "                      print('Batch {}/{} Training Loss: {}, Training Accuracy: {} '.format(batch+1, num_batches, batch_loss[0], accuracy))\n",
        "                      print('Batch {}/{} Validation Loss: {}, Validation Accuracy: {} '.format(batch+1, num_batches, batch_loss_val[0], accuracy_val))\n",
        "\n",
        "            epoch_loss += batch_loss[0]\n",
        "            epoch_loss_val += batch_loss_val[0]\n",
        "            epoch_acc_agg += accuracy\n",
        "            epoch_acc_agg_val += accuracy_val\n",
        "            n_steps+=1\n",
        "            \n",
        "        print('Epoch {} Training Loss: {}, Training Accuracy: {} '.format(epoch+1, epoch_loss, epoch_acc_agg/n_steps))\n",
        "        print('Epoch {} Validation Loss: {}, Training Accuracy: {} '.format(epoch+1, epoch_loss_val, epoch_acc_agg_val/n_steps))\n",
        "        \n",
        "        if epoch == 0:\n",
        "            path = saver.save(sess, save_dir, epoch)\n",
        "        else:\n",
        "            path = saver.save(sess, save_dir, epoch, write_meta_graph=False)\n",
        "\n",
        "        print(\"Saved in: %s\" % path)\n",
        "        epoch+=1"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1/102 Training Loss: 3.299989700317383, Training Accuracy: 0.8375756048387096 \n",
            "Batch 1/102 Validation Loss: 3.1928985118865967, Validation Accuracy: 0.8949092741935484 \n",
            "Batch 11/102 Training Loss: 0.701086699962616, Training Accuracy: 0.8602570564516129 \n",
            "Batch 11/102 Validation Loss: 0.4752120077610016, Validation Accuracy: 0.8961693548387096 \n",
            "Batch 21/102 Training Loss: 0.6003575325012207, Training Accuracy: 0.8612651209677419 \n",
            "Batch 21/102 Validation Loss: 0.44034773111343384, Validation Accuracy: 0.8989415322580645 \n",
            "Batch 31/102 Training Loss: 0.5613174438476562, Training Accuracy: 0.859375 \n",
            "Batch 31/102 Validation Loss: 0.41438376903533936, Validation Accuracy: 0.8971774193548387 \n",
            "Batch 41/102 Training Loss: 0.48603853583335876, Training Accuracy: 0.8731098790322581 \n",
            "Batch 41/102 Validation Loss: 0.37895667552948, Validation Accuracy: 0.8931451612903226 \n",
            "Batch 51/102 Training Loss: 0.49619928002357483, Training Accuracy: 0.8762600806451613 \n",
            "Batch 51/102 Validation Loss: 0.3689720928668976, Validation Accuracy: 0.8940272177419355 \n",
            "Batch 61/102 Training Loss: 0.5043936371803284, Training Accuracy: 0.8734879032258065 \n",
            "Batch 61/102 Validation Loss: 0.3571006655693054, Validation Accuracy: 0.007560483870967742 \n",
            "Batch 71/102 Training Loss: 0.4509247839450836, Training Accuracy: 0.8886088709677419 \n",
            "Batch 71/102 Validation Loss: 0.35309135913848877, Validation Accuracy: 0.005670362903225807 \n",
            "Batch 81/102 Training Loss: 0.46911153197288513, Training Accuracy: 0.8858366935483871 \n",
            "Batch 81/102 Validation Loss: 0.3383640646934509, Validation Accuracy: 0.9053679435483871 \n",
            "Batch 91/102 Training Loss: 0.4444735646247864, Training Accuracy: 0.8859627016129032 \n",
            "Batch 91/102 Validation Loss: 0.31908971071243286, Validation Accuracy: 0.899445564516129 \n",
            "Batch 101/102 Training Loss: 0.4044014811515808, Training Accuracy: 0.8942792338709677 \n",
            "Batch 101/102 Validation Loss: 0.2943445146083832, Validation Accuracy: 0.8664314516129032 \n",
            "Epoch 1 Training Loss: 57.851822942495346, Training Accuracy: 0.872996224699557 \n",
            "Epoch 1 Validation Loss: 43.099956542253494, Training Accuracy: 0.6459049849778618 \n",
            "Saved in: drive/My Drive/data_PA3/-0\n",
            "Batch 1/102 Training Loss: 0.4227254390716553, Training Accuracy: 0.890625 \n",
            "Batch 1/102 Validation Loss: 0.2879718542098999, Validation Accuracy: 0.014616935483870967 \n",
            "Batch 11/102 Training Loss: 0.3560228645801544, Training Accuracy: 0.9085181451612904 \n",
            "Batch 11/102 Validation Loss: 0.26981765031814575, Validation Accuracy: 0.34513608870967744 \n",
            "Batch 21/102 Training Loss: 0.3070451617240906, Training Accuracy: 0.9246471774193549 \n",
            "Batch 21/102 Validation Loss: 0.24671167135238647, Validation Accuracy: 0.010080645161290322 \n",
            "Batch 31/102 Training Loss: 0.27543726563453674, Training Accuracy: 0.9343497983870968 \n",
            "Batch 31/102 Validation Loss: 0.21625865995883942, Validation Accuracy: 0.007560483870967742 \n",
            "Batch 41/102 Training Loss: 0.2210378348827362, Training Accuracy: 0.9501008064516129 \n",
            "Batch 41/102 Validation Loss: 0.17987924814224243, Validation Accuracy: 0.027091733870967742 \n",
            "Batch 51/102 Training Loss: 0.19814860820770264, Training Accuracy: 0.9589213709677419 \n",
            "Batch 51/102 Validation Loss: 0.14891459047794342, Validation Accuracy: 0.3293850806451613 \n",
            "Batch 61/102 Training Loss: 0.16059724986553192, Training Accuracy: 0.9671118951612904 \n",
            "Batch 61/102 Validation Loss: 0.11709295958280563, Validation Accuracy: 0.23714717741935484 \n",
            "Batch 71/102 Training Loss: 0.1113143116235733, Training Accuracy: 0.9819808467741935 \n",
            "Batch 71/102 Validation Loss: 0.09295805543661118, Validation Accuracy: 0.13659274193548387 \n",
            "Batch 81/102 Training Loss: 0.09686949104070663, Training Accuracy: 0.9831149193548387 \n",
            "Batch 81/102 Validation Loss: 0.06755509227514267, Validation Accuracy: 0.4604334677419355 \n",
            "Batch 91/102 Training Loss: 0.07903847098350525, Training Accuracy: 0.9862651209677419 \n",
            "Batch 91/102 Validation Loss: 0.050103381276130676, Validation Accuracy: 0.5457409274193549 \n",
            "Batch 101/102 Training Loss: 0.056866224855184555, Training Accuracy: 0.9920614919354839 \n",
            "Batch 101/102 Validation Loss: 0.03755553811788559, Validation Accuracy: 0.5452368951612904 \n",
            "Epoch 2 Training Loss: 20.801421362906694, Training Accuracy: 0.9537995137571156 \n",
            "Epoch 2 Validation Loss: 15.746517460793257, Training Accuracy: 0.32885387017710316 \n",
            "Saved in: drive/My Drive/data_PA3/-1\n",
            "Batch 1/102 Training Loss: 0.05515199527144432, Training Accuracy: 0.9919354838709677 \n",
            "Batch 1/102 Validation Loss: 0.03475244343280792, Validation Accuracy: 0.44455645161290325 \n",
            "Batch 11/102 Training Loss: 0.043526701629161835, Training Accuracy: 0.9942036290322581 \n",
            "Batch 11/102 Validation Loss: 0.02691137045621872, Validation Accuracy: 0.5016381048387096 \n",
            "Batch 21/102 Training Loss: 0.029910871759057045, Training Accuracy: 0.9964717741935484 \n",
            "Batch 21/102 Validation Loss: 0.02109084278345108, Validation Accuracy: 0.45992943548387094 \n",
            "Batch 31/102 Training Loss: 0.02546735666692257, Training Accuracy: 0.9976058467741935 \n",
            "Batch 31/102 Validation Loss: 0.01635880582034588, Validation Accuracy: 0.2469758064516129 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-395700b06458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mencoder_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdecoder_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0my_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y_sequence_length\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mbatch_loss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_cost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mencoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdecoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0my_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y_sequence_length\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_logits_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mencoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdecoder_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0my_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y_sequence_length\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N8EeqiOjE2Rs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tf_graph = tf.Graph()\n",
        "# with tf_graph.as_default():\n",
        "  \n",
        "#   encoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) #[batch_size, X_MAX_LENGTH]\n",
        "#   decoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) #[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "#   y = tf.placeholder(dtype=tf.float32, shape=[None, None, None])#[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "  \n",
        "#   with tf.variable_scope('encoder_word_embeddings'):\n",
        "\n",
        "#       word_embeddings = tf.get_variable('encoder_word_embeddings', [X_VOCAB_SIZE, inembsize])\n",
        "#       encoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, encoder_x)\n",
        "#       encoder_embedded_word_ids = tf.reshape(encoder_embedded_word_ids, [-1, X_MAX_LENGTH, inembsize])\n",
        "\n",
        "#   with tf.variable_scope('encoder'):\n",
        "\n",
        "#       for n in range(encoder_layers): \n",
        "#                 encoder_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "#                 encoder_out, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
        "#                 cell_fw = encoder_cell,  ## initializer, reuse to be set\n",
        "#                 cell_bw = encoder_cell,  ## initializer, reuse to be set\n",
        "#                 inputs = encoder_embedded_word_ids,\n",
        "#                 dtype = tf.float32)\n",
        "\n",
        "#   #           (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
        "#   #               cell_fw = tf.nn.rnn_cell.LSTMCell(state_size//2, state_is_tuple=True),  ## initializer, reuse to be set\n",
        "#   #               cell_bw = tf.nn.rnn_cell.LSTMCell(state_size//2, state_is_tuple=True),  ## initializer, reuse to be set\n",
        "#   #               inputs = encoder_embedded_word_ids,\n",
        "\n",
        "#   #               sequence_length =  [61]* batch_size,\n",
        "#   #               dtype = tf.float32)\n",
        "\n",
        "#   #           encoder_embedded_word_ids = tf.concat((out_fw, out_bw), 2)\n",
        "\n",
        "#   #           bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n",
        "#   #           bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n",
        "#   #           bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
        "#   #           encoder_state = tuple([bi_lstm_state] * encoder_layers)\n",
        "\n",
        "#   #           encoder_state = tuple(encoder_state[-1] for _ in range(encoder_layers))\n",
        "\n",
        "\n",
        "#   with tf.variable_scope('decoder_word_embeddings'):\n",
        "\n",
        "#       word_embeddings = tf.get_variable('decoder_word_embeddings', [Y_VOCAB_SIZE, outembsize])\n",
        "#       decoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, decoder_x)\n",
        "#       decoder_embedded_word_ids = tf.reshape(decoder_embedded_word_ids,  [-1, Y_MAX_LENGTH,  outembsize])\n",
        "\n",
        "\n",
        "\n",
        "#   with tf.variable_scope('decoder'):\n",
        "\n",
        "#       decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(state_size) for _ in range(decoder_layers)])\n",
        "#       dense_layer = tf.layers.Dense(Y_VOCAB_SIZE)\n",
        "\n",
        "#       #decoder for training data\n",
        "#       training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embedded_word_ids, sequence_length = [62]* batch_size)\n",
        "\n",
        "\n",
        "\n",
        "#       training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "#               cell = decoder_cell,\n",
        "#               helper = training_helper,\n",
        "#               initial_state = encoder_state,\n",
        "#               output_layer = dense_layer)\n",
        "\n",
        "#       training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "#               decoder = training_decoder,\n",
        "#               impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further\n",
        "#               maximum_iterations = Y_MAX_LENGTH)   \n",
        "      \n",
        "#       training_logits = training_decoder_output.rnn_output\n",
        "#       softmax_out = tf.nn.softmax(training_logits)\n",
        "      \n",
        "#       cost =  tf.nn.softmax_cross_entropy_with_logits(logits=training_logits, labels=y)\n",
        "\n",
        "#       total_cost = tf.reduce_mean(cost)\n",
        "#       optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost)\n",
        "\n",
        "#       y_t = tf.argmax(softmax_out,axis=2)\n",
        "#       y_t = tf.cast(y_t, tf.int32)\n",
        "\n",
        "#       #prediction = tf.boolean_mask(y_t, masks)\n",
        "#       prediction = y_t\n",
        "#       #mask_label = tf.boolean_mask(decoder_x, masks)\n",
        "#       mask_label = decoder_x\n",
        "\n",
        "#       correct_pred = tf.equal(prediction, mask_label)\n",
        "#       accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "           \n",
        "#   with tf.variable_scope('decoder', reuse=True):   \n",
        "#       #decoder for test data\n",
        "#       predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding = word_embeddings,\n",
        "#               start_tokens = tf.fill([batch_size], y_word_to_idx['GO']), end_token = y_word_to_idx['EOS'])\n",
        "                           \n",
        "#       predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "#               cell = decoder_cell,\n",
        "#               helper = predicting_helper,\n",
        "#               initial_state = encoder_state,\n",
        "#               output_layer = dense_layer)\n",
        "\n",
        "#       predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "#               decoder = predicting_decoder,\n",
        "#               impute_finished = True,\n",
        "#               maximum_iterations = 2 * (X_MAX_LENGTH))\n",
        "\n",
        "#       predicting_ids = predicting_decoder_output.sample_id\n",
        "      \n",
        "#       #predicting_ids = tf.cast(predicting_ids, tf.float32)\n",
        "#       #softmax_out_val = tf.nn.softmax(predicting_ids)\n",
        "# #       print(predicting_ids)\n",
        "      \n",
        "# #       cost_val =  tf.nn.softmax_cross_entropy_with_logits(logits=predicting_ids, labels=y)\n",
        "\n",
        "# #       total_cost_val = tf.reduce_mean(cost_val)\n",
        "#       #optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost_val)\n",
        "      \n",
        "#       #y_t_val = tf.argmax(predicting_ids,axis=2)\n",
        "#       #y_t_val = tf.cast(y_t_val, tf.int32)\n",
        "\n",
        "#       #prediction = tf.boolean_mask(y_t, masks)\n",
        "# #       prediction_val = predicting_ids\n",
        "      \n",
        "# #       prediction_val = tf.cast(prediction_val, tf.int32)\n",
        "# #       #mask_label = tf.boolean_mask(decoder_x, masks)\n",
        "      \n",
        "# #       mask_label_val = decoder_x\n",
        "\n",
        "# #       correct_pred_val= tf.equal(prediction_val, mask_label_val)\n",
        "# #       accuracy_val = tf.reduce_mean(tf.cast(correct_pred_val, tf.float32))\n",
        "      \n",
        "      \n",
        "#       #masks = tf.sequence_mask(tf.count_nonzero(decoder_x, 1, dtype=tf.int32), Y_MAX_LENGTH, dtype=tf.float32)  \n",
        "\n",
        "#       #cost = tf.contrib.seq2seq.sequence_loss(logits = softmax_out,targets = decoder_x)\n",
        "\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJenXzc7e901",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}