{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PA3_Final2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SC8_UGCBoV0U",
        "colab_type": "code",
        "outputId": "38f41489-f6fd-4c38-c934-040003224a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zubJkNs2oZ3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzxGwBy2obr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "train = pd.read_csv('/content/gdrive/My Drive/train.csv')\n",
        "valid = pd.read_csv('/content/gdrive/My Drive/valid.csv')\n",
        "test = pd.read_csv('/content/gdrive/My Drive/test_final.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Crk_55cppWLA",
        "colab_type": "code",
        "outputId": "4d607854-83a9-43a3-896b-624e63d9c0aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the unique values of the source as well as the target\n",
        "\n",
        "# Finding out the vocuabulary of the english words\n",
        "character_english = []\n",
        "for i in range(len(train)):\n",
        "  w = train['ENG'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_english.append(w[j])\n",
        "for i in range(len(valid)):\n",
        "  w = valid['ENG'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_english.append(w[j])\n",
        "for i in range(len(test)):\n",
        "  w = test['ENG'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_english.append(w[j])\n",
        "\n",
        "print('The number of unique English characters is %d' %len(np.unique(character_english)))  \n",
        "\n",
        "english_vocab = np.unique(character_english)\n",
        "\n",
        "# Finding out the vocuabulary of the Hindi words\n",
        "character_hindi = []\n",
        "for i in range(len(train)):\n",
        "  w = train['HIN'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_hindi.append(w[j])\n",
        "for i in range(len(valid)):\n",
        "  w = valid['HIN'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_hindi.append(w[j])\n",
        "\n",
        "print('The number of unique Hindi characters is %d' %len(np.unique(character_hindi)))  \n",
        "\n",
        "hindi_vocab = np.unique(character_hindi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of unique English characters is 43\n",
            "The number of unique Hindi characters is 83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PsgreH1MoqS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Data Preprocessing\n",
        "\n",
        "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
        "\n",
        "def create_lookup_tables(vocabulary): # vocabulary contains the vocabularies created above\n",
        "    # make a list of unique words\n",
        "    #vocab = set(text.split())\n",
        "    vocab = vocabulary\n",
        "    # (1)\n",
        "    # starts with the special tokens\n",
        "    vocab_to_int = copy.copy(CODES)\n",
        "\n",
        "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
        "    # since vocab_to_int already contains special tokens\n",
        "    for v_i, v in enumerate(vocab, len(CODES)):\n",
        "        vocab_to_int[v] = v_i\n",
        "\n",
        "    # (2)\n",
        "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
        "\n",
        "    return vocab_to_int, int_to_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j7jyJBQ8q3tS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
        "    \"\"\"\n",
        "        1st, 2nd args: raw string text to be converted\n",
        "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
        "    \n",
        "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
        "    \"\"\"\n",
        "    # empty list of converted words\n",
        "    source_text_id = []\n",
        "    target_text_id = []\n",
        "    \n",
        "    # make a list of words (extraction)\n",
        "    source_words = source_text #source_text.split(\"\\n\")\n",
        "    target_words = target_text #target_text.split(\"\\n\")\n",
        "    \n",
        "    max_source_word_length = max([len(word.split(\" \")) for word in source_words])\n",
        "    max_target_word_length = max([len(word.split(\" \")) for word in target_words])\n",
        "    \n",
        "    # iterating through each word \n",
        "    for i in range(len(source_words)):\n",
        "        # extract words one by one\n",
        "        source_word = source_words[i]\n",
        "        target_word = target_words[i]\n",
        "        \n",
        "        # make a list of characters (extraction) from the chosen word\n",
        "        source_char = source_word.split(\" \")\n",
        "        target_char = target_word.split(\" \")\n",
        "        \n",
        "        # empty list of converted characters to index in the chosen word\n",
        "        source_char_id = []\n",
        "        target_char_id = []\n",
        "        \n",
        "        for index, token in enumerate(source_char):\n",
        "            if (token != \"\"):\n",
        "                source_char_id.append(source_vocab_to_int[token])\n",
        "        \n",
        "        for index, token in enumerate(target_char):\n",
        "            if (token != \"\"):\n",
        "                target_char_id.append(target_vocab_to_int[token])\n",
        "                \n",
        "        # put <EOS> token at the end of the chosen target word\n",
        "        # this token suggests when to stop creating a sequence\n",
        "        target_char_id.append(target_vocab_to_int['<EOS>'])\n",
        "            \n",
        "        # add each converted word in the final list\n",
        "        source_text_id.append(source_char_id)\n",
        "        target_text_id.append(target_char_id)\n",
        "    \n",
        "    return source_text_id, target_text_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXuh3a_8r7TJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data Pre-processing\n",
        "# create lookup tables for English and Hindi data\n",
        "source_vocab_to_int, source_int_to_vocab = create_lookup_tables(english_vocab)#(source_text)\n",
        "target_vocab_to_int, target_int_to_vocab = create_lookup_tables(hindi_vocab)#(target_text)\n",
        "\n",
        "# create list of words whose characters are represented in index\n",
        "source_text, target_text = text_to_ids(train['ENG'], train['HIN'], create_lookup_tables(english_vocab)[0], create_lookup_tables(hindi_vocab)[0])#(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-XB_KxJODx-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "source_int_text, target_int_text = source_text, target_text\n",
        "source_vocab_to_int, target_vocab_to_int = source_vocab_to_int, target_vocab_to_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z4qsajYjN7D2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Building the Neural Network\n",
        "\n",
        "def enc_dec_model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
        "    \n",
        "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
        "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
        "    \n",
        "    return inputs, targets, target_sequence_length, max_target_len\n",
        "  \n",
        "def hyperparam_inputs():\n",
        "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    \n",
        "    return lr_rate, keep_prob  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VZnk7rgHcb46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display_step = 300\n",
        "\n",
        "epochs = 13\n",
        "batch_size = 128\n",
        "\n",
        "rnn_size = 512\n",
        "num_layers = 2\n",
        "#num_layers2 = 2\n",
        "encoding_embedding_size = 256\n",
        "decoding_embedding_size = 256\n",
        "\n",
        "learning_rate = 0.001\n",
        "keep_probability = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "78t5uVI9RhjR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
        "    \"\"\"\n",
        "    Preprocess target data for encoding\n",
        "    :return: Preprocessed target data\n",
        "    \"\"\"\n",
        "    # get '<GO>' id\n",
        "    go_id = target_vocab_to_int['<GO>']\n",
        "    \n",
        "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
        "    \n",
        "    return after_concat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O5MDjmHJRvkF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
        "                   source_vocab_size, \n",
        "                   encoding_embedding_size):\n",
        "    \"\"\"\n",
        "    :return: tuple (RNN output, RNN state)\n",
        "    \"\"\"\n",
        "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
        "                                             vocab_size=source_vocab_size, \n",
        "                                             embed_dim=encoding_embedding_size)\n",
        "    \n",
        "    #stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob)  for _ in range(num_layers)])\n",
        "    \n",
        "    stacked_cells = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
        "    \n",
        "    outputs, state = tf.nn.bidirectional_dynamic_rnn(stacked_cells, stacked_cells, embed, dtype=tf.float32)\n",
        "    return outputs, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pKbYwkVU5DZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "attention = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "un6jCM5QRxX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
        "                         target_sequence_length, max_summary_length, \n",
        "                         output_layer, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a training process in decoding layer \n",
        "    :return: BasicDecoderOutput containing training logits and sample_id\n",
        "    \"\"\"\n",
        "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
        "                                             output_keep_prob=keep_prob)\n",
        "    \n",
        "    # for only input layer\n",
        "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
        "                                               target_sequence_length)\n",
        "    if attention == 1:\n",
        "      # Define attention mechanism\n",
        "      attn_mech = tf.contrib.seq2seq.LuongMonotonicAttention(\n",
        "      num_units = rnn_size, memory = [batch_size, max_target_sequence_length, rnn_size])\n",
        "#       memory_sequence_length = target_sequence_length)\n",
        "\n",
        "      # Define attention cell\n",
        "      attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "      cell = dec_cell, attention_mechanism = attn_mech,\n",
        "      alignment_history=True)\n",
        "\n",
        "\n",
        "      decoder = tf.contrib.seq2seq.BasicDecoder(attn_cell, \n",
        "                                                helper, \n",
        "                                                encoder_state, \n",
        "                                                output_layer)\n",
        "\n",
        "    else:\n",
        "      decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
        "                                                helper, \n",
        "                                                encoder_state, \n",
        "                                                output_layer)\n",
        "\n",
        "    \n",
        "    # unrolling the decoder layer\n",
        "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
        "                                                      impute_finished=True, \n",
        "                                                      maximum_iterations=max_summary_length)\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BMksCUnKR17Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
        "                         end_of_sequence_id, max_target_sequence_length,\n",
        "                         vocab_size, output_layer, batch_size, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a inference process in decoding layer \n",
        "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
        "    \"\"\"\n",
        "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
        "                                             output_keep_prob=keep_prob)\n",
        "    \n",
        "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
        "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
        "                                                      end_of_sequence_id)\n",
        "    \n",
        "    if attention == 1:\n",
        "      # Define attention mechanism\n",
        "      attn_mech = tf.contrib.seq2seq.LuongMonotonicAttention(\n",
        "      num_units = rnn_size, memory = [batch_size, max_target_sequence_length, rnn_size])\n",
        "#       memory_sequence_length = target_sequence_length)\n",
        "\n",
        "      # Define attention cell\n",
        "      attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "      cell = dec_cell, attention_mechanism = attn_mech,\n",
        "      alignment_history=True)\n",
        "\n",
        "\n",
        "      decoder = tf.contrib.seq2seq.BasicDecoder(attn_cell, \n",
        "                                                helper, \n",
        "                                                encoder_state, \n",
        "                                                output_layer)\n",
        "\n",
        "    else:\n",
        "      decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
        "                                                helper, \n",
        "                                                encoder_state, \n",
        "                                                output_layer)\n",
        "    \n",
        "#     decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
        "#                                               helper, \n",
        "#                                               encoder_state, \n",
        "#                                               output_layer)\n",
        "    \n",
        "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
        "                                                      impute_finished=True, \n",
        "                                                      maximum_iterations=max_target_sequence_length)\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xPALY6HAR5l9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer(dec_input, encoder_state,\n",
        "                   target_sequence_length, max_target_sequence_length,\n",
        "                   rnn_size,\n",
        "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
        "                   batch_size, keep_prob, decoding_embedding_size):\n",
        "    \"\"\"\n",
        "    Create decoding layer\n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"\n",
        "    target_vocab_size = len(target_vocab_to_int)\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "    \n",
        "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
        "    \n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        output_layer = tf.layers.Dense(target_vocab_size)\n",
        "        train_output = decoding_layer_train(encoder_state, \n",
        "                                            cells, \n",
        "                                            dec_embed_input, \n",
        "                                            target_sequence_length, \n",
        "                                            max_target_sequence_length, \n",
        "                                            output_layer, \n",
        "                                            keep_prob)\n",
        "\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        infer_output = decoding_layer_infer(encoder_state, \n",
        "                                            cells, \n",
        "                                            dec_embeddings, \n",
        "                                            target_vocab_to_int['<GO>'], \n",
        "                                            target_vocab_to_int['<EOS>'], \n",
        "                                            max_target_sequence_length, \n",
        "                                            target_vocab_size, \n",
        "                                            output_layer,\n",
        "                                            batch_size,\n",
        "                                            keep_prob)\n",
        "\n",
        "    return (train_output, infer_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LugY0hr8R9Xn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
        "                  target_sequence_length,\n",
        "                  max_target_word_length,\n",
        "                  source_vocab_size, target_vocab_size,\n",
        "                  enc_embedding_size, dec_embedding_size,\n",
        "                  rnn_size, num_layers, target_vocab_to_int):\n",
        "    \"\"\"\n",
        "    Build the Sequence-to-Sequence model\n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"\n",
        "    enc_outputs, enc_states = encoding_layer(input_data, \n",
        "                                             rnn_size, \n",
        "                                             num_layers, \n",
        "                                             keep_prob, \n",
        "                                             source_vocab_size, \n",
        "                                             enc_embedding_size)\n",
        "    \n",
        "    dec_input = process_decoder_input(target_data, \n",
        "                                      target_vocab_to_int, \n",
        "                                      batch_size)\n",
        "    \n",
        "    train_output, infer_output = decoding_layer(dec_input,\n",
        "                                               enc_states, \n",
        "                                               target_sequence_length, \n",
        "                                               max_target_word_length,\n",
        "                                               rnn_size,\n",
        "                                              num_layers,\n",
        "                                              target_vocab_to_int,\n",
        "                                              target_vocab_size,\n",
        "                                              batch_size,\n",
        "                                              keep_prob,\n",
        "                                              dec_embedding_size)\n",
        "    \n",
        "    return train_output, infer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g8Qf6AXDSW42",
        "colab_type": "code",
        "outputId": "9b5743a3-d92a-4faf-c663-303d15ec92b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "cell_type": "code",
      "source": [
        "#Build the graph\n",
        "\n",
        "max_target_word_length = max([len(word) for word in source_int_text])\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
        "    lr, keep_prob = hyperparam_inputs()\n",
        "    \n",
        "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                   targets,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size,\n",
        "                                                   target_sequence_length,\n",
        "                                                   max_target_sequence_length,\n",
        "                                                   len(source_vocab_to_int),\n",
        "                                                   len(target_vocab_to_int),\n",
        "                                                   encoding_embedding_size,\n",
        "                                                   decoding_embedding_size,\n",
        "                                                   rnn_size,\n",
        "                                                   num_layers,\n",
        "                                                   target_vocab_to_int)\n",
        "    \n",
        "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
        "    # - Returns a mask tensor representing the first N positions of each cell.\n",
        "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function - weighted softmax cross entropy\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-16-52fc95b70637>:13: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-16-52fc95b70637>:15: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From <ipython-input-25-9a2c83b42768>:14: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-25-9a2c83b42768>:14: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MTqSgWwoSvan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get the batches and pad the source and target sequences\n",
        "\n",
        "def pad_sentence_batch(sentence_batch, pad_int):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
        "\n",
        "\n",
        "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
        "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(sources)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "\n",
        "        # Slice the right amount for the batch\n",
        "        sources_batch = sources[start_i:start_i + batch_size]\n",
        "        targets_batch = targets[start_i:start_i + batch_size]\n",
        "\n",
        "        # Pad\n",
        "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
        "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_targets_lengths = []\n",
        "        for target in pad_targets_batch:\n",
        "            pad_targets_lengths.append(len(target))\n",
        "\n",
        "        pad_source_lengths = []\n",
        "        for source in pad_sources_batch:\n",
        "            pad_source_lengths.append(len(source))\n",
        "\n",
        "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O3DMsF5NTDjI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(target, logits):\n",
        "    \"\"\"\n",
        "    Calculate accuracy\n",
        "    \"\"\"\n",
        "    max_seq = max(target.shape[1], logits.shape[1])\n",
        "    if max_seq - target.shape[1]:\n",
        "        target = np.pad(\n",
        "            target,\n",
        "            [(0,0),(0,max_seq - target.shape[1])],\n",
        "            'constant')\n",
        "    if max_seq - logits.shape[1]:\n",
        "        logits = np.pad(\n",
        "            logits,\n",
        "            [(0,0),(0,max_seq - logits.shape[1])],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(target, logits))\n",
        "\n",
        "# Split data to training and validation sets\n",
        "train_source = source_int_text[batch_size:]\n",
        "train_target = target_int_text[batch_size:]\n",
        "valid_source = source_int_text[:batch_size]\n",
        "valid_target = target_int_text[:batch_size]\n",
        "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
        "                                                                                                             valid_target,\n",
        "                                                                                                             batch_size,\n",
        "                                                                                                             source_vocab_to_int['<PAD>'],\n",
        "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PcDWI61DX-RZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "display_step = 10\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJtelmgzWf4W",
        "colab_type": "code",
        "outputId": "3fcfa207-1612-4178-b64e-424302950f8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7422
        }
      },
      "cell_type": "code",
      "source": [
        "save_path = '/content/gdrive/My Drive/checkpoints/dev'\n",
        "\n",
        "\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        #print('Epoch 1')\n",
        "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(get_batches(train_source, train_target, batch_size,source_vocab_to_int['<PAD>'],target_vocab_to_int['<PAD>'])):\n",
        "\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: source_batch,\n",
        "                 targets: target_batch,\n",
        "                 lr: learning_rate,\n",
        "                 target_sequence_length: targets_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "           \n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                batch_train_logits = sess.run(\n",
        "                    inference_logits,\n",
        "                    {input_data: source_batch,\n",
        "                     target_sequence_length: targets_lengths,\n",
        "                     keep_prob: 1.0})\n",
        "\n",
        "                batch_valid_logits = sess.run(\n",
        "                    inference_logits,\n",
        "                    {input_data: valid_sources_batch,\n",
        "                     target_sequence_length: valid_targets_lengths,\n",
        "                     keep_prob: 1.0})\n",
        "\n",
        "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
        "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
        "\n",
        "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
        "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
        "\n",
        "#     test_logits = sess.run(inference_logits, \n",
        "#                           {input_data : })\n",
        "    \n",
        "    \n",
        "    \n",
        "    #Save Model\n",
        "    saver.save(sess, save_path)\n",
        "    print('Model Trained and Saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch   10/102 - Train Accuracy: 0.7727, Validation Accuracy: 0.7749, Loss: 1.2668\n",
            "Epoch   0 Batch   20/102 - Train Accuracy: 0.6815, Validation Accuracy: 0.7798, Loss: 1.4506\n",
            "Epoch   0 Batch   30/102 - Train Accuracy: 0.7279, Validation Accuracy: 0.7817, Loss: 1.1348\n",
            "Epoch   0 Batch   40/102 - Train Accuracy: 0.6268, Validation Accuracy: 0.7354, Loss: 1.3784\n",
            "Epoch   0 Batch   50/102 - Train Accuracy: 0.7203, Validation Accuracy: 0.7694, Loss: 1.0353\n",
            "Epoch   0 Batch   60/102 - Train Accuracy: 0.7329, Validation Accuracy: 0.7150, Loss: 0.8030\n",
            "Epoch   0 Batch   70/102 - Train Accuracy: 0.7299, Validation Accuracy: 0.7853, Loss: 1.0372\n",
            "Epoch   0 Batch   80/102 - Train Accuracy: 0.7572, Validation Accuracy: 0.7528, Loss: 0.7760\n",
            "Epoch   0 Batch   90/102 - Train Accuracy: 0.8040, Validation Accuracy: 0.7861, Loss: 0.7177\n",
            "Epoch   0 Batch  100/102 - Train Accuracy: 0.6344, Validation Accuracy: 0.7881, Loss: 1.3612\n",
            "Epoch   1 Batch   10/102 - Train Accuracy: 0.7868, Validation Accuracy: 0.7882, Loss: 0.7879\n",
            "Epoch   1 Batch   20/102 - Train Accuracy: 0.6888, Validation Accuracy: 0.7905, Loss: 1.1349\n",
            "Epoch   1 Batch   30/102 - Train Accuracy: 0.7401, Validation Accuracy: 0.7917, Loss: 0.9542\n",
            "Epoch   1 Batch   40/102 - Train Accuracy: 0.6635, Validation Accuracy: 0.7912, Loss: 1.2246\n",
            "Epoch   1 Batch   50/102 - Train Accuracy: 0.7489, Validation Accuracy: 0.7944, Loss: 0.9265\n",
            "Epoch   1 Batch   60/102 - Train Accuracy: 0.8026, Validation Accuracy: 0.7926, Loss: 0.7285\n",
            "Epoch   1 Batch   70/102 - Train Accuracy: 0.7328, Validation Accuracy: 0.7869, Loss: 0.9400\n",
            "Epoch   1 Batch   80/102 - Train Accuracy: 0.7867, Validation Accuracy: 0.7848, Loss: 0.7184\n",
            "Epoch   1 Batch   90/102 - Train Accuracy: 0.8077, Validation Accuracy: 0.7856, Loss: 0.6600\n",
            "Epoch   1 Batch  100/102 - Train Accuracy: 0.6400, Validation Accuracy: 0.7910, Loss: 1.2597\n",
            "Epoch   2 Batch   10/102 - Train Accuracy: 0.7876, Validation Accuracy: 0.7785, Loss: 0.7118\n",
            "Epoch   2 Batch   20/102 - Train Accuracy: 0.6893, Validation Accuracy: 0.7900, Loss: 1.0468\n",
            "Epoch   2 Batch   30/102 - Train Accuracy: 0.7266, Validation Accuracy: 0.7786, Loss: 0.8682\n",
            "Epoch   2 Batch   40/102 - Train Accuracy: 0.6424, Validation Accuracy: 0.7756, Loss: 1.0899\n",
            "Epoch   2 Batch   50/102 - Train Accuracy: 0.7299, Validation Accuracy: 0.7850, Loss: 0.8222\n",
            "Epoch   2 Batch   60/102 - Train Accuracy: 0.7764, Validation Accuracy: 0.7563, Loss: 0.6428\n",
            "Epoch   2 Batch   70/102 - Train Accuracy: 0.7318, Validation Accuracy: 0.7858, Loss: 0.8503\n",
            "Epoch   2 Batch   80/102 - Train Accuracy: 0.7886, Validation Accuracy: 0.7788, Loss: 0.6436\n",
            "Epoch   2 Batch   90/102 - Train Accuracy: 0.7959, Validation Accuracy: 0.7799, Loss: 0.6091\n",
            "Epoch   2 Batch  100/102 - Train Accuracy: 0.6378, Validation Accuracy: 0.7868, Loss: 1.1593\n",
            "Epoch   3 Batch   10/102 - Train Accuracy: 0.7924, Validation Accuracy: 0.7889, Loss: 0.6566\n",
            "Epoch   3 Batch   20/102 - Train Accuracy: 0.6607, Validation Accuracy: 0.7611, Loss: 0.9629\n",
            "Epoch   3 Batch   30/102 - Train Accuracy: 0.7148, Validation Accuracy: 0.7616, Loss: 0.8029\n",
            "Epoch   3 Batch   40/102 - Train Accuracy: 0.6508, Validation Accuracy: 0.7747, Loss: 1.0242\n",
            "Epoch   3 Batch   50/102 - Train Accuracy: 0.7158, Validation Accuracy: 0.7559, Loss: 0.7794\n",
            "Epoch   3 Batch   60/102 - Train Accuracy: 0.7879, Validation Accuracy: 0.7708, Loss: 0.6096\n",
            "Epoch   3 Batch   70/102 - Train Accuracy: 0.7365, Validation Accuracy: 0.7853, Loss: 0.7910\n",
            "Epoch   3 Batch   80/102 - Train Accuracy: 0.7916, Validation Accuracy: 0.7865, Loss: 0.5988\n",
            "Epoch   3 Batch   90/102 - Train Accuracy: 0.8012, Validation Accuracy: 0.7798, Loss: 0.5721\n",
            "Epoch   3 Batch  100/102 - Train Accuracy: 0.6331, Validation Accuracy: 0.7863, Loss: 1.0754\n",
            "Epoch   4 Batch   10/102 - Train Accuracy: 0.7894, Validation Accuracy: 0.7826, Loss: 0.6089\n",
            "Epoch   4 Batch   20/102 - Train Accuracy: 0.7063, Validation Accuracy: 0.7912, Loss: 0.9052\n",
            "Epoch   4 Batch   30/102 - Train Accuracy: 0.7523, Validation Accuracy: 0.7837, Loss: 0.7379\n",
            "Epoch   4 Batch   40/102 - Train Accuracy: 0.6653, Validation Accuracy: 0.7855, Loss: 0.9269\n",
            "Epoch   4 Batch   50/102 - Train Accuracy: 0.7455, Validation Accuracy: 0.7808, Loss: 0.7050\n",
            "Epoch   4 Batch   60/102 - Train Accuracy: 0.7894, Validation Accuracy: 0.7770, Loss: 0.5590\n",
            "Epoch   4 Batch   70/102 - Train Accuracy: 0.7448, Validation Accuracy: 0.7871, Loss: 0.7165\n",
            "Epoch   4 Batch   80/102 - Train Accuracy: 0.8114, Validation Accuracy: 0.8042, Loss: 0.5449\n",
            "Epoch   4 Batch   90/102 - Train Accuracy: 0.8180, Validation Accuracy: 0.7980, Loss: 0.4985\n",
            "Epoch   4 Batch  100/102 - Train Accuracy: 0.6625, Validation Accuracy: 0.8011, Loss: 0.9797\n",
            "Epoch   5 Batch   10/102 - Train Accuracy: 0.8045, Validation Accuracy: 0.7977, Loss: 0.5343\n",
            "Epoch   5 Batch   20/102 - Train Accuracy: 0.7068, Validation Accuracy: 0.7990, Loss: 0.7991\n",
            "Epoch   5 Batch   30/102 - Train Accuracy: 0.7594, Validation Accuracy: 0.7988, Loss: 0.6427\n",
            "Epoch   5 Batch   40/102 - Train Accuracy: 0.6989, Validation Accuracy: 0.8066, Loss: 0.7947\n",
            "Epoch   5 Batch   50/102 - Train Accuracy: 0.7754, Validation Accuracy: 0.8052, Loss: 0.6033\n",
            "Epoch   5 Batch   60/102 - Train Accuracy: 0.8234, Validation Accuracy: 0.8055, Loss: 0.4866\n",
            "Epoch   5 Batch   70/102 - Train Accuracy: 0.7760, Validation Accuracy: 0.8099, Loss: 0.6034\n",
            "Epoch   5 Batch   80/102 - Train Accuracy: 0.8217, Validation Accuracy: 0.8118, Loss: 0.4720\n",
            "Epoch   5 Batch   90/102 - Train Accuracy: 0.8421, Validation Accuracy: 0.8205, Loss: 0.4322\n",
            "Epoch   5 Batch  100/102 - Train Accuracy: 0.7100, Validation Accuracy: 0.8281, Loss: 0.8280\n",
            "Epoch   6 Batch   10/102 - Train Accuracy: 0.8278, Validation Accuracy: 0.8188, Loss: 0.4528\n",
            "Epoch   6 Batch   20/102 - Train Accuracy: 0.7484, Validation Accuracy: 0.8224, Loss: 0.6708\n",
            "Epoch   6 Batch   30/102 - Train Accuracy: 0.7833, Validation Accuracy: 0.8265, Loss: 0.5558\n",
            "Epoch   6 Batch   40/102 - Train Accuracy: 0.7440, Validation Accuracy: 0.8377, Loss: 0.6789\n",
            "Epoch   6 Batch   50/102 - Train Accuracy: 0.7855, Validation Accuracy: 0.8273, Loss: 0.4956\n",
            "Epoch   6 Batch   60/102 - Train Accuracy: 0.8488, Validation Accuracy: 0.8312, Loss: 0.4120\n",
            "Epoch   6 Batch   70/102 - Train Accuracy: 0.8010, Validation Accuracy: 0.8363, Loss: 0.4867\n",
            "Epoch   6 Batch   80/102 - Train Accuracy: 0.8430, Validation Accuracy: 0.8371, Loss: 0.4020\n",
            "Epoch   6 Batch   90/102 - Train Accuracy: 0.8582, Validation Accuracy: 0.8411, Loss: 0.3555\n",
            "Epoch   6 Batch  100/102 - Train Accuracy: 0.7419, Validation Accuracy: 0.8389, Loss: 0.7153\n",
            "Epoch   7 Batch   10/102 - Train Accuracy: 0.8424, Validation Accuracy: 0.8281, Loss: 0.3947\n",
            "Epoch   7 Batch   20/102 - Train Accuracy: 0.7688, Validation Accuracy: 0.8372, Loss: 0.5723\n",
            "Epoch   7 Batch   30/102 - Train Accuracy: 0.8139, Validation Accuracy: 0.8397, Loss: 0.4680\n",
            "Epoch   7 Batch   40/102 - Train Accuracy: 0.7599, Validation Accuracy: 0.8415, Loss: 0.5769\n",
            "Epoch   7 Batch   50/102 - Train Accuracy: 0.8116, Validation Accuracy: 0.8438, Loss: 0.4444\n",
            "Epoch   7 Batch   60/102 - Train Accuracy: 0.8478, Validation Accuracy: 0.8397, Loss: 0.3443\n",
            "Epoch   7 Batch   70/102 - Train Accuracy: 0.8156, Validation Accuracy: 0.8481, Loss: 0.4061\n",
            "Epoch   7 Batch   80/102 - Train Accuracy: 0.8549, Validation Accuracy: 0.8481, Loss: 0.3443\n",
            "Epoch   7 Batch   90/102 - Train Accuracy: 0.8737, Validation Accuracy: 0.8459, Loss: 0.3017\n",
            "Epoch   7 Batch  100/102 - Train Accuracy: 0.7634, Validation Accuracy: 0.8488, Loss: 0.5936\n",
            "Epoch   8 Batch   10/102 - Train Accuracy: 0.8566, Validation Accuracy: 0.8475, Loss: 0.3351\n",
            "Epoch   8 Batch   20/102 - Train Accuracy: 0.7893, Validation Accuracy: 0.8470, Loss: 0.4906\n",
            "Epoch   8 Batch   30/102 - Train Accuracy: 0.8316, Validation Accuracy: 0.8493, Loss: 0.4087\n",
            "Epoch   8 Batch   40/102 - Train Accuracy: 0.7740, Validation Accuracy: 0.8506, Loss: 0.4742\n",
            "Epoch   8 Batch   50/102 - Train Accuracy: 0.8335, Validation Accuracy: 0.8594, Loss: 0.3745\n",
            "Epoch   8 Batch   60/102 - Train Accuracy: 0.8636, Validation Accuracy: 0.8566, Loss: 0.2976\n",
            "Epoch   8 Batch   70/102 - Train Accuracy: 0.8383, Validation Accuracy: 0.8452, Loss: 0.3558\n",
            "Epoch   8 Batch   80/102 - Train Accuracy: 0.8635, Validation Accuracy: 0.8553, Loss: 0.3271\n",
            "Epoch   8 Batch   90/102 - Train Accuracy: 0.8723, Validation Accuracy: 0.8512, Loss: 0.2696\n",
            "Epoch   8 Batch  100/102 - Train Accuracy: 0.7747, Validation Accuracy: 0.8563, Loss: 0.5649\n",
            "Epoch   9 Batch   10/102 - Train Accuracy: 0.8711, Validation Accuracy: 0.8537, Loss: 0.3038\n",
            "Epoch   9 Batch   20/102 - Train Accuracy: 0.8229, Validation Accuracy: 0.8574, Loss: 0.4424\n",
            "Epoch   9 Batch   30/102 - Train Accuracy: 0.8408, Validation Accuracy: 0.8538, Loss: 0.3674\n",
            "Epoch   9 Batch   40/102 - Train Accuracy: 0.7939, Validation Accuracy: 0.8607, Loss: 0.4239\n",
            "Epoch   9 Batch   50/102 - Train Accuracy: 0.8556, Validation Accuracy: 0.8545, Loss: 0.3333\n",
            "Epoch   9 Batch   60/102 - Train Accuracy: 0.8761, Validation Accuracy: 0.8651, Loss: 0.2771\n",
            "Epoch   9 Batch   70/102 - Train Accuracy: 0.8482, Validation Accuracy: 0.8561, Loss: 0.3186\n",
            "Epoch   9 Batch   80/102 - Train Accuracy: 0.8669, Validation Accuracy: 0.8555, Loss: 0.2890\n",
            "Epoch   9 Batch   90/102 - Train Accuracy: 0.8763, Validation Accuracy: 0.8494, Loss: 0.2553\n",
            "Epoch   9 Batch  100/102 - Train Accuracy: 0.7847, Validation Accuracy: 0.8667, Loss: 0.4789\n",
            "Epoch  10 Batch   10/102 - Train Accuracy: 0.8728, Validation Accuracy: 0.8587, Loss: 0.2689\n",
            "Epoch  10 Batch   20/102 - Train Accuracy: 0.8201, Validation Accuracy: 0.8577, Loss: 0.3907\n",
            "Epoch  10 Batch   30/102 - Train Accuracy: 0.8497, Validation Accuracy: 0.8639, Loss: 0.3251\n",
            "Epoch  10 Batch   40/102 - Train Accuracy: 0.8227, Validation Accuracy: 0.8613, Loss: 0.3644\n",
            "Epoch  10 Batch   50/102 - Train Accuracy: 0.8623, Validation Accuracy: 0.8691, Loss: 0.3044\n",
            "Epoch  10 Batch   60/102 - Train Accuracy: 0.8716, Validation Accuracy: 0.8641, Loss: 0.2578\n",
            "Epoch  10 Batch   70/102 - Train Accuracy: 0.8633, Validation Accuracy: 0.8590, Loss: 0.2793\n",
            "Epoch  10 Batch   80/102 - Train Accuracy: 0.8728, Validation Accuracy: 0.8628, Loss: 0.2624\n",
            "Epoch  10 Batch   90/102 - Train Accuracy: 0.8923, Validation Accuracy: 0.8647, Loss: 0.2266\n",
            "Epoch  10 Batch  100/102 - Train Accuracy: 0.8000, Validation Accuracy: 0.8667, Loss: 0.4592\n",
            "Epoch  11 Batch   10/102 - Train Accuracy: 0.8821, Validation Accuracy: 0.8644, Loss: 0.2442\n",
            "Epoch  11 Batch   20/102 - Train Accuracy: 0.8273, Validation Accuracy: 0.8678, Loss: 0.3591\n",
            "Epoch  11 Batch   30/102 - Train Accuracy: 0.8580, Validation Accuracy: 0.8711, Loss: 0.2952\n",
            "Epoch  11 Batch   40/102 - Train Accuracy: 0.8260, Validation Accuracy: 0.8651, Loss: 0.3404\n",
            "Epoch  11 Batch   50/102 - Train Accuracy: 0.8737, Validation Accuracy: 0.8678, Loss: 0.2696\n",
            "Epoch  11 Batch   60/102 - Train Accuracy: 0.8848, Validation Accuracy: 0.8631, Loss: 0.2408\n",
            "Epoch  11 Batch   70/102 - Train Accuracy: 0.8786, Validation Accuracy: 0.8709, Loss: 0.2489\n",
            "Epoch  11 Batch   80/102 - Train Accuracy: 0.8742, Validation Accuracy: 0.8657, Loss: 0.2376\n",
            "Epoch  11 Batch   90/102 - Train Accuracy: 0.8938, Validation Accuracy: 0.8717, Loss: 0.2137\n",
            "Epoch  11 Batch  100/102 - Train Accuracy: 0.8328, Validation Accuracy: 0.8654, Loss: 0.3873\n",
            "Epoch  12 Batch   10/102 - Train Accuracy: 0.8852, Validation Accuracy: 0.8573, Loss: 0.2281\n",
            "Epoch  12 Batch   20/102 - Train Accuracy: 0.8401, Validation Accuracy: 0.8682, Loss: 0.3141\n",
            "Epoch  12 Batch   30/102 - Train Accuracy: 0.8734, Validation Accuracy: 0.8735, Loss: 0.2789\n",
            "Epoch  12 Batch   40/102 - Train Accuracy: 0.8639, Validation Accuracy: 0.8695, Loss: 0.3099\n",
            "Epoch  12 Batch   50/102 - Train Accuracy: 0.8819, Validation Accuracy: 0.8700, Loss: 0.2573\n",
            "Epoch  12 Batch   60/102 - Train Accuracy: 0.8937, Validation Accuracy: 0.8660, Loss: 0.2176\n",
            "Epoch  12 Batch   70/102 - Train Accuracy: 0.8938, Validation Accuracy: 0.8714, Loss: 0.2329\n",
            "Epoch  12 Batch   80/102 - Train Accuracy: 0.8820, Validation Accuracy: 0.8708, Loss: 0.2376\n",
            "Epoch  12 Batch   90/102 - Train Accuracy: 0.9051, Validation Accuracy: 0.8706, Loss: 0.2019\n",
            "Epoch  12 Batch  100/102 - Train Accuracy: 0.8350, Validation Accuracy: 0.8742, Loss: 0.3744\n",
            "Epoch  13 Batch   10/102 - Train Accuracy: 0.8973, Validation Accuracy: 0.8683, Loss: 0.2185\n",
            "Epoch  13 Batch   20/102 - Train Accuracy: 0.8458, Validation Accuracy: 0.8735, Loss: 0.3042\n",
            "Epoch  13 Batch   30/102 - Train Accuracy: 0.8750, Validation Accuracy: 0.8701, Loss: 0.2525\n",
            "Epoch  13 Batch   40/102 - Train Accuracy: 0.8831, Validation Accuracy: 0.8745, Loss: 0.2600\n",
            "Epoch  13 Batch   50/102 - Train Accuracy: 0.8850, Validation Accuracy: 0.8726, Loss: 0.2357\n",
            "Epoch  13 Batch   60/102 - Train Accuracy: 0.9014, Validation Accuracy: 0.8748, Loss: 0.1998\n",
            "Epoch  13 Batch   70/102 - Train Accuracy: 0.8992, Validation Accuracy: 0.8774, Loss: 0.2130\n",
            "Epoch  13 Batch   80/102 - Train Accuracy: 0.8852, Validation Accuracy: 0.8763, Loss: 0.2088\n",
            "Epoch  13 Batch   90/102 - Train Accuracy: 0.9006, Validation Accuracy: 0.8768, Loss: 0.1947\n",
            "Epoch  13 Batch  100/102 - Train Accuracy: 0.8394, Validation Accuracy: 0.8800, Loss: 0.3423\n",
            "Epoch  14 Batch   10/102 - Train Accuracy: 0.8990, Validation Accuracy: 0.8740, Loss: 0.1977\n",
            "Epoch  14 Batch   20/102 - Train Accuracy: 0.8646, Validation Accuracy: 0.8813, Loss: 0.2754\n",
            "Epoch  14 Batch   30/102 - Train Accuracy: 0.8796, Validation Accuracy: 0.8778, Loss: 0.2415\n",
            "Epoch  14 Batch   40/102 - Train Accuracy: 0.8840, Validation Accuracy: 0.8809, Loss: 0.2474\n",
            "Epoch  14 Batch   50/102 - Train Accuracy: 0.8949, Validation Accuracy: 0.8804, Loss: 0.2329\n",
            "Epoch  14 Batch   60/102 - Train Accuracy: 0.8981, Validation Accuracy: 0.8722, Loss: 0.1878\n",
            "Epoch  14 Batch   70/102 - Train Accuracy: 0.9096, Validation Accuracy: 0.8737, Loss: 0.1878\n",
            "Epoch  14 Batch   80/102 - Train Accuracy: 0.8986, Validation Accuracy: 0.8729, Loss: 0.2191\n",
            "Epoch  14 Batch   90/102 - Train Accuracy: 0.9079, Validation Accuracy: 0.8719, Loss: 0.1751\n",
            "Epoch  14 Batch  100/102 - Train Accuracy: 0.8647, Validation Accuracy: 0.8817, Loss: 0.3050\n",
            "Epoch  15 Batch   10/102 - Train Accuracy: 0.9044, Validation Accuracy: 0.8750, Loss: 0.1976\n",
            "Epoch  15 Batch   20/102 - Train Accuracy: 0.8648, Validation Accuracy: 0.8807, Loss: 0.2509\n",
            "Epoch  15 Batch   30/102 - Train Accuracy: 0.9033, Validation Accuracy: 0.8805, Loss: 0.2176\n",
            "Epoch  15 Batch   40/102 - Train Accuracy: 0.8921, Validation Accuracy: 0.8773, Loss: 0.2262\n",
            "Epoch  15 Batch   50/102 - Train Accuracy: 0.8895, Validation Accuracy: 0.8840, Loss: 0.2156\n",
            "Epoch  15 Batch   60/102 - Train Accuracy: 0.9076, Validation Accuracy: 0.8765, Loss: 0.1786\n",
            "Epoch  15 Batch   70/102 - Train Accuracy: 0.9190, Validation Accuracy: 0.8789, Loss: 0.1812\n",
            "Epoch  15 Batch   80/102 - Train Accuracy: 0.8960, Validation Accuracy: 0.8828, Loss: 0.2012\n",
            "Epoch  15 Batch   90/102 - Train Accuracy: 0.9127, Validation Accuracy: 0.8802, Loss: 0.1629\n",
            "Epoch  15 Batch  100/102 - Train Accuracy: 0.8731, Validation Accuracy: 0.8799, Loss: 0.2995\n",
            "Epoch  16 Batch   10/102 - Train Accuracy: 0.9154, Validation Accuracy: 0.8696, Loss: 0.1780\n",
            "Epoch  16 Batch   20/102 - Train Accuracy: 0.8826, Validation Accuracy: 0.8783, Loss: 0.2237\n",
            "Epoch  16 Batch   30/102 - Train Accuracy: 0.9028, Validation Accuracy: 0.8758, Loss: 0.2019\n",
            "Epoch  16 Batch   40/102 - Train Accuracy: 0.8999, Validation Accuracy: 0.8771, Loss: 0.2118\n",
            "Epoch  16 Batch   50/102 - Train Accuracy: 0.8944, Validation Accuracy: 0.8760, Loss: 0.1960\n",
            "Epoch  16 Batch   60/102 - Train Accuracy: 0.9118, Validation Accuracy: 0.8796, Loss: 0.1644\n",
            "Epoch  16 Batch   70/102 - Train Accuracy: 0.9237, Validation Accuracy: 0.8784, Loss: 0.1666\n",
            "Epoch  16 Batch   80/102 - Train Accuracy: 0.8948, Validation Accuracy: 0.8783, Loss: 0.1924\n",
            "Epoch  16 Batch   90/102 - Train Accuracy: 0.9176, Validation Accuracy: 0.8843, Loss: 0.1526\n",
            "Epoch  16 Batch  100/102 - Train Accuracy: 0.8847, Validation Accuracy: 0.8787, Loss: 0.2776\n",
            "Epoch  17 Batch   10/102 - Train Accuracy: 0.9141, Validation Accuracy: 0.8856, Loss: 0.1736\n",
            "Epoch  17 Batch   20/102 - Train Accuracy: 0.8974, Validation Accuracy: 0.8810, Loss: 0.2220\n",
            "Epoch  17 Batch   30/102 - Train Accuracy: 0.9044, Validation Accuracy: 0.8862, Loss: 0.1786\n",
            "Epoch  17 Batch   40/102 - Train Accuracy: 0.9141, Validation Accuracy: 0.8815, Loss: 0.1917\n",
            "Epoch  17 Batch   50/102 - Train Accuracy: 0.8969, Validation Accuracy: 0.8805, Loss: 0.1895\n",
            "Epoch  17 Batch   60/102 - Train Accuracy: 0.9144, Validation Accuracy: 0.8833, Loss: 0.1618\n",
            "Epoch  17 Batch   70/102 - Train Accuracy: 0.9336, Validation Accuracy: 0.8825, Loss: 0.1584\n",
            "Epoch  17 Batch   80/102 - Train Accuracy: 0.9027, Validation Accuracy: 0.8750, Loss: 0.1751\n",
            "Epoch  17 Batch   90/102 - Train Accuracy: 0.9162, Validation Accuracy: 0.8883, Loss: 0.1470\n",
            "Epoch  17 Batch  100/102 - Train Accuracy: 0.8825, Validation Accuracy: 0.8849, Loss: 0.2392\n",
            "Epoch  18 Batch   10/102 - Train Accuracy: 0.9174, Validation Accuracy: 0.8836, Loss: 0.1539\n",
            "Epoch  18 Batch   20/102 - Train Accuracy: 0.8966, Validation Accuracy: 0.8833, Loss: 0.1983\n",
            "Epoch  18 Batch   30/102 - Train Accuracy: 0.9203, Validation Accuracy: 0.8854, Loss: 0.1715\n",
            "Epoch  18 Batch   40/102 - Train Accuracy: 0.9183, Validation Accuracy: 0.8826, Loss: 0.1786\n",
            "Epoch  18 Batch   50/102 - Train Accuracy: 0.9085, Validation Accuracy: 0.8942, Loss: 0.1730\n",
            "Epoch  18 Batch   60/102 - Train Accuracy: 0.9191, Validation Accuracy: 0.8844, Loss: 0.1540\n",
            "Epoch  18 Batch   70/102 - Train Accuracy: 0.9336, Validation Accuracy: 0.8815, Loss: 0.1517\n",
            "Epoch  18 Batch   80/102 - Train Accuracy: 0.9118, Validation Accuracy: 0.8838, Loss: 0.1639\n",
            "Epoch  18 Batch   90/102 - Train Accuracy: 0.9285, Validation Accuracy: 0.8835, Loss: 0.1375\n",
            "Epoch  18 Batch  100/102 - Train Accuracy: 0.9025, Validation Accuracy: 0.8840, Loss: 0.2167\n",
            "Epoch  19 Batch   10/102 - Train Accuracy: 0.9286, Validation Accuracy: 0.8836, Loss: 0.1525\n",
            "Epoch  19 Batch   20/102 - Train Accuracy: 0.9047, Validation Accuracy: 0.8844, Loss: 0.1909\n",
            "Epoch  19 Batch   30/102 - Train Accuracy: 0.9138, Validation Accuracy: 0.8817, Loss: 0.1601\n",
            "Epoch  19 Batch   40/102 - Train Accuracy: 0.9303, Validation Accuracy: 0.8791, Loss: 0.1788\n",
            "Epoch  19 Batch   50/102 - Train Accuracy: 0.9250, Validation Accuracy: 0.8841, Loss: 0.1678\n",
            "Epoch  19 Batch   60/102 - Train Accuracy: 0.9185, Validation Accuracy: 0.8867, Loss: 0.1400\n",
            "Epoch  19 Batch   70/102 - Train Accuracy: 0.9352, Validation Accuracy: 0.8835, Loss: 0.1376\n",
            "Epoch  19 Batch   80/102 - Train Accuracy: 0.9217, Validation Accuracy: 0.8848, Loss: 0.1531\n",
            "Epoch  19 Batch   90/102 - Train Accuracy: 0.9360, Validation Accuracy: 0.8859, Loss: 0.1194\n",
            "Epoch  19 Batch  100/102 - Train Accuracy: 0.9150, Validation Accuracy: 0.8872, Loss: 0.2076\n",
            "Epoch  20 Batch   10/102 - Train Accuracy: 0.9325, Validation Accuracy: 0.8856, Loss: 0.1355\n",
            "Epoch  20 Batch   20/102 - Train Accuracy: 0.9154, Validation Accuracy: 0.8817, Loss: 0.1646\n",
            "Epoch  20 Batch   30/102 - Train Accuracy: 0.9281, Validation Accuracy: 0.8856, Loss: 0.1486\n",
            "Epoch  20 Batch   40/102 - Train Accuracy: 0.9315, Validation Accuracy: 0.8862, Loss: 0.1462\n",
            "Epoch  20 Batch   50/102 - Train Accuracy: 0.9252, Validation Accuracy: 0.8866, Loss: 0.1571\n",
            "Epoch  20 Batch   60/102 - Train Accuracy: 0.9277, Validation Accuracy: 0.8919, Loss: 0.1326\n",
            "Epoch  20 Batch   70/102 - Train Accuracy: 0.9417, Validation Accuracy: 0.8877, Loss: 0.1148\n",
            "Epoch  20 Batch   80/102 - Train Accuracy: 0.9273, Validation Accuracy: 0.8828, Loss: 0.1536\n",
            "Epoch  20 Batch   90/102 - Train Accuracy: 0.9350, Validation Accuracy: 0.8895, Loss: 0.1098\n",
            "Epoch  20 Batch  100/102 - Train Accuracy: 0.9119, Validation Accuracy: 0.8866, Loss: 0.1948\n",
            "Epoch  21 Batch   10/102 - Train Accuracy: 0.9433, Validation Accuracy: 0.8916, Loss: 0.1428\n",
            "Epoch  21 Batch   20/102 - Train Accuracy: 0.9227, Validation Accuracy: 0.8892, Loss: 0.1660\n",
            "Epoch  21 Batch   30/102 - Train Accuracy: 0.9276, Validation Accuracy: 0.8787, Loss: 0.1624\n",
            "Epoch  21 Batch   40/102 - Train Accuracy: 0.9405, Validation Accuracy: 0.8877, Loss: 0.1376\n",
            "Epoch  21 Batch   50/102 - Train Accuracy: 0.9335, Validation Accuracy: 0.8882, Loss: 0.1453\n",
            "Epoch  21 Batch   60/102 - Train Accuracy: 0.9285, Validation Accuracy: 0.8900, Loss: 0.1169\n",
            "Epoch  21 Batch   70/102 - Train Accuracy: 0.9576, Validation Accuracy: 0.8918, Loss: 0.1122\n",
            "Epoch  21 Batch   80/102 - Train Accuracy: 0.9265, Validation Accuracy: 0.8888, Loss: 0.1484\n",
            "Epoch  21 Batch   90/102 - Train Accuracy: 0.9433, Validation Accuracy: 0.8903, Loss: 0.1105\n",
            "Epoch  21 Batch  100/102 - Train Accuracy: 0.9253, Validation Accuracy: 0.8833, Loss: 0.1746\n",
            "Epoch  22 Batch   10/102 - Train Accuracy: 0.9408, Validation Accuracy: 0.8903, Loss: 0.1311\n",
            "Epoch  22 Batch   20/102 - Train Accuracy: 0.9253, Validation Accuracy: 0.8887, Loss: 0.1405\n",
            "Epoch  22 Batch   30/102 - Train Accuracy: 0.9345, Validation Accuracy: 0.8916, Loss: 0.1392\n",
            "Epoch  22 Batch   40/102 - Train Accuracy: 0.9435, Validation Accuracy: 0.8826, Loss: 0.1233\n",
            "Epoch  22 Batch   50/102 - Train Accuracy: 0.9435, Validation Accuracy: 0.8851, Loss: 0.1322\n",
            "Epoch  22 Batch   60/102 - Train Accuracy: 0.9354, Validation Accuracy: 0.8924, Loss: 0.1175\n",
            "Epoch  22 Batch   70/102 - Train Accuracy: 0.9586, Validation Accuracy: 0.8918, Loss: 0.1147\n",
            "Epoch  22 Batch   80/102 - Train Accuracy: 0.9380, Validation Accuracy: 0.8921, Loss: 0.1333\n",
            "Epoch  22 Batch   90/102 - Train Accuracy: 0.9448, Validation Accuracy: 0.8900, Loss: 0.1054\n",
            "Epoch  22 Batch  100/102 - Train Accuracy: 0.9366, Validation Accuracy: 0.8918, Loss: 0.1674\n",
            "Epoch  23 Batch   10/102 - Train Accuracy: 0.9487, Validation Accuracy: 0.8866, Loss: 0.1198\n",
            "Epoch  23 Batch   20/102 - Train Accuracy: 0.9458, Validation Accuracy: 0.8867, Loss: 0.1294\n",
            "Epoch  23 Batch   30/102 - Train Accuracy: 0.9444, Validation Accuracy: 0.8877, Loss: 0.1404\n",
            "Epoch  23 Batch   40/102 - Train Accuracy: 0.9585, Validation Accuracy: 0.8846, Loss: 0.1366\n",
            "Epoch  23 Batch   50/102 - Train Accuracy: 0.9315, Validation Accuracy: 0.8836, Loss: 0.1204\n",
            "Epoch  23 Batch   60/102 - Train Accuracy: 0.9375, Validation Accuracy: 0.8921, Loss: 0.1084\n",
            "Epoch  23 Batch   70/102 - Train Accuracy: 0.9750, Validation Accuracy: 0.8864, Loss: 0.0977\n",
            "Epoch  23 Batch   80/102 - Train Accuracy: 0.9404, Validation Accuracy: 0.8880, Loss: 0.1250\n",
            "Epoch  23 Batch   90/102 - Train Accuracy: 0.9470, Validation Accuracy: 0.8880, Loss: 0.0994\n",
            "Epoch  23 Batch  100/102 - Train Accuracy: 0.9397, Validation Accuracy: 0.8952, Loss: 0.1648\n",
            "Epoch  24 Batch   10/102 - Train Accuracy: 0.9403, Validation Accuracy: 0.8921, Loss: 0.1257\n",
            "Epoch  24 Batch   20/102 - Train Accuracy: 0.9414, Validation Accuracy: 0.8848, Loss: 0.1214\n",
            "Epoch  24 Batch   30/102 - Train Accuracy: 0.9469, Validation Accuracy: 0.8844, Loss: 0.1382\n",
            "Epoch  24 Batch   40/102 - Train Accuracy: 0.9672, Validation Accuracy: 0.8908, Loss: 0.1105\n",
            "Epoch  24 Batch   50/102 - Train Accuracy: 0.9353, Validation Accuracy: 0.8848, Loss: 0.1144\n",
            "Epoch  24 Batch   60/102 - Train Accuracy: 0.9434, Validation Accuracy: 0.8874, Loss: 0.1079\n",
            "Epoch  24 Batch   70/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8931, Loss: 0.0905\n",
            "Epoch  24 Batch   80/102 - Train Accuracy: 0.9434, Validation Accuracy: 0.8913, Loss: 0.1319\n",
            "Epoch  24 Batch   90/102 - Train Accuracy: 0.9506, Validation Accuracy: 0.8880, Loss: 0.0957\n",
            "Epoch  24 Batch  100/102 - Train Accuracy: 0.9513, Validation Accuracy: 0.8867, Loss: 0.1500\n",
            "Epoch  25 Batch   10/102 - Train Accuracy: 0.9477, Validation Accuracy: 0.8939, Loss: 0.1104\n",
            "Epoch  25 Batch   20/102 - Train Accuracy: 0.9589, Validation Accuracy: 0.8901, Loss: 0.1269\n",
            "Epoch  25 Batch   30/102 - Train Accuracy: 0.9515, Validation Accuracy: 0.8880, Loss: 0.1156\n",
            "Epoch  25 Batch   40/102 - Train Accuracy: 0.9594, Validation Accuracy: 0.8866, Loss: 0.1201\n",
            "Epoch  25 Batch   50/102 - Train Accuracy: 0.9440, Validation Accuracy: 0.8927, Loss: 0.1201\n",
            "Epoch  25 Batch   60/102 - Train Accuracy: 0.9551, Validation Accuracy: 0.8888, Loss: 0.1065\n",
            "Epoch  25 Batch   70/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8851, Loss: 0.0869\n",
            "Epoch  25 Batch   80/102 - Train Accuracy: 0.9402, Validation Accuracy: 0.8866, Loss: 0.1193\n",
            "Epoch  25 Batch   90/102 - Train Accuracy: 0.9609, Validation Accuracy: 0.8895, Loss: 0.0837\n",
            "Epoch  25 Batch  100/102 - Train Accuracy: 0.9537, Validation Accuracy: 0.8918, Loss: 0.1297\n",
            "Epoch  26 Batch   10/102 - Train Accuracy: 0.9505, Validation Accuracy: 0.8880, Loss: 0.0976\n",
            "Epoch  26 Batch   20/102 - Train Accuracy: 0.9563, Validation Accuracy: 0.8864, Loss: 0.1159\n",
            "Epoch  26 Batch   30/102 - Train Accuracy: 0.9527, Validation Accuracy: 0.8848, Loss: 0.1167\n",
            "Epoch  26 Batch   40/102 - Train Accuracy: 0.9703, Validation Accuracy: 0.8851, Loss: 0.0967\n",
            "Epoch  26 Batch   50/102 - Train Accuracy: 0.9531, Validation Accuracy: 0.8887, Loss: 0.1051\n",
            "Epoch  26 Batch   60/102 - Train Accuracy: 0.9548, Validation Accuracy: 0.8896, Loss: 0.0942\n",
            "Epoch  26 Batch   70/102 - Train Accuracy: 0.9786, Validation Accuracy: 0.8877, Loss: 0.0877\n",
            "Epoch  26 Batch   80/102 - Train Accuracy: 0.9501, Validation Accuracy: 0.8903, Loss: 0.0977\n",
            "Epoch  26 Batch   90/102 - Train Accuracy: 0.9631, Validation Accuracy: 0.8908, Loss: 0.0793\n",
            "Epoch  26 Batch  100/102 - Train Accuracy: 0.9606, Validation Accuracy: 0.8875, Loss: 0.1247\n",
            "Epoch  27 Batch   10/102 - Train Accuracy: 0.9513, Validation Accuracy: 0.8856, Loss: 0.0955\n",
            "Epoch  27 Batch   20/102 - Train Accuracy: 0.9563, Validation Accuracy: 0.8890, Loss: 0.1003\n",
            "Epoch  27 Batch   30/102 - Train Accuracy: 0.9600, Validation Accuracy: 0.8896, Loss: 0.1098\n",
            "Epoch  27 Batch   40/102 - Train Accuracy: 0.9684, Validation Accuracy: 0.8936, Loss: 0.0993\n",
            "Epoch  27 Batch   50/102 - Train Accuracy: 0.9612, Validation Accuracy: 0.8918, Loss: 0.0955\n",
            "Epoch  27 Batch   60/102 - Train Accuracy: 0.9619, Validation Accuracy: 0.8887, Loss: 0.0845\n",
            "Epoch  27 Batch   70/102 - Train Accuracy: 0.9789, Validation Accuracy: 0.8875, Loss: 0.0835\n",
            "Epoch  27 Batch   80/102 - Train Accuracy: 0.9506, Validation Accuracy: 0.8924, Loss: 0.1069\n",
            "Epoch  27 Batch   90/102 - Train Accuracy: 0.9616, Validation Accuracy: 0.8914, Loss: 0.0749\n",
            "Epoch  27 Batch  100/102 - Train Accuracy: 0.9631, Validation Accuracy: 0.8921, Loss: 0.1104\n",
            "Epoch  28 Batch   10/102 - Train Accuracy: 0.9535, Validation Accuracy: 0.8838, Loss: 0.0851\n",
            "Epoch  28 Batch   20/102 - Train Accuracy: 0.9547, Validation Accuracy: 0.8940, Loss: 0.1047\n",
            "Epoch  28 Batch   30/102 - Train Accuracy: 0.9623, Validation Accuracy: 0.8926, Loss: 0.0938\n",
            "Epoch  28 Batch   40/102 - Train Accuracy: 0.9832, Validation Accuracy: 0.8882, Loss: 0.0815\n",
            "Epoch  28 Batch   50/102 - Train Accuracy: 0.9679, Validation Accuracy: 0.8890, Loss: 0.0856\n",
            "Epoch  28 Batch   60/102 - Train Accuracy: 0.9652, Validation Accuracy: 0.8905, Loss: 0.0850\n",
            "Epoch  28 Batch   70/102 - Train Accuracy: 0.9828, Validation Accuracy: 0.8861, Loss: 0.0661\n",
            "Epoch  28 Batch   80/102 - Train Accuracy: 0.9515, Validation Accuracy: 0.8953, Loss: 0.1103\n",
            "Epoch  28 Batch   90/102 - Train Accuracy: 0.9711, Validation Accuracy: 0.8866, Loss: 0.0802\n",
            "Epoch  28 Batch  100/102 - Train Accuracy: 0.9703, Validation Accuracy: 0.8949, Loss: 0.1092\n",
            "Epoch  29 Batch   10/102 - Train Accuracy: 0.9619, Validation Accuracy: 0.8885, Loss: 0.0868\n",
            "Epoch  29 Batch   20/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8908, Loss: 0.0875\n",
            "Epoch  29 Batch   30/102 - Train Accuracy: 0.9630, Validation Accuracy: 0.8968, Loss: 0.0804\n",
            "Epoch  29 Batch   40/102 - Train Accuracy: 0.9648, Validation Accuracy: 0.8867, Loss: 0.0781\n",
            "Epoch  29 Batch   50/102 - Train Accuracy: 0.9618, Validation Accuracy: 0.8906, Loss: 0.0847\n",
            "Epoch  29 Batch   60/102 - Train Accuracy: 0.9718, Validation Accuracy: 0.8957, Loss: 0.0810\n",
            "Epoch  29 Batch   70/102 - Train Accuracy: 0.9792, Validation Accuracy: 0.8966, Loss: 0.0623\n",
            "Epoch  29 Batch   80/102 - Train Accuracy: 0.9530, Validation Accuracy: 0.8926, Loss: 0.0870\n",
            "Epoch  29 Batch   90/102 - Train Accuracy: 0.9673, Validation Accuracy: 0.8932, Loss: 0.0787\n",
            "Epoch  29 Batch  100/102 - Train Accuracy: 0.9731, Validation Accuracy: 0.8947, Loss: 0.1036\n",
            "Epoch  30 Batch   10/102 - Train Accuracy: 0.9639, Validation Accuracy: 0.8864, Loss: 0.0876\n",
            "Epoch  30 Batch   20/102 - Train Accuracy: 0.9578, Validation Accuracy: 0.8861, Loss: 0.0921\n",
            "Epoch  30 Batch   30/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8880, Loss: 0.0807\n",
            "Epoch  30 Batch   40/102 - Train Accuracy: 0.9784, Validation Accuracy: 0.8849, Loss: 0.0702\n",
            "Epoch  30 Batch   50/102 - Train Accuracy: 0.9685, Validation Accuracy: 0.8893, Loss: 0.0747\n",
            "Epoch  30 Batch   60/102 - Train Accuracy: 0.9718, Validation Accuracy: 0.8903, Loss: 0.0721\n",
            "Epoch  30 Batch   70/102 - Train Accuracy: 0.9781, Validation Accuracy: 0.8921, Loss: 0.0643\n",
            "Epoch  30 Batch   80/102 - Train Accuracy: 0.9571, Validation Accuracy: 0.8879, Loss: 0.0911\n",
            "Epoch  30 Batch   90/102 - Train Accuracy: 0.9711, Validation Accuracy: 0.8836, Loss: 0.0653\n",
            "Epoch  30 Batch  100/102 - Train Accuracy: 0.9734, Validation Accuracy: 0.8919, Loss: 0.0950\n",
            "Epoch  31 Batch   10/102 - Train Accuracy: 0.9663, Validation Accuracy: 0.8893, Loss: 0.0763\n",
            "Epoch  31 Batch   20/102 - Train Accuracy: 0.9695, Validation Accuracy: 0.8923, Loss: 0.0908\n",
            "Epoch  31 Batch   30/102 - Train Accuracy: 0.9690, Validation Accuracy: 0.8867, Loss: 0.0812\n",
            "Epoch  31 Batch   40/102 - Train Accuracy: 0.9784, Validation Accuracy: 0.8890, Loss: 0.0782\n",
            "Epoch  31 Batch   50/102 - Train Accuracy: 0.9699, Validation Accuracy: 0.8848, Loss: 0.0747\n",
            "Epoch  31 Batch   60/102 - Train Accuracy: 0.9761, Validation Accuracy: 0.8913, Loss: 0.0713\n",
            "Epoch  31 Batch   70/102 - Train Accuracy: 0.9898, Validation Accuracy: 0.8890, Loss: 0.0524\n",
            "Epoch  31 Batch   80/102 - Train Accuracy: 0.9657, Validation Accuracy: 0.8872, Loss: 0.0852\n",
            "Epoch  31 Batch   90/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8823, Loss: 0.0605\n",
            "Epoch  31 Batch  100/102 - Train Accuracy: 0.9816, Validation Accuracy: 0.8856, Loss: 0.0856\n",
            "Epoch  32 Batch   10/102 - Train Accuracy: 0.9714, Validation Accuracy: 0.8883, Loss: 0.0730\n",
            "Epoch  32 Batch   20/102 - Train Accuracy: 0.9664, Validation Accuracy: 0.8892, Loss: 0.0731\n",
            "Epoch  32 Batch   30/102 - Train Accuracy: 0.9669, Validation Accuracy: 0.8875, Loss: 0.0641\n",
            "Epoch  32 Batch   40/102 - Train Accuracy: 0.9814, Validation Accuracy: 0.8888, Loss: 0.0785\n",
            "Epoch  32 Batch   50/102 - Train Accuracy: 0.9679, Validation Accuracy: 0.8895, Loss: 0.0751\n",
            "Epoch  32 Batch   60/102 - Train Accuracy: 0.9673, Validation Accuracy: 0.8825, Loss: 0.0670\n",
            "Epoch  32 Batch   70/102 - Train Accuracy: 0.9872, Validation Accuracy: 0.8843, Loss: 0.0552\n",
            "Epoch  32 Batch   80/102 - Train Accuracy: 0.9589, Validation Accuracy: 0.8892, Loss: 0.0819\n",
            "Epoch  32 Batch   90/102 - Train Accuracy: 0.9714, Validation Accuracy: 0.8859, Loss: 0.0616\n",
            "Epoch  32 Batch  100/102 - Train Accuracy: 0.9800, Validation Accuracy: 0.8900, Loss: 0.0748\n",
            "Epoch  33 Batch   10/102 - Train Accuracy: 0.9762, Validation Accuracy: 0.8900, Loss: 0.0682\n",
            "Epoch  33 Batch   20/102 - Train Accuracy: 0.9740, Validation Accuracy: 0.8885, Loss: 0.0827\n",
            "Epoch  33 Batch   30/102 - Train Accuracy: 0.9706, Validation Accuracy: 0.8905, Loss: 0.0670\n",
            "Epoch  33 Batch   40/102 - Train Accuracy: 0.9865, Validation Accuracy: 0.8918, Loss: 0.0686\n",
            "Epoch  33 Batch   50/102 - Train Accuracy: 0.9795, Validation Accuracy: 0.8934, Loss: 0.0714\n",
            "Epoch  33 Batch   60/102 - Train Accuracy: 0.9740, Validation Accuracy: 0.8893, Loss: 0.0601\n",
            "Epoch  33 Batch   70/102 - Train Accuracy: 0.9802, Validation Accuracy: 0.8908, Loss: 0.0466\n",
            "Epoch  33 Batch   80/102 - Train Accuracy: 0.9644, Validation Accuracy: 0.8893, Loss: 0.0714\n",
            "Epoch  33 Batch   90/102 - Train Accuracy: 0.9746, Validation Accuracy: 0.8906, Loss: 0.0586\n",
            "Epoch  33 Batch  100/102 - Train Accuracy: 0.9844, Validation Accuracy: 0.8926, Loss: 0.0725\n",
            "Epoch  34 Batch   10/102 - Train Accuracy: 0.9769, Validation Accuracy: 0.8924, Loss: 0.0602\n",
            "Epoch  34 Batch   20/102 - Train Accuracy: 0.9703, Validation Accuracy: 0.8921, Loss: 0.0763\n",
            "Epoch  34 Batch   30/102 - Train Accuracy: 0.9754, Validation Accuracy: 0.8898, Loss: 0.0600\n",
            "Epoch  34 Batch   40/102 - Train Accuracy: 0.9871, Validation Accuracy: 0.8879, Loss: 0.0651\n",
            "Epoch  34 Batch   50/102 - Train Accuracy: 0.9779, Validation Accuracy: 0.8841, Loss: 0.0730\n",
            "Epoch  34 Batch   60/102 - Train Accuracy: 0.9762, Validation Accuracy: 0.8921, Loss: 0.0647\n",
            "Epoch  34 Batch   70/102 - Train Accuracy: 0.9927, Validation Accuracy: 0.8846, Loss: 0.0434\n",
            "Epoch  34 Batch   80/102 - Train Accuracy: 0.9689, Validation Accuracy: 0.8874, Loss: 0.0756\n",
            "Epoch  34 Batch   90/102 - Train Accuracy: 0.9729, Validation Accuracy: 0.8859, Loss: 0.0603\n",
            "Epoch  34 Batch  100/102 - Train Accuracy: 0.9862, Validation Accuracy: 0.8875, Loss: 0.0767\n",
            "Epoch  35 Batch   10/102 - Train Accuracy: 0.9758, Validation Accuracy: 0.8849, Loss: 0.0602\n",
            "Epoch  35 Batch   20/102 - Train Accuracy: 0.9781, Validation Accuracy: 0.8859, Loss: 0.0676\n",
            "Epoch  35 Batch   30/102 - Train Accuracy: 0.9805, Validation Accuracy: 0.8856, Loss: 0.0653\n",
            "Epoch  35 Batch   40/102 - Train Accuracy: 0.9877, Validation Accuracy: 0.8882, Loss: 0.0540\n",
            "Epoch  35 Batch   50/102 - Train Accuracy: 0.9739, Validation Accuracy: 0.8826, Loss: 0.0611\n",
            "Epoch  35 Batch   60/102 - Train Accuracy: 0.9751, Validation Accuracy: 0.8910, Loss: 0.0544\n",
            "Epoch  35 Batch   70/102 - Train Accuracy: 0.9904, Validation Accuracy: 0.8903, Loss: 0.0441\n",
            "Epoch  35 Batch   80/102 - Train Accuracy: 0.9711, Validation Accuracy: 0.8905, Loss: 0.0674\n",
            "Epoch  35 Batch   90/102 - Train Accuracy: 0.9802, Validation Accuracy: 0.8885, Loss: 0.0566\n",
            "Epoch  35 Batch  100/102 - Train Accuracy: 0.9828, Validation Accuracy: 0.8867, Loss: 0.0675\n",
            "Epoch  36 Batch   10/102 - Train Accuracy: 0.9760, Validation Accuracy: 0.8877, Loss: 0.0627\n",
            "Epoch  36 Batch   20/102 - Train Accuracy: 0.9742, Validation Accuracy: 0.8936, Loss: 0.0637\n",
            "Epoch  36 Batch   30/102 - Train Accuracy: 0.9733, Validation Accuracy: 0.8830, Loss: 0.0565\n",
            "Epoch  36 Batch   40/102 - Train Accuracy: 0.9883, Validation Accuracy: 0.8916, Loss: 0.0555\n",
            "Epoch  36 Batch   50/102 - Train Accuracy: 0.9842, Validation Accuracy: 0.8888, Loss: 0.0548\n",
            "Epoch  36 Batch   60/102 - Train Accuracy: 0.9826, Validation Accuracy: 0.8883, Loss: 0.0601\n",
            "Epoch  36 Batch   70/102 - Train Accuracy: 0.9906, Validation Accuracy: 0.8875, Loss: 0.0369\n",
            "Epoch  36 Batch   80/102 - Train Accuracy: 0.9691, Validation Accuracy: 0.8931, Loss: 0.0666\n",
            "Epoch  36 Batch   90/102 - Train Accuracy: 0.9824, Validation Accuracy: 0.8862, Loss: 0.0482\n",
            "Epoch  36 Batch  100/102 - Train Accuracy: 0.9838, Validation Accuracy: 0.8879, Loss: 0.0783\n",
            "Epoch  37 Batch   10/102 - Train Accuracy: 0.9792, Validation Accuracy: 0.8882, Loss: 0.0555\n",
            "Epoch  37 Batch   20/102 - Train Accuracy: 0.9849, Validation Accuracy: 0.8854, Loss: 0.0650\n",
            "Epoch  37 Batch   30/102 - Train Accuracy: 0.9793, Validation Accuracy: 0.8879, Loss: 0.0577\n",
            "Epoch  37 Batch   40/102 - Train Accuracy: 0.9919, Validation Accuracy: 0.8934, Loss: 0.0421\n",
            "Epoch  37 Batch   50/102 - Train Accuracy: 0.9810, Validation Accuracy: 0.8913, Loss: 0.0486\n",
            "Epoch  37 Batch   60/102 - Train Accuracy: 0.9792, Validation Accuracy: 0.8906, Loss: 0.0578\n",
            "Epoch  37 Batch   70/102 - Train Accuracy: 0.9935, Validation Accuracy: 0.8833, Loss: 0.0419\n",
            "Epoch  37 Batch   80/102 - Train Accuracy: 0.9708, Validation Accuracy: 0.8854, Loss: 0.0629\n",
            "Epoch  37 Batch   90/102 - Train Accuracy: 0.9852, Validation Accuracy: 0.8923, Loss: 0.0487\n",
            "Epoch  37 Batch  100/102 - Train Accuracy: 0.9850, Validation Accuracy: 0.8820, Loss: 0.0662\n",
            "Epoch  38 Batch   10/102 - Train Accuracy: 0.9727, Validation Accuracy: 0.8895, Loss: 0.0640\n",
            "Epoch  38 Batch   20/102 - Train Accuracy: 0.9828, Validation Accuracy: 0.8861, Loss: 0.0680\n",
            "Epoch  38 Batch   30/102 - Train Accuracy: 0.9770, Validation Accuracy: 0.8879, Loss: 0.0648\n",
            "Epoch  38 Batch   40/102 - Train Accuracy: 0.9898, Validation Accuracy: 0.8885, Loss: 0.0519\n",
            "Epoch  38 Batch   50/102 - Train Accuracy: 0.9797, Validation Accuracy: 0.8898, Loss: 0.0583\n",
            "Epoch  38 Batch   60/102 - Train Accuracy: 0.9788, Validation Accuracy: 0.8836, Loss: 0.0539\n",
            "Epoch  38 Batch   70/102 - Train Accuracy: 0.9914, Validation Accuracy: 0.8848, Loss: 0.0378\n",
            "Epoch  38 Batch   80/102 - Train Accuracy: 0.9778, Validation Accuracy: 0.8893, Loss: 0.0540\n",
            "Epoch  38 Batch   90/102 - Train Accuracy: 0.9829, Validation Accuracy: 0.8809, Loss: 0.0449\n",
            "Epoch  38 Batch  100/102 - Train Accuracy: 0.9916, Validation Accuracy: 0.8851, Loss: 0.0645\n",
            "Epoch  39 Batch   10/102 - Train Accuracy: 0.9818, Validation Accuracy: 0.8908, Loss: 0.0503\n",
            "Epoch  39 Batch   20/102 - Train Accuracy: 0.9810, Validation Accuracy: 0.8844, Loss: 0.0592\n",
            "Epoch  39 Batch   30/102 - Train Accuracy: 0.9798, Validation Accuracy: 0.8901, Loss: 0.0499\n",
            "Epoch  39 Batch   40/102 - Train Accuracy: 0.9832, Validation Accuracy: 0.8864, Loss: 0.0567\n",
            "Epoch  39 Batch   50/102 - Train Accuracy: 0.9824, Validation Accuracy: 0.8870, Loss: 0.0579\n",
            "Epoch  39 Batch   60/102 - Train Accuracy: 0.9788, Validation Accuracy: 0.8862, Loss: 0.0430\n",
            "Epoch  39 Batch   70/102 - Train Accuracy: 0.9953, Validation Accuracy: 0.8888, Loss: 0.0375\n",
            "Epoch  39 Batch   80/102 - Train Accuracy: 0.9665, Validation Accuracy: 0.8882, Loss: 0.0569\n",
            "Epoch  39 Batch   90/102 - Train Accuracy: 0.9875, Validation Accuracy: 0.8903, Loss: 0.0412\n",
            "Epoch  39 Batch  100/102 - Train Accuracy: 0.9931, Validation Accuracy: 0.8910, Loss: 0.0562\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KmC3yrPfq9yw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W6ObRf6kq-eW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z0A6eJGMXqj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_params(params):\n",
        "    with open('params.p', 'wb') as out_file:\n",
        "        pickle.dump(params, out_file)\n",
        "\n",
        "\n",
        "def load_params():\n",
        "    with open('params.p', mode='rb') as in_file:\n",
        "        return pickle.load(in_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xk3fJy1tqq5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save parameters for checkpoint\n",
        "import pickle\n",
        "save_params(save_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UvL0vd9NquFV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #Checkpoint\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# import problem_unittests as tests\n",
        "\n",
        "# _, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
        "load_path = load_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e5xUWHXvVHC9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EzqUfYmIq0mA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Transliterate the word\n",
        "def word_to_seq(word, vocab_to_int):\n",
        "    results = []\n",
        "    for char in word.split(\" \"):\n",
        "        if char in vocab_to_int:\n",
        "            results.append(vocab_to_int[char])\n",
        "        else:\n",
        "            results.append(vocab_to_int['<UNK>'])\n",
        "            \n",
        "    return results\n",
        "\n",
        "translate_word = \"A M E R I C A N\"\n",
        "\n",
        "translate_word = word_to_seq(translate_word, source_vocab_to_int)\n",
        "\n",
        "# translate_sentence = train['ENG'][0]\n",
        "\n",
        "# translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
        "\n",
        "# for i in range(len(train['ENG'])):\n",
        "#   #translate_sentence = sentence_to_seq(train['ENG'][i], source_vocab_to_int)\n",
        "#   translate_sentence = train['ENG'][i]\n",
        "#   if i == 0:\n",
        "#     translated_sentence = np.vstack([translate_sentence, translate_sentence])\n",
        "#   else:\n",
        "#     translated_sentence = np.vstack([translated_sentence, translate_sentence])\n",
        "  \n",
        "# translated_sentence = translated_sentence[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V_SxQ7_bX4sz",
        "colab_type": "code",
        "outputId": "9e01d1f3-af5d-40d9-9126-11b18a92c9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "translate_word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16, 28, 20, 33, 24, 18, 16, 29]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "bdBLcYhGq0dr",
        "colab_type": "code",
        "outputId": "83e0fa2f-7e37-4b27-96a8-d552854afac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "loaded_graph = tf.Graph()\n",
        "counter = 0\n",
        "\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
        "    loader.restore(sess, load_path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "    str_vec = []\n",
        "#     for i in range(len(test[\"ENG\"])):\n",
        "#       str_vec.append(\" \")\n",
        "#     xx = pd.DataFrame({'hin' : str_vec})\n",
        "    \n",
        "    for it, s in enumerate(test['ENG']):\n",
        "      translate_word = word_to_seq(s, source_vocab_to_int)\n",
        "      \n",
        "      translate_logits = sess.run(logits, {input_data: [translate_word]*batch_size,\n",
        "                                         target_sequence_length: [len(translate_word)*2]*batch_size,\n",
        "                                         keep_prob: 1.0})[0]\n",
        "      translate_logits = \" \".join([target_int_to_vocab[i] for i in translate_logits ])\n",
        "      \n",
        "#       xx['hin'][it] = str(translate_logits)\n",
        "#       if it == 0:\n",
        "#         final_trans = np.vstack([translate_logits, translate_logits])\n",
        "#       else:\n",
        "#         final_trans = np.vstack([final_trans, translate_logits])\n",
        "      str_vec.append(translate_logits)    \n",
        "    \n",
        "# print('Input')\n",
        "# print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
        "# print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
        "\n",
        "# print('\\nPrediction')\n",
        "# print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
        "# print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/checkpoints/dev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cCVanQsqjkow",
        "colab_type": "code",
        "outputId": "d07e4866-24cd-4b00-d256-0fa63d180ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "str_vec[1].split()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '', '', '<EOS>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "Hwc9Irw10AIg",
        "colab_type": "code",
        "outputId": "354ff867-8227-4d28-d673-3b99e20eae48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "train['HIN'][0].split()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '', '', '', '', '', '', '', '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "wlLchkmtmgEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "str_vec1 = []\n",
        "for i in str_vec:\n",
        "  str_vec1.append(i.split('<')[0])\n",
        "\n",
        "for i in range(len(str_vec1)):\n",
        "  str_vec1[i] = str_vec1[i][:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W1ODyNHq8LQV",
        "colab_type": "code",
        "outputId": "0097eef9-24f4-4829-bbc8-25aeabd7232c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "str_vec1[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'     '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "iczjBAIBKadM",
        "colab_type": "code",
        "outputId": "0b958c66-ae38-4362-a4de-4523af13be25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "cell_type": "code",
      "source": [
        "loaded_graph = train_graph#tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
        "    loader.restore(sess, load_path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "    translate_logits = sess.run(logits, {input_data: [translate_word]*batch_size,\n",
        "                                         target_sequence_length: [len(translate_word)*2]*batch_size,\n",
        "                                         keep_prob: 1.0})[0]\n",
        "\n",
        "print('Input')\n",
        "print('  Word Ids:      {}'.format([i for i in translate_word]))\n",
        "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_word]))\n",
        "\n",
        "print('\\nPrediction')\n",
        "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
        "print('  Hindi Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/checkpoints/dev\n",
            "Input\n",
            "  Word Ids:      [41, 24, 30, 29]\n",
            "  English Words: ['Z', 'I', 'O', 'N']\n",
            "\n",
            "Prediction\n",
            "  Word Ids:      [38, 64, 66, 56, 50, 75, 50, 1]\n",
            "  Hindi Words:        <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lNMkv-ZU8J84",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQfgef6W1WSh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert(s): \n",
        "  \n",
        "    # initialization of string to \"\" \n",
        "    new = \"\" \n",
        "  \n",
        "    # traverse in the string  \n",
        "    for x in s: \n",
        "        new += x  \n",
        "  \n",
        "    # return string  \n",
        "    return new \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEC9bPFL7pYI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yy = copy.deepcopy(str_vec1)\n",
        "for i in range(len(str_vec1)):\n",
        "  \n",
        "  yy[i] = convert(str_vec1[i].split())\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XVWWLSSKRpiX",
        "colab_type": "code",
        "outputId": "f96d7a30-44b4-47f7-d841-c81858b80fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "yy[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "IvrErAD9iRQz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pred_s = pd.DataFrame({'hin' : str_vec1})  \n",
        "pred_s = pd.DataFrame({'id':test['id'], 'HIN': str_vec1}, columns = ['id','HIN'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itpVlrBgpbiO",
        "colab_type": "code",
        "outputId": "2baf6158-af66-461c-8570-852b52a48eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1992
        }
      },
      "cell_type": "code",
      "source": [
        "pred_s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>HIN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>  </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td> </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>       </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>   </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>      </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>      </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>          </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>        </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>      </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>       </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>       </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>   </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>      </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>  </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>      )</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>       </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>      </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>      </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>       </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>970</td>\n",
              "      <td>          </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>971</th>\n",
              "      <td>971</td>\n",
              "      <td>        </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>972</td>\n",
              "      <td>   </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>973</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>974</td>\n",
              "      <td>         </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>975</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>976</td>\n",
              "      <td>       </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977</th>\n",
              "      <td>977</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>978</th>\n",
              "      <td>978</td>\n",
              "      <td>       </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>979</th>\n",
              "      <td>979</td>\n",
              "      <td>     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>980</th>\n",
              "      <td>980</td>\n",
              "      <td>      </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>981</th>\n",
              "      <td>981</td>\n",
              "      <td>      </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>982</td>\n",
              "      <td>          </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>983</td>\n",
              "      <td>         </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>984</td>\n",
              "      <td>        </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>985</td>\n",
              "      <td>        </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>986</td>\n",
              "      <td>   </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>987</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>988</td>\n",
              "      <td>        </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>989</td>\n",
              "      <td>   </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>990</td>\n",
              "      <td>          </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>991</td>\n",
              "      <td>     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>992</td>\n",
              "      <td>    </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>993</td>\n",
              "      <td>        </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>994</td>\n",
              "      <td>     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>995</td>\n",
              "      <td>   </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>996</td>\n",
              "      <td>       </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>997</td>\n",
              "      <td>         </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>998</td>\n",
              "      <td>         </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>999</td>\n",
              "      <td>      </td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                    HIN\n",
              "0      0                 \n",
              "1      1                    \n",
              "2      2                  \n",
              "3      3                  \n",
              "4      4                     \n",
              "5      5               \n",
              "6      6                 \n",
              "7      7                   \n",
              "8      8                  \n",
              "9      9                \n",
              "10    10                \n",
              "11    11            \n",
              "12    12                  \n",
              "13    13              \n",
              "14    14                \n",
              "15    15               \n",
              "16    16                  \n",
              "17    17               \n",
              "18    18                   \n",
              "19    19                \n",
              "20    20                    \n",
              "21    21                  \n",
              "22    22                 \n",
              "23    23                  \n",
              "24    24                  \n",
              "25    25                )\n",
              "26    26               \n",
              "27    27                \n",
              "28    28                \n",
              "29    29               \n",
              "..   ...                    ...\n",
              "970  970            \n",
              "971  971              \n",
              "972  972                   \n",
              "973  973                  \n",
              "974  974             \n",
              "975  975                  \n",
              "976  976               \n",
              "977  977                  \n",
              "978  978               \n",
              "979  979                 \n",
              "980  980                \n",
              "981  981                \n",
              "982  982            \n",
              "983  983             \n",
              "984  984              \n",
              "985  985              \n",
              "986  986                   \n",
              "987  987                  \n",
              "988  988              \n",
              "989  989                   \n",
              "990  990            \n",
              "991  991                 \n",
              "992  992                  \n",
              "993  993              \n",
              "994  994                 \n",
              "995  995                   \n",
              "996  996               \n",
              "997  997             \n",
              "998  998             \n",
              "999  999                \n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "4m1GIIis-i0E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "pred_s.to_csv('/content/gdrive/My Drive/submission_pa3.csv', index = False)\n",
        "files.download('/content/gdrive/My Drive/submission_pa3.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5tubxZ3jIts",
        "colab_type": "code",
        "outputId": "f73d2c47-fa7a-4d22-9d72-1b85d69bfd43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        " train['HIN'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'        '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "AuSHwqsKek9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}