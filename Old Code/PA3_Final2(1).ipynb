{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PA3_Final2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SC8_UGCBoV0U",
        "colab_type": "code",
        "outputId": "38f41489-f6fd-4c38-c934-040003224a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zubJkNs2oZ3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzxGwBy2obr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "train = pd.read_csv('/content/gdrive/My Drive/train.csv')\n",
        "valid = pd.read_csv('/content/gdrive/My Drive/valid.csv')\n",
        "test = pd.read_csv('/content/gdrive/My Drive/test_final.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Crk_55cppWLA",
        "colab_type": "code",
        "outputId": "4d607854-83a9-43a3-896b-624e63d9c0aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the unique values of the source as well as the target\n",
        "\n",
        "# Finding out the vocuabulary of the english words\n",
        "character_english = []\n",
        "for i in range(len(train)):\n",
        "  w = train['ENG'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_english.append(w[j])\n",
        "for i in range(len(valid)):\n",
        "  w = valid['ENG'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_english.append(w[j])\n",
        "for i in range(len(test)):\n",
        "  w = test['ENG'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_english.append(w[j])\n",
        "\n",
        "print('The number of unique English characters is %d' %len(np.unique(character_english)))  \n",
        "\n",
        "english_vocab = np.unique(character_english)\n",
        "\n",
        "# Finding out the vocuabulary of the Hindi words\n",
        "character_hindi = []\n",
        "for i in range(len(train)):\n",
        "  w = train['HIN'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_hindi.append(w[j])\n",
        "for i in range(len(valid)):\n",
        "  w = valid['HIN'][i].split()\n",
        "  for j in range(len(w)):\n",
        "    character_hindi.append(w[j])\n",
        "\n",
        "print('The number of unique Hindi characters is %d' %len(np.unique(character_hindi)))  \n",
        "\n",
        "hindi_vocab = np.unique(character_hindi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of unique English characters is 43\n",
            "The number of unique Hindi characters is 83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PsgreH1MoqS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Data Preprocessing\n",
        "\n",
        "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
        "\n",
        "def create_lookup_tables(vocabulary): # vocabulary contains the vocabularies created above\n",
        "    # make a list of unique words\n",
        "    #vocab = set(text.split())\n",
        "    vocab = vocabulary\n",
        "    # (1)\n",
        "    # starts with the special tokens\n",
        "    vocab_to_int = copy.copy(CODES)\n",
        "\n",
        "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
        "    # since vocab_to_int already contains special tokens\n",
        "    for v_i, v in enumerate(vocab, len(CODES)):\n",
        "        vocab_to_int[v] = v_i\n",
        "\n",
        "    # (2)\n",
        "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
        "\n",
        "    return vocab_to_int, int_to_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j7jyJBQ8q3tS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
        "    \"\"\"\n",
        "        1st, 2nd args: raw string text to be converted\n",
        "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
        "    \n",
        "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
        "    \"\"\"\n",
        "    # empty list of converted words\n",
        "    source_text_id = []\n",
        "    target_text_id = []\n",
        "    \n",
        "    # make a list of words (extraction)\n",
        "    source_words = source_text #source_text.split(\"\\n\")\n",
        "    target_words = target_text #target_text.split(\"\\n\")\n",
        "    \n",
        "    max_source_word_length = max([len(word.split(\" \")) for word in source_words])\n",
        "    max_target_word_length = max([len(word.split(\" \")) for word in target_words])\n",
        "    \n",
        "    # iterating through each word \n",
        "    for i in range(len(source_words)):\n",
        "        # extract words one by one\n",
        "        source_word = source_words[i]\n",
        "        target_word = target_words[i]\n",
        "        \n",
        "        # make a list of characters (extraction) from the chosen word\n",
        "        source_char = source_word.split(\" \")\n",
        "        target_char = target_word.split(\" \")\n",
        "        \n",
        "        # empty list of converted characters to index in the chosen word\n",
        "        source_char_id = []\n",
        "        target_char_id = []\n",
        "        \n",
        "        for index, token in enumerate(source_char):\n",
        "            if (token != \"\"):\n",
        "                source_char_id.append(source_vocab_to_int[token])\n",
        "        \n",
        "        for index, token in enumerate(target_char):\n",
        "            if (token != \"\"):\n",
        "                target_char_id.append(target_vocab_to_int[token])\n",
        "                \n",
        "        # put <EOS> token at the end of the chosen target word\n",
        "        # this token suggests when to stop creating a sequence\n",
        "        target_char_id.append(target_vocab_to_int['<EOS>'])\n",
        "            \n",
        "        # add each converted word in the final list\n",
        "        source_text_id.append(source_char_id)\n",
        "        target_text_id.append(target_char_id)\n",
        "    \n",
        "    return source_text_id, target_text_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXuh3a_8r7TJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data Pre-processing\n",
        "# create lookup tables for English and Hindi data\n",
        "source_vocab_to_int, source_int_to_vocab = create_lookup_tables(english_vocab)#(source_text)\n",
        "target_vocab_to_int, target_int_to_vocab = create_lookup_tables(hindi_vocab)#(target_text)\n",
        "\n",
        "# create list of words whose characters are represented in index\n",
        "source_text, target_text = text_to_ids(train['ENG'], train['HIN'], create_lookup_tables(english_vocab)[0], create_lookup_tables(hindi_vocab)[0])#(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-XB_KxJODx-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "source_int_text, target_int_text = source_text, target_text\n",
        "source_vocab_to_int, target_vocab_to_int = source_vocab_to_int, target_vocab_to_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z4qsajYjN7D2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Building the Neural Network\n",
        "\n",
        "def enc_dec_model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
        "    \n",
        "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
        "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
        "    \n",
        "    return inputs, targets, target_sequence_length, max_target_len\n",
        "  \n",
        "def hyperparam_inputs():\n",
        "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    \n",
        "    return lr_rate, keep_prob  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VZnk7rgHcb46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display_step = 300\n",
        "\n",
        "epochs = 13\n",
        "batch_size = 128\n",
        "\n",
        "rnn_size = 512\n",
        "num_layers = 2\n",
        "#num_layers2 = 2\n",
        "encoding_embedding_size = 256\n",
        "decoding_embedding_size = 256\n",
        "\n",
        "learning_rate = 0.001\n",
        "keep_probability = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "78t5uVI9RhjR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
        "    \"\"\"\n",
        "    Preprocess target data for encoding\n",
        "    :return: Preprocessed target data\n",
        "    \"\"\"\n",
        "    # get '<GO>' id\n",
        "    go_id = target_vocab_to_int['<GO>']\n",
        "    \n",
        "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
        "    \n",
        "    return after_concat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O5MDjmHJRvkF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
        "                   source_vocab_size, \n",
        "                   encoding_embedding_size):\n",
        "    \"\"\"\n",
        "    :return: tuple (RNN output, RNN state)\n",
        "    \"\"\"\n",
        "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
        "                                             vocab_size=source_vocab_size, \n",
        "                                             embed_dim=encoding_embedding_size)\n",
        "    \n",
        "    #stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob)  for _ in range(num_layers)])\n",
        "    \n",
        "    stacked_cells = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
        "    \n",
        "    outputs, state = tf.nn.bidirectional_dynamic_rnn(stacked_cells, stacked_cells, embed, dtype=tf.float32)\n",
        "    return outputs, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pKbYwkVU5DZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "attention = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "un6jCM5QRxX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
        "                         target_sequence_length, max_summary_length, \n",
        "                         output_layer, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a training process in decoding layer \n",
        "    :return: BasicDecoderOutput containing training logits and sample_id\n",
        "    \"\"\"\n",
        "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
        "                                             output_keep_prob=keep_prob)\n",
        "    \n",
        "    # for only input layer\n",
        "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
        "                                               target_sequence_length)\n",
        "    if attention == 1:\n",
        "      # Define attention mechanism\n",
        "      attn_mech = tf.contrib.seq2seq.LuongMonotonicAttention(\n",
        "      num_units = rnn_size, memory = [batch_size, max_target_sequence_length, rnn_size])\n",
        "#       memory_sequence_length = target_sequence_length)\n",
        "\n",
        "      # Define attention cell\n",
        "      attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "      cell = dec_cell, attention_mechanism = attn_mech,\n",
        "      alignment_history=True)\n",
        "\n",
        "\n",
        "      decoder = tf.contrib.seq2seq.BasicDecoder(attn_cell, \n",
        "                                                helper, \n",
        "                                                encoder_state, \n",
        "                                                output_layer)\n",
        "\n",
        "    else:\n",
        "      decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
        "                                                helper, \n",
        "                                                encoder_state, \n",
        "                                                output_layer)\n",
        "\n",
        "    \n",
        "    # unrolling the decoder layer\n",
        "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
        "                                                      impute_finished=True, \n",
        "                                                      maximum_iterations=max_summary_length)\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BMksCUnKR17Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
        "                         end_of_sequence_id, max_target_sequence_length,\n",
        "                         vocab_size, output_layer, batch_size, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a inference process in decoding layer \n",
        "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
        "    \"\"\"\n",
        "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
        "                                             output_keep_prob=keep_prob)\n",
        "    \n",
        "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
        "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
        "                                                      end_of_sequence_id)\n",
        "    \n",
        "    if attention == 1:\n",
        "      # Define attention mechanism\n",
        "      attn_mech = tf.contrib.seq2seq.LuongMonotonicAttention(\n",
        "      num_units = rnn_size, memory = [batch_size, max_target_sequence_length, rnn_size])\n",
        "#       memory_sequence_length = target_sequence_length)\n",
        "\n",
        "      # Define attention cell\n",
        "      attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "      cell = dec_cell, attention_mechanism = attn_mech,\n",
        "      alignment_history=True)\n",
        "\n",
        "\n",
        "      decoder = tf.contrib.seq2seq.BasicDecoder(attn_cell, \n",
        "                                                helper, \n",
        "                                                encoder_state, \n",
        "                                                output_layer)\n",
        "\n",
        "    else:\n",
        "      decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
        "                                                helper, \n",
        "                                                encoder_state, \n",
        "                                                output_layer)\n",
        "    \n",
        "#     decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
        "#                                               helper, \n",
        "#                                               encoder_state, \n",
        "#                                               output_layer)\n",
        "    \n",
        "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
        "                                                      impute_finished=True, \n",
        "                                                      maximum_iterations=max_target_sequence_length)\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xPALY6HAR5l9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer(dec_input, encoder_state,\n",
        "                   target_sequence_length, max_target_sequence_length,\n",
        "                   rnn_size,\n",
        "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
        "                   batch_size, keep_prob, decoding_embedding_size):\n",
        "    \"\"\"\n",
        "    Create decoding layer\n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"\n",
        "    target_vocab_size = len(target_vocab_to_int)\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "    \n",
        "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
        "    \n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        output_layer = tf.layers.Dense(target_vocab_size)\n",
        "        train_output = decoding_layer_train(encoder_state, \n",
        "                                            cells, \n",
        "                                            dec_embed_input, \n",
        "                                            target_sequence_length, \n",
        "                                            max_target_sequence_length, \n",
        "                                            output_layer, \n",
        "                                            keep_prob)\n",
        "\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        infer_output = decoding_layer_infer(encoder_state, \n",
        "                                            cells, \n",
        "                                            dec_embeddings, \n",
        "                                            target_vocab_to_int['<GO>'], \n",
        "                                            target_vocab_to_int['<EOS>'], \n",
        "                                            max_target_sequence_length, \n",
        "                                            target_vocab_size, \n",
        "                                            output_layer,\n",
        "                                            batch_size,\n",
        "                                            keep_prob)\n",
        "\n",
        "    return (train_output, infer_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LugY0hr8R9Xn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
        "                  target_sequence_length,\n",
        "                  max_target_word_length,\n",
        "                  source_vocab_size, target_vocab_size,\n",
        "                  enc_embedding_size, dec_embedding_size,\n",
        "                  rnn_size, num_layers, target_vocab_to_int):\n",
        "    \"\"\"\n",
        "    Build the Sequence-to-Sequence model\n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"\n",
        "    enc_outputs, enc_states = encoding_layer(input_data, \n",
        "                                             rnn_size, \n",
        "                                             num_layers, \n",
        "                                             keep_prob, \n",
        "                                             source_vocab_size, \n",
        "                                             enc_embedding_size)\n",
        "    \n",
        "    dec_input = process_decoder_input(target_data, \n",
        "                                      target_vocab_to_int, \n",
        "                                      batch_size)\n",
        "    \n",
        "    train_output, infer_output = decoding_layer(dec_input,\n",
        "                                               enc_states, \n",
        "                                               target_sequence_length, \n",
        "                                               max_target_word_length,\n",
        "                                               rnn_size,\n",
        "                                              num_layers,\n",
        "                                              target_vocab_to_int,\n",
        "                                              target_vocab_size,\n",
        "                                              batch_size,\n",
        "                                              keep_prob,\n",
        "                                              dec_embedding_size)\n",
        "    \n",
        "    return train_output, infer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g8Qf6AXDSW42",
        "colab_type": "code",
        "outputId": "9b5743a3-d92a-4faf-c663-303d15ec92b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "cell_type": "code",
      "source": [
        "#Build the graph\n",
        "\n",
        "max_target_word_length = max([len(word) for word in source_int_text])\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
        "    lr, keep_prob = hyperparam_inputs()\n",
        "    \n",
        "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                   targets,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size,\n",
        "                                                   target_sequence_length,\n",
        "                                                   max_target_sequence_length,\n",
        "                                                   len(source_vocab_to_int),\n",
        "                                                   len(target_vocab_to_int),\n",
        "                                                   encoding_embedding_size,\n",
        "                                                   decoding_embedding_size,\n",
        "                                                   rnn_size,\n",
        "                                                   num_layers,\n",
        "                                                   target_vocab_to_int)\n",
        "    \n",
        "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
        "    # - Returns a mask tensor representing the first N positions of each cell.\n",
        "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function - weighted softmax cross entropy\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-16-52fc95b70637>:13: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-16-52fc95b70637>:15: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From <ipython-input-25-9a2c83b42768>:14: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-25-9a2c83b42768>:14: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MTqSgWwoSvan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get the batches and pad the source and target sequences\n",
        "\n",
        "def pad_sentence_batch(sentence_batch, pad_int):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
        "\n",
        "\n",
        "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
        "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(sources)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "\n",
        "        # Slice the right amount for the batch\n",
        "        sources_batch = sources[start_i:start_i + batch_size]\n",
        "        targets_batch = targets[start_i:start_i + batch_size]\n",
        "\n",
        "        # Pad\n",
        "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
        "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_targets_lengths = []\n",
        "        for target in pad_targets_batch:\n",
        "            pad_targets_lengths.append(len(target))\n",
        "\n",
        "        pad_source_lengths = []\n",
        "        for source in pad_sources_batch:\n",
        "            pad_source_lengths.append(len(source))\n",
        "\n",
        "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O3DMsF5NTDjI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(target, logits):\n",
        "    \"\"\"\n",
        "    Calculate accuracy\n",
        "    \"\"\"\n",
        "    max_seq = max(target.shape[1], logits.shape[1])\n",
        "    if max_seq - target.shape[1]:\n",
        "        target = np.pad(\n",
        "            target,\n",
        "            [(0,0),(0,max_seq - target.shape[1])],\n",
        "            'constant')\n",
        "    if max_seq - logits.shape[1]:\n",
        "        logits = np.pad(\n",
        "            logits,\n",
        "            [(0,0),(0,max_seq - logits.shape[1])],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(target, logits))\n",
        "\n",
        "# Split data to training and validation sets\n",
        "train_source = source_int_text[batch_size:]\n",
        "train_target = target_int_text[batch_size:]\n",
        "valid_source = source_int_text[:batch_size]\n",
        "valid_target = target_int_text[:batch_size]\n",
        "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
        "                                                                                                             valid_target,\n",
        "                                                                                                             batch_size,\n",
        "                                                                                                             source_vocab_to_int['<PAD>'],\n",
        "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PcDWI61DX-RZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "display_step = 10\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJtelmgzWf4W",
        "colab_type": "code",
        "outputId": "3fcfa207-1612-4178-b64e-424302950f8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7422
        }
      },
      "cell_type": "code",
      "source": [
        "save_path = '/content/gdrive/My Drive/checkpoints/dev'\n",
        "\n",
        "\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        #print('Epoch 1')\n",
        "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(get_batches(train_source, train_target, batch_size,source_vocab_to_int['<PAD>'],target_vocab_to_int['<PAD>'])):\n",
        "\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: source_batch,\n",
        "                 targets: target_batch,\n",
        "                 lr: learning_rate,\n",
        "                 target_sequence_length: targets_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "           \n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                batch_train_logits = sess.run(\n",
        "                    inference_logits,\n",
        "                    {input_data: source_batch,\n",
        "                     target_sequence_length: targets_lengths,\n",
        "                     keep_prob: 1.0})\n",
        "\n",
        "                batch_valid_logits = sess.run(\n",
        "                    inference_logits,\n",
        "                    {input_data: valid_sources_batch,\n",
        "                     target_sequence_length: valid_targets_lengths,\n",
        "                     keep_prob: 1.0})\n",
        "\n",
        "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
        "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
        "\n",
        "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
        "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
        "\n",
        "#     test_logits = sess.run(inference_logits, \n",
        "#                           {input_data : })\n",
        "    \n",
        "    \n",
        "    \n",
        "    #Save Model\n",
        "    saver.save(sess, save_path)\n",
        "    print('Model Trained and Saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch   10/102 - Train Accuracy: 0.7727, Validation Accuracy: 0.7749, Loss: 1.2668\n",
            "Epoch   0 Batch   20/102 - Train Accuracy: 0.6815, Validation Accuracy: 0.7798, Loss: 1.4506\n",
            "Epoch   0 Batch   30/102 - Train Accuracy: 0.7279, Validation Accuracy: 0.7817, Loss: 1.1348\n",
            "Epoch   0 Batch   40/102 - Train Accuracy: 0.6268, Validation Accuracy: 0.7354, Loss: 1.3784\n",
            "Epoch   0 Batch   50/102 - Train Accuracy: 0.7203, Validation Accuracy: 0.7694, Loss: 1.0353\n",
            "Epoch   0 Batch   60/102 - Train Accuracy: 0.7329, Validation Accuracy: 0.7150, Loss: 0.8030\n",
            "Epoch   0 Batch   70/102 - Train Accuracy: 0.7299, Validation Accuracy: 0.7853, Loss: 1.0372\n",
            "Epoch   0 Batch   80/102 - Train Accuracy: 0.7572, Validation Accuracy: 0.7528, Loss: 0.7760\n",
            "Epoch   0 Batch   90/102 - Train Accuracy: 0.8040, Validation Accuracy: 0.7861, Loss: 0.7177\n",
            "Epoch   0 Batch  100/102 - Train Accuracy: 0.6344, Validation Accuracy: 0.7881, Loss: 1.3612\n",
            "Epoch   1 Batch   10/102 - Train Accuracy: 0.7868, Validation Accuracy: 0.7882, Loss: 0.7879\n",
            "Epoch   1 Batch   20/102 - Train Accuracy: 0.6888, Validation Accuracy: 0.7905, Loss: 1.1349\n",
            "Epoch   1 Batch   30/102 - Train Accuracy: 0.7401, Validation Accuracy: 0.7917, Loss: 0.9542\n",
            "Epoch   1 Batch   40/102 - Train Accuracy: 0.6635, Validation Accuracy: 0.7912, Loss: 1.2246\n",
            "Epoch   1 Batch   50/102 - Train Accuracy: 0.7489, Validation Accuracy: 0.7944, Loss: 0.9265\n",
            "Epoch   1 Batch   60/102 - Train Accuracy: 0.8026, Validation Accuracy: 0.7926, Loss: 0.7285\n",
            "Epoch   1 Batch   70/102 - Train Accuracy: 0.7328, Validation Accuracy: 0.7869, Loss: 0.9400\n",
            "Epoch   1 Batch   80/102 - Train Accuracy: 0.7867, Validation Accuracy: 0.7848, Loss: 0.7184\n",
            "Epoch   1 Batch   90/102 - Train Accuracy: 0.8077, Validation Accuracy: 0.7856, Loss: 0.6600\n",
            "Epoch   1 Batch  100/102 - Train Accuracy: 0.6400, Validation Accuracy: 0.7910, Loss: 1.2597\n",
            "Epoch   2 Batch   10/102 - Train Accuracy: 0.7876, Validation Accuracy: 0.7785, Loss: 0.7118\n",
            "Epoch   2 Batch   20/102 - Train Accuracy: 0.6893, Validation Accuracy: 0.7900, Loss: 1.0468\n",
            "Epoch   2 Batch   30/102 - Train Accuracy: 0.7266, Validation Accuracy: 0.7786, Loss: 0.8682\n",
            "Epoch   2 Batch   40/102 - Train Accuracy: 0.6424, Validation Accuracy: 0.7756, Loss: 1.0899\n",
            "Epoch   2 Batch   50/102 - Train Accuracy: 0.7299, Validation Accuracy: 0.7850, Loss: 0.8222\n",
            "Epoch   2 Batch   60/102 - Train Accuracy: 0.7764, Validation Accuracy: 0.7563, Loss: 0.6428\n",
            "Epoch   2 Batch   70/102 - Train Accuracy: 0.7318, Validation Accuracy: 0.7858, Loss: 0.8503\n",
            "Epoch   2 Batch   80/102 - Train Accuracy: 0.7886, Validation Accuracy: 0.7788, Loss: 0.6436\n",
            "Epoch   2 Batch   90/102 - Train Accuracy: 0.7959, Validation Accuracy: 0.7799, Loss: 0.6091\n",
            "Epoch   2 Batch  100/102 - Train Accuracy: 0.6378, Validation Accuracy: 0.7868, Loss: 1.1593\n",
            "Epoch   3 Batch   10/102 - Train Accuracy: 0.7924, Validation Accuracy: 0.7889, Loss: 0.6566\n",
            "Epoch   3 Batch   20/102 - Train Accuracy: 0.6607, Validation Accuracy: 0.7611, Loss: 0.9629\n",
            "Epoch   3 Batch   30/102 - Train Accuracy: 0.7148, Validation Accuracy: 0.7616, Loss: 0.8029\n",
            "Epoch   3 Batch   40/102 - Train Accuracy: 0.6508, Validation Accuracy: 0.7747, Loss: 1.0242\n",
            "Epoch   3 Batch   50/102 - Train Accuracy: 0.7158, Validation Accuracy: 0.7559, Loss: 0.7794\n",
            "Epoch   3 Batch   60/102 - Train Accuracy: 0.7879, Validation Accuracy: 0.7708, Loss: 0.6096\n",
            "Epoch   3 Batch   70/102 - Train Accuracy: 0.7365, Validation Accuracy: 0.7853, Loss: 0.7910\n",
            "Epoch   3 Batch   80/102 - Train Accuracy: 0.7916, Validation Accuracy: 0.7865, Loss: 0.5988\n",
            "Epoch   3 Batch   90/102 - Train Accuracy: 0.8012, Validation Accuracy: 0.7798, Loss: 0.5721\n",
            "Epoch   3 Batch  100/102 - Train Accuracy: 0.6331, Validation Accuracy: 0.7863, Loss: 1.0754\n",
            "Epoch   4 Batch   10/102 - Train Accuracy: 0.7894, Validation Accuracy: 0.7826, Loss: 0.6089\n",
            "Epoch   4 Batch   20/102 - Train Accuracy: 0.7063, Validation Accuracy: 0.7912, Loss: 0.9052\n",
            "Epoch   4 Batch   30/102 - Train Accuracy: 0.7523, Validation Accuracy: 0.7837, Loss: 0.7379\n",
            "Epoch   4 Batch   40/102 - Train Accuracy: 0.6653, Validation Accuracy: 0.7855, Loss: 0.9269\n",
            "Epoch   4 Batch   50/102 - Train Accuracy: 0.7455, Validation Accuracy: 0.7808, Loss: 0.7050\n",
            "Epoch   4 Batch   60/102 - Train Accuracy: 0.7894, Validation Accuracy: 0.7770, Loss: 0.5590\n",
            "Epoch   4 Batch   70/102 - Train Accuracy: 0.7448, Validation Accuracy: 0.7871, Loss: 0.7165\n",
            "Epoch   4 Batch   80/102 - Train Accuracy: 0.8114, Validation Accuracy: 0.8042, Loss: 0.5449\n",
            "Epoch   4 Batch   90/102 - Train Accuracy: 0.8180, Validation Accuracy: 0.7980, Loss: 0.4985\n",
            "Epoch   4 Batch  100/102 - Train Accuracy: 0.6625, Validation Accuracy: 0.8011, Loss: 0.9797\n",
            "Epoch   5 Batch   10/102 - Train Accuracy: 0.8045, Validation Accuracy: 0.7977, Loss: 0.5343\n",
            "Epoch   5 Batch   20/102 - Train Accuracy: 0.7068, Validation Accuracy: 0.7990, Loss: 0.7991\n",
            "Epoch   5 Batch   30/102 - Train Accuracy: 0.7594, Validation Accuracy: 0.7988, Loss: 0.6427\n",
            "Epoch   5 Batch   40/102 - Train Accuracy: 0.6989, Validation Accuracy: 0.8066, Loss: 0.7947\n",
            "Epoch   5 Batch   50/102 - Train Accuracy: 0.7754, Validation Accuracy: 0.8052, Loss: 0.6033\n",
            "Epoch   5 Batch   60/102 - Train Accuracy: 0.8234, Validation Accuracy: 0.8055, Loss: 0.4866\n",
            "Epoch   5 Batch   70/102 - Train Accuracy: 0.7760, Validation Accuracy: 0.8099, Loss: 0.6034\n",
            "Epoch   5 Batch   80/102 - Train Accuracy: 0.8217, Validation Accuracy: 0.8118, Loss: 0.4720\n",
            "Epoch   5 Batch   90/102 - Train Accuracy: 0.8421, Validation Accuracy: 0.8205, Loss: 0.4322\n",
            "Epoch   5 Batch  100/102 - Train Accuracy: 0.7100, Validation Accuracy: 0.8281, Loss: 0.8280\n",
            "Epoch   6 Batch   10/102 - Train Accuracy: 0.8278, Validation Accuracy: 0.8188, Loss: 0.4528\n",
            "Epoch   6 Batch   20/102 - Train Accuracy: 0.7484, Validation Accuracy: 0.8224, Loss: 0.6708\n",
            "Epoch   6 Batch   30/102 - Train Accuracy: 0.7833, Validation Accuracy: 0.8265, Loss: 0.5558\n",
            "Epoch   6 Batch   40/102 - Train Accuracy: 0.7440, Validation Accuracy: 0.8377, Loss: 0.6789\n",
            "Epoch   6 Batch   50/102 - Train Accuracy: 0.7855, Validation Accuracy: 0.8273, Loss: 0.4956\n",
            "Epoch   6 Batch   60/102 - Train Accuracy: 0.8488, Validation Accuracy: 0.8312, Loss: 0.4120\n",
            "Epoch   6 Batch   70/102 - Train Accuracy: 0.8010, Validation Accuracy: 0.8363, Loss: 0.4867\n",
            "Epoch   6 Batch   80/102 - Train Accuracy: 0.8430, Validation Accuracy: 0.8371, Loss: 0.4020\n",
            "Epoch   6 Batch   90/102 - Train Accuracy: 0.8582, Validation Accuracy: 0.8411, Loss: 0.3555\n",
            "Epoch   6 Batch  100/102 - Train Accuracy: 0.7419, Validation Accuracy: 0.8389, Loss: 0.7153\n",
            "Epoch   7 Batch   10/102 - Train Accuracy: 0.8424, Validation Accuracy: 0.8281, Loss: 0.3947\n",
            "Epoch   7 Batch   20/102 - Train Accuracy: 0.7688, Validation Accuracy: 0.8372, Loss: 0.5723\n",
            "Epoch   7 Batch   30/102 - Train Accuracy: 0.8139, Validation Accuracy: 0.8397, Loss: 0.4680\n",
            "Epoch   7 Batch   40/102 - Train Accuracy: 0.7599, Validation Accuracy: 0.8415, Loss: 0.5769\n",
            "Epoch   7 Batch   50/102 - Train Accuracy: 0.8116, Validation Accuracy: 0.8438, Loss: 0.4444\n",
            "Epoch   7 Batch   60/102 - Train Accuracy: 0.8478, Validation Accuracy: 0.8397, Loss: 0.3443\n",
            "Epoch   7 Batch   70/102 - Train Accuracy: 0.8156, Validation Accuracy: 0.8481, Loss: 0.4061\n",
            "Epoch   7 Batch   80/102 - Train Accuracy: 0.8549, Validation Accuracy: 0.8481, Loss: 0.3443\n",
            "Epoch   7 Batch   90/102 - Train Accuracy: 0.8737, Validation Accuracy: 0.8459, Loss: 0.3017\n",
            "Epoch   7 Batch  100/102 - Train Accuracy: 0.7634, Validation Accuracy: 0.8488, Loss: 0.5936\n",
            "Epoch   8 Batch   10/102 - Train Accuracy: 0.8566, Validation Accuracy: 0.8475, Loss: 0.3351\n",
            "Epoch   8 Batch   20/102 - Train Accuracy: 0.7893, Validation Accuracy: 0.8470, Loss: 0.4906\n",
            "Epoch   8 Batch   30/102 - Train Accuracy: 0.8316, Validation Accuracy: 0.8493, Loss: 0.4087\n",
            "Epoch   8 Batch   40/102 - Train Accuracy: 0.7740, Validation Accuracy: 0.8506, Loss: 0.4742\n",
            "Epoch   8 Batch   50/102 - Train Accuracy: 0.8335, Validation Accuracy: 0.8594, Loss: 0.3745\n",
            "Epoch   8 Batch   60/102 - Train Accuracy: 0.8636, Validation Accuracy: 0.8566, Loss: 0.2976\n",
            "Epoch   8 Batch   70/102 - Train Accuracy: 0.8383, Validation Accuracy: 0.8452, Loss: 0.3558\n",
            "Epoch   8 Batch   80/102 - Train Accuracy: 0.8635, Validation Accuracy: 0.8553, Loss: 0.3271\n",
            "Epoch   8 Batch   90/102 - Train Accuracy: 0.8723, Validation Accuracy: 0.8512, Loss: 0.2696\n",
            "Epoch   8 Batch  100/102 - Train Accuracy: 0.7747, Validation Accuracy: 0.8563, Loss: 0.5649\n",
            "Epoch   9 Batch   10/102 - Train Accuracy: 0.8711, Validation Accuracy: 0.8537, Loss: 0.3038\n",
            "Epoch   9 Batch   20/102 - Train Accuracy: 0.8229, Validation Accuracy: 0.8574, Loss: 0.4424\n",
            "Epoch   9 Batch   30/102 - Train Accuracy: 0.8408, Validation Accuracy: 0.8538, Loss: 0.3674\n",
            "Epoch   9 Batch   40/102 - Train Accuracy: 0.7939, Validation Accuracy: 0.8607, Loss: 0.4239\n",
            "Epoch   9 Batch   50/102 - Train Accuracy: 0.8556, Validation Accuracy: 0.8545, Loss: 0.3333\n",
            "Epoch   9 Batch   60/102 - Train Accuracy: 0.8761, Validation Accuracy: 0.8651, Loss: 0.2771\n",
            "Epoch   9 Batch   70/102 - Train Accuracy: 0.8482, Validation Accuracy: 0.8561, Loss: 0.3186\n",
            "Epoch   9 Batch   80/102 - Train Accuracy: 0.8669, Validation Accuracy: 0.8555, Loss: 0.2890\n",
            "Epoch   9 Batch   90/102 - Train Accuracy: 0.8763, Validation Accuracy: 0.8494, Loss: 0.2553\n",
            "Epoch   9 Batch  100/102 - Train Accuracy: 0.7847, Validation Accuracy: 0.8667, Loss: 0.4789\n",
            "Epoch  10 Batch   10/102 - Train Accuracy: 0.8728, Validation Accuracy: 0.8587, Loss: 0.2689\n",
            "Epoch  10 Batch   20/102 - Train Accuracy: 0.8201, Validation Accuracy: 0.8577, Loss: 0.3907\n",
            "Epoch  10 Batch   30/102 - Train Accuracy: 0.8497, Validation Accuracy: 0.8639, Loss: 0.3251\n",
            "Epoch  10 Batch   40/102 - Train Accuracy: 0.8227, Validation Accuracy: 0.8613, Loss: 0.3644\n",
            "Epoch  10 Batch   50/102 - Train Accuracy: 0.8623, Validation Accuracy: 0.8691, Loss: 0.3044\n",
            "Epoch  10 Batch   60/102 - Train Accuracy: 0.8716, Validation Accuracy: 0.8641, Loss: 0.2578\n",
            "Epoch  10 Batch   70/102 - Train Accuracy: 0.8633, Validation Accuracy: 0.8590, Loss: 0.2793\n",
            "Epoch  10 Batch   80/102 - Train Accuracy: 0.8728, Validation Accuracy: 0.8628, Loss: 0.2624\n",
            "Epoch  10 Batch   90/102 - Train Accuracy: 0.8923, Validation Accuracy: 0.8647, Loss: 0.2266\n",
            "Epoch  10 Batch  100/102 - Train Accuracy: 0.8000, Validation Accuracy: 0.8667, Loss: 0.4592\n",
            "Epoch  11 Batch   10/102 - Train Accuracy: 0.8821, Validation Accuracy: 0.8644, Loss: 0.2442\n",
            "Epoch  11 Batch   20/102 - Train Accuracy: 0.8273, Validation Accuracy: 0.8678, Loss: 0.3591\n",
            "Epoch  11 Batch   30/102 - Train Accuracy: 0.8580, Validation Accuracy: 0.8711, Loss: 0.2952\n",
            "Epoch  11 Batch   40/102 - Train Accuracy: 0.8260, Validation Accuracy: 0.8651, Loss: 0.3404\n",
            "Epoch  11 Batch   50/102 - Train Accuracy: 0.8737, Validation Accuracy: 0.8678, Loss: 0.2696\n",
            "Epoch  11 Batch   60/102 - Train Accuracy: 0.8848, Validation Accuracy: 0.8631, Loss: 0.2408\n",
            "Epoch  11 Batch   70/102 - Train Accuracy: 0.8786, Validation Accuracy: 0.8709, Loss: 0.2489\n",
            "Epoch  11 Batch   80/102 - Train Accuracy: 0.8742, Validation Accuracy: 0.8657, Loss: 0.2376\n",
            "Epoch  11 Batch   90/102 - Train Accuracy: 0.8938, Validation Accuracy: 0.8717, Loss: 0.2137\n",
            "Epoch  11 Batch  100/102 - Train Accuracy: 0.8328, Validation Accuracy: 0.8654, Loss: 0.3873\n",
            "Epoch  12 Batch   10/102 - Train Accuracy: 0.8852, Validation Accuracy: 0.8573, Loss: 0.2281\n",
            "Epoch  12 Batch   20/102 - Train Accuracy: 0.8401, Validation Accuracy: 0.8682, Loss: 0.3141\n",
            "Epoch  12 Batch   30/102 - Train Accuracy: 0.8734, Validation Accuracy: 0.8735, Loss: 0.2789\n",
            "Epoch  12 Batch   40/102 - Train Accuracy: 0.8639, Validation Accuracy: 0.8695, Loss: 0.3099\n",
            "Epoch  12 Batch   50/102 - Train Accuracy: 0.8819, Validation Accuracy: 0.8700, Loss: 0.2573\n",
            "Epoch  12 Batch   60/102 - Train Accuracy: 0.8937, Validation Accuracy: 0.8660, Loss: 0.2176\n",
            "Epoch  12 Batch   70/102 - Train Accuracy: 0.8938, Validation Accuracy: 0.8714, Loss: 0.2329\n",
            "Epoch  12 Batch   80/102 - Train Accuracy: 0.8820, Validation Accuracy: 0.8708, Loss: 0.2376\n",
            "Epoch  12 Batch   90/102 - Train Accuracy: 0.9051, Validation Accuracy: 0.8706, Loss: 0.2019\n",
            "Epoch  12 Batch  100/102 - Train Accuracy: 0.8350, Validation Accuracy: 0.8742, Loss: 0.3744\n",
            "Epoch  13 Batch   10/102 - Train Accuracy: 0.8973, Validation Accuracy: 0.8683, Loss: 0.2185\n",
            "Epoch  13 Batch   20/102 - Train Accuracy: 0.8458, Validation Accuracy: 0.8735, Loss: 0.3042\n",
            "Epoch  13 Batch   30/102 - Train Accuracy: 0.8750, Validation Accuracy: 0.8701, Loss: 0.2525\n",
            "Epoch  13 Batch   40/102 - Train Accuracy: 0.8831, Validation Accuracy: 0.8745, Loss: 0.2600\n",
            "Epoch  13 Batch   50/102 - Train Accuracy: 0.8850, Validation Accuracy: 0.8726, Loss: 0.2357\n",
            "Epoch  13 Batch   60/102 - Train Accuracy: 0.9014, Validation Accuracy: 0.8748, Loss: 0.1998\n",
            "Epoch  13 Batch   70/102 - Train Accuracy: 0.8992, Validation Accuracy: 0.8774, Loss: 0.2130\n",
            "Epoch  13 Batch   80/102 - Train Accuracy: 0.8852, Validation Accuracy: 0.8763, Loss: 0.2088\n",
            "Epoch  13 Batch   90/102 - Train Accuracy: 0.9006, Validation Accuracy: 0.8768, Loss: 0.1947\n",
            "Epoch  13 Batch  100/102 - Train Accuracy: 0.8394, Validation Accuracy: 0.8800, Loss: 0.3423\n",
            "Epoch  14 Batch   10/102 - Train Accuracy: 0.8990, Validation Accuracy: 0.8740, Loss: 0.1977\n",
            "Epoch  14 Batch   20/102 - Train Accuracy: 0.8646, Validation Accuracy: 0.8813, Loss: 0.2754\n",
            "Epoch  14 Batch   30/102 - Train Accuracy: 0.8796, Validation Accuracy: 0.8778, Loss: 0.2415\n",
            "Epoch  14 Batch   40/102 - Train Accuracy: 0.8840, Validation Accuracy: 0.8809, Loss: 0.2474\n",
            "Epoch  14 Batch   50/102 - Train Accuracy: 0.8949, Validation Accuracy: 0.8804, Loss: 0.2329\n",
            "Epoch  14 Batch   60/102 - Train Accuracy: 0.8981, Validation Accuracy: 0.8722, Loss: 0.1878\n",
            "Epoch  14 Batch   70/102 - Train Accuracy: 0.9096, Validation Accuracy: 0.8737, Loss: 0.1878\n",
            "Epoch  14 Batch   80/102 - Train Accuracy: 0.8986, Validation Accuracy: 0.8729, Loss: 0.2191\n",
            "Epoch  14 Batch   90/102 - Train Accuracy: 0.9079, Validation Accuracy: 0.8719, Loss: 0.1751\n",
            "Epoch  14 Batch  100/102 - Train Accuracy: 0.8647, Validation Accuracy: 0.8817, Loss: 0.3050\n",
            "Epoch  15 Batch   10/102 - Train Accuracy: 0.9044, Validation Accuracy: 0.8750, Loss: 0.1976\n",
            "Epoch  15 Batch   20/102 - Train Accuracy: 0.8648, Validation Accuracy: 0.8807, Loss: 0.2509\n",
            "Epoch  15 Batch   30/102 - Train Accuracy: 0.9033, Validation Accuracy: 0.8805, Loss: 0.2176\n",
            "Epoch  15 Batch   40/102 - Train Accuracy: 0.8921, Validation Accuracy: 0.8773, Loss: 0.2262\n",
            "Epoch  15 Batch   50/102 - Train Accuracy: 0.8895, Validation Accuracy: 0.8840, Loss: 0.2156\n",
            "Epoch  15 Batch   60/102 - Train Accuracy: 0.9076, Validation Accuracy: 0.8765, Loss: 0.1786\n",
            "Epoch  15 Batch   70/102 - Train Accuracy: 0.9190, Validation Accuracy: 0.8789, Loss: 0.1812\n",
            "Epoch  15 Batch   80/102 - Train Accuracy: 0.8960, Validation Accuracy: 0.8828, Loss: 0.2012\n",
            "Epoch  15 Batch   90/102 - Train Accuracy: 0.9127, Validation Accuracy: 0.8802, Loss: 0.1629\n",
            "Epoch  15 Batch  100/102 - Train Accuracy: 0.8731, Validation Accuracy: 0.8799, Loss: 0.2995\n",
            "Epoch  16 Batch   10/102 - Train Accuracy: 0.9154, Validation Accuracy: 0.8696, Loss: 0.1780\n",
            "Epoch  16 Batch   20/102 - Train Accuracy: 0.8826, Validation Accuracy: 0.8783, Loss: 0.2237\n",
            "Epoch  16 Batch   30/102 - Train Accuracy: 0.9028, Validation Accuracy: 0.8758, Loss: 0.2019\n",
            "Epoch  16 Batch   40/102 - Train Accuracy: 0.8999, Validation Accuracy: 0.8771, Loss: 0.2118\n",
            "Epoch  16 Batch   50/102 - Train Accuracy: 0.8944, Validation Accuracy: 0.8760, Loss: 0.1960\n",
            "Epoch  16 Batch   60/102 - Train Accuracy: 0.9118, Validation Accuracy: 0.8796, Loss: 0.1644\n",
            "Epoch  16 Batch   70/102 - Train Accuracy: 0.9237, Validation Accuracy: 0.8784, Loss: 0.1666\n",
            "Epoch  16 Batch   80/102 - Train Accuracy: 0.8948, Validation Accuracy: 0.8783, Loss: 0.1924\n",
            "Epoch  16 Batch   90/102 - Train Accuracy: 0.9176, Validation Accuracy: 0.8843, Loss: 0.1526\n",
            "Epoch  16 Batch  100/102 - Train Accuracy: 0.8847, Validation Accuracy: 0.8787, Loss: 0.2776\n",
            "Epoch  17 Batch   10/102 - Train Accuracy: 0.9141, Validation Accuracy: 0.8856, Loss: 0.1736\n",
            "Epoch  17 Batch   20/102 - Train Accuracy: 0.8974, Validation Accuracy: 0.8810, Loss: 0.2220\n",
            "Epoch  17 Batch   30/102 - Train Accuracy: 0.9044, Validation Accuracy: 0.8862, Loss: 0.1786\n",
            "Epoch  17 Batch   40/102 - Train Accuracy: 0.9141, Validation Accuracy: 0.8815, Loss: 0.1917\n",
            "Epoch  17 Batch   50/102 - Train Accuracy: 0.8969, Validation Accuracy: 0.8805, Loss: 0.1895\n",
            "Epoch  17 Batch   60/102 - Train Accuracy: 0.9144, Validation Accuracy: 0.8833, Loss: 0.1618\n",
            "Epoch  17 Batch   70/102 - Train Accuracy: 0.9336, Validation Accuracy: 0.8825, Loss: 0.1584\n",
            "Epoch  17 Batch   80/102 - Train Accuracy: 0.9027, Validation Accuracy: 0.8750, Loss: 0.1751\n",
            "Epoch  17 Batch   90/102 - Train Accuracy: 0.9162, Validation Accuracy: 0.8883, Loss: 0.1470\n",
            "Epoch  17 Batch  100/102 - Train Accuracy: 0.8825, Validation Accuracy: 0.8849, Loss: 0.2392\n",
            "Epoch  18 Batch   10/102 - Train Accuracy: 0.9174, Validation Accuracy: 0.8836, Loss: 0.1539\n",
            "Epoch  18 Batch   20/102 - Train Accuracy: 0.8966, Validation Accuracy: 0.8833, Loss: 0.1983\n",
            "Epoch  18 Batch   30/102 - Train Accuracy: 0.9203, Validation Accuracy: 0.8854, Loss: 0.1715\n",
            "Epoch  18 Batch   40/102 - Train Accuracy: 0.9183, Validation Accuracy: 0.8826, Loss: 0.1786\n",
            "Epoch  18 Batch   50/102 - Train Accuracy: 0.9085, Validation Accuracy: 0.8942, Loss: 0.1730\n",
            "Epoch  18 Batch   60/102 - Train Accuracy: 0.9191, Validation Accuracy: 0.8844, Loss: 0.1540\n",
            "Epoch  18 Batch   70/102 - Train Accuracy: 0.9336, Validation Accuracy: 0.8815, Loss: 0.1517\n",
            "Epoch  18 Batch   80/102 - Train Accuracy: 0.9118, Validation Accuracy: 0.8838, Loss: 0.1639\n",
            "Epoch  18 Batch   90/102 - Train Accuracy: 0.9285, Validation Accuracy: 0.8835, Loss: 0.1375\n",
            "Epoch  18 Batch  100/102 - Train Accuracy: 0.9025, Validation Accuracy: 0.8840, Loss: 0.2167\n",
            "Epoch  19 Batch   10/102 - Train Accuracy: 0.9286, Validation Accuracy: 0.8836, Loss: 0.1525\n",
            "Epoch  19 Batch   20/102 - Train Accuracy: 0.9047, Validation Accuracy: 0.8844, Loss: 0.1909\n",
            "Epoch  19 Batch   30/102 - Train Accuracy: 0.9138, Validation Accuracy: 0.8817, Loss: 0.1601\n",
            "Epoch  19 Batch   40/102 - Train Accuracy: 0.9303, Validation Accuracy: 0.8791, Loss: 0.1788\n",
            "Epoch  19 Batch   50/102 - Train Accuracy: 0.9250, Validation Accuracy: 0.8841, Loss: 0.1678\n",
            "Epoch  19 Batch   60/102 - Train Accuracy: 0.9185, Validation Accuracy: 0.8867, Loss: 0.1400\n",
            "Epoch  19 Batch   70/102 - Train Accuracy: 0.9352, Validation Accuracy: 0.8835, Loss: 0.1376\n",
            "Epoch  19 Batch   80/102 - Train Accuracy: 0.9217, Validation Accuracy: 0.8848, Loss: 0.1531\n",
            "Epoch  19 Batch   90/102 - Train Accuracy: 0.9360, Validation Accuracy: 0.8859, Loss: 0.1194\n",
            "Epoch  19 Batch  100/102 - Train Accuracy: 0.9150, Validation Accuracy: 0.8872, Loss: 0.2076\n",
            "Epoch  20 Batch   10/102 - Train Accuracy: 0.9325, Validation Accuracy: 0.8856, Loss: 0.1355\n",
            "Epoch  20 Batch   20/102 - Train Accuracy: 0.9154, Validation Accuracy: 0.8817, Loss: 0.1646\n",
            "Epoch  20 Batch   30/102 - Train Accuracy: 0.9281, Validation Accuracy: 0.8856, Loss: 0.1486\n",
            "Epoch  20 Batch   40/102 - Train Accuracy: 0.9315, Validation Accuracy: 0.8862, Loss: 0.1462\n",
            "Epoch  20 Batch   50/102 - Train Accuracy: 0.9252, Validation Accuracy: 0.8866, Loss: 0.1571\n",
            "Epoch  20 Batch   60/102 - Train Accuracy: 0.9277, Validation Accuracy: 0.8919, Loss: 0.1326\n",
            "Epoch  20 Batch   70/102 - Train Accuracy: 0.9417, Validation Accuracy: 0.8877, Loss: 0.1148\n",
            "Epoch  20 Batch   80/102 - Train Accuracy: 0.9273, Validation Accuracy: 0.8828, Loss: 0.1536\n",
            "Epoch  20 Batch   90/102 - Train Accuracy: 0.9350, Validation Accuracy: 0.8895, Loss: 0.1098\n",
            "Epoch  20 Batch  100/102 - Train Accuracy: 0.9119, Validation Accuracy: 0.8866, Loss: 0.1948\n",
            "Epoch  21 Batch   10/102 - Train Accuracy: 0.9433, Validation Accuracy: 0.8916, Loss: 0.1428\n",
            "Epoch  21 Batch   20/102 - Train Accuracy: 0.9227, Validation Accuracy: 0.8892, Loss: 0.1660\n",
            "Epoch  21 Batch   30/102 - Train Accuracy: 0.9276, Validation Accuracy: 0.8787, Loss: 0.1624\n",
            "Epoch  21 Batch   40/102 - Train Accuracy: 0.9405, Validation Accuracy: 0.8877, Loss: 0.1376\n",
            "Epoch  21 Batch   50/102 - Train Accuracy: 0.9335, Validation Accuracy: 0.8882, Loss: 0.1453\n",
            "Epoch  21 Batch   60/102 - Train Accuracy: 0.9285, Validation Accuracy: 0.8900, Loss: 0.1169\n",
            "Epoch  21 Batch   70/102 - Train Accuracy: 0.9576, Validation Accuracy: 0.8918, Loss: 0.1122\n",
            "Epoch  21 Batch   80/102 - Train Accuracy: 0.9265, Validation Accuracy: 0.8888, Loss: 0.1484\n",
            "Epoch  21 Batch   90/102 - Train Accuracy: 0.9433, Validation Accuracy: 0.8903, Loss: 0.1105\n",
            "Epoch  21 Batch  100/102 - Train Accuracy: 0.9253, Validation Accuracy: 0.8833, Loss: 0.1746\n",
            "Epoch  22 Batch   10/102 - Train Accuracy: 0.9408, Validation Accuracy: 0.8903, Loss: 0.1311\n",
            "Epoch  22 Batch   20/102 - Train Accuracy: 0.9253, Validation Accuracy: 0.8887, Loss: 0.1405\n",
            "Epoch  22 Batch   30/102 - Train Accuracy: 0.9345, Validation Accuracy: 0.8916, Loss: 0.1392\n",
            "Epoch  22 Batch   40/102 - Train Accuracy: 0.9435, Validation Accuracy: 0.8826, Loss: 0.1233\n",
            "Epoch  22 Batch   50/102 - Train Accuracy: 0.9435, Validation Accuracy: 0.8851, Loss: 0.1322\n",
            "Epoch  22 Batch   60/102 - Train Accuracy: 0.9354, Validation Accuracy: 0.8924, Loss: 0.1175\n",
            "Epoch  22 Batch   70/102 - Train Accuracy: 0.9586, Validation Accuracy: 0.8918, Loss: 0.1147\n",
            "Epoch  22 Batch   80/102 - Train Accuracy: 0.9380, Validation Accuracy: 0.8921, Loss: 0.1333\n",
            "Epoch  22 Batch   90/102 - Train Accuracy: 0.9448, Validation Accuracy: 0.8900, Loss: 0.1054\n",
            "Epoch  22 Batch  100/102 - Train Accuracy: 0.9366, Validation Accuracy: 0.8918, Loss: 0.1674\n",
            "Epoch  23 Batch   10/102 - Train Accuracy: 0.9487, Validation Accuracy: 0.8866, Loss: 0.1198\n",
            "Epoch  23 Batch   20/102 - Train Accuracy: 0.9458, Validation Accuracy: 0.8867, Loss: 0.1294\n",
            "Epoch  23 Batch   30/102 - Train Accuracy: 0.9444, Validation Accuracy: 0.8877, Loss: 0.1404\n",
            "Epoch  23 Batch   40/102 - Train Accuracy: 0.9585, Validation Accuracy: 0.8846, Loss: 0.1366\n",
            "Epoch  23 Batch   50/102 - Train Accuracy: 0.9315, Validation Accuracy: 0.8836, Loss: 0.1204\n",
            "Epoch  23 Batch   60/102 - Train Accuracy: 0.9375, Validation Accuracy: 0.8921, Loss: 0.1084\n",
            "Epoch  23 Batch   70/102 - Train Accuracy: 0.9750, Validation Accuracy: 0.8864, Loss: 0.0977\n",
            "Epoch  23 Batch   80/102 - Train Accuracy: 0.9404, Validation Accuracy: 0.8880, Loss: 0.1250\n",
            "Epoch  23 Batch   90/102 - Train Accuracy: 0.9470, Validation Accuracy: 0.8880, Loss: 0.0994\n",
            "Epoch  23 Batch  100/102 - Train Accuracy: 0.9397, Validation Accuracy: 0.8952, Loss: 0.1648\n",
            "Epoch  24 Batch   10/102 - Train Accuracy: 0.9403, Validation Accuracy: 0.8921, Loss: 0.1257\n",
            "Epoch  24 Batch   20/102 - Train Accuracy: 0.9414, Validation Accuracy: 0.8848, Loss: 0.1214\n",
            "Epoch  24 Batch   30/102 - Train Accuracy: 0.9469, Validation Accuracy: 0.8844, Loss: 0.1382\n",
            "Epoch  24 Batch   40/102 - Train Accuracy: 0.9672, Validation Accuracy: 0.8908, Loss: 0.1105\n",
            "Epoch  24 Batch   50/102 - Train Accuracy: 0.9353, Validation Accuracy: 0.8848, Loss: 0.1144\n",
            "Epoch  24 Batch   60/102 - Train Accuracy: 0.9434, Validation Accuracy: 0.8874, Loss: 0.1079\n",
            "Epoch  24 Batch   70/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8931, Loss: 0.0905\n",
            "Epoch  24 Batch   80/102 - Train Accuracy: 0.9434, Validation Accuracy: 0.8913, Loss: 0.1319\n",
            "Epoch  24 Batch   90/102 - Train Accuracy: 0.9506, Validation Accuracy: 0.8880, Loss: 0.0957\n",
            "Epoch  24 Batch  100/102 - Train Accuracy: 0.9513, Validation Accuracy: 0.8867, Loss: 0.1500\n",
            "Epoch  25 Batch   10/102 - Train Accuracy: 0.9477, Validation Accuracy: 0.8939, Loss: 0.1104\n",
            "Epoch  25 Batch   20/102 - Train Accuracy: 0.9589, Validation Accuracy: 0.8901, Loss: 0.1269\n",
            "Epoch  25 Batch   30/102 - Train Accuracy: 0.9515, Validation Accuracy: 0.8880, Loss: 0.1156\n",
            "Epoch  25 Batch   40/102 - Train Accuracy: 0.9594, Validation Accuracy: 0.8866, Loss: 0.1201\n",
            "Epoch  25 Batch   50/102 - Train Accuracy: 0.9440, Validation Accuracy: 0.8927, Loss: 0.1201\n",
            "Epoch  25 Batch   60/102 - Train Accuracy: 0.9551, Validation Accuracy: 0.8888, Loss: 0.1065\n",
            "Epoch  25 Batch   70/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8851, Loss: 0.0869\n",
            "Epoch  25 Batch   80/102 - Train Accuracy: 0.9402, Validation Accuracy: 0.8866, Loss: 0.1193\n",
            "Epoch  25 Batch   90/102 - Train Accuracy: 0.9609, Validation Accuracy: 0.8895, Loss: 0.0837\n",
            "Epoch  25 Batch  100/102 - Train Accuracy: 0.9537, Validation Accuracy: 0.8918, Loss: 0.1297\n",
            "Epoch  26 Batch   10/102 - Train Accuracy: 0.9505, Validation Accuracy: 0.8880, Loss: 0.0976\n",
            "Epoch  26 Batch   20/102 - Train Accuracy: 0.9563, Validation Accuracy: 0.8864, Loss: 0.1159\n",
            "Epoch  26 Batch   30/102 - Train Accuracy: 0.9527, Validation Accuracy: 0.8848, Loss: 0.1167\n",
            "Epoch  26 Batch   40/102 - Train Accuracy: 0.9703, Validation Accuracy: 0.8851, Loss: 0.0967\n",
            "Epoch  26 Batch   50/102 - Train Accuracy: 0.9531, Validation Accuracy: 0.8887, Loss: 0.1051\n",
            "Epoch  26 Batch   60/102 - Train Accuracy: 0.9548, Validation Accuracy: 0.8896, Loss: 0.0942\n",
            "Epoch  26 Batch   70/102 - Train Accuracy: 0.9786, Validation Accuracy: 0.8877, Loss: 0.0877\n",
            "Epoch  26 Batch   80/102 - Train Accuracy: 0.9501, Validation Accuracy: 0.8903, Loss: 0.0977\n",
            "Epoch  26 Batch   90/102 - Train Accuracy: 0.9631, Validation Accuracy: 0.8908, Loss: 0.0793\n",
            "Epoch  26 Batch  100/102 - Train Accuracy: 0.9606, Validation Accuracy: 0.8875, Loss: 0.1247\n",
            "Epoch  27 Batch   10/102 - Train Accuracy: 0.9513, Validation Accuracy: 0.8856, Loss: 0.0955\n",
            "Epoch  27 Batch   20/102 - Train Accuracy: 0.9563, Validation Accuracy: 0.8890, Loss: 0.1003\n",
            "Epoch  27 Batch   30/102 - Train Accuracy: 0.9600, Validation Accuracy: 0.8896, Loss: 0.1098\n",
            "Epoch  27 Batch   40/102 - Train Accuracy: 0.9684, Validation Accuracy: 0.8936, Loss: 0.0993\n",
            "Epoch  27 Batch   50/102 - Train Accuracy: 0.9612, Validation Accuracy: 0.8918, Loss: 0.0955\n",
            "Epoch  27 Batch   60/102 - Train Accuracy: 0.9619, Validation Accuracy: 0.8887, Loss: 0.0845\n",
            "Epoch  27 Batch   70/102 - Train Accuracy: 0.9789, Validation Accuracy: 0.8875, Loss: 0.0835\n",
            "Epoch  27 Batch   80/102 - Train Accuracy: 0.9506, Validation Accuracy: 0.8924, Loss: 0.1069\n",
            "Epoch  27 Batch   90/102 - Train Accuracy: 0.9616, Validation Accuracy: 0.8914, Loss: 0.0749\n",
            "Epoch  27 Batch  100/102 - Train Accuracy: 0.9631, Validation Accuracy: 0.8921, Loss: 0.1104\n",
            "Epoch  28 Batch   10/102 - Train Accuracy: 0.9535, Validation Accuracy: 0.8838, Loss: 0.0851\n",
            "Epoch  28 Batch   20/102 - Train Accuracy: 0.9547, Validation Accuracy: 0.8940, Loss: 0.1047\n",
            "Epoch  28 Batch   30/102 - Train Accuracy: 0.9623, Validation Accuracy: 0.8926, Loss: 0.0938\n",
            "Epoch  28 Batch   40/102 - Train Accuracy: 0.9832, Validation Accuracy: 0.8882, Loss: 0.0815\n",
            "Epoch  28 Batch   50/102 - Train Accuracy: 0.9679, Validation Accuracy: 0.8890, Loss: 0.0856\n",
            "Epoch  28 Batch   60/102 - Train Accuracy: 0.9652, Validation Accuracy: 0.8905, Loss: 0.0850\n",
            "Epoch  28 Batch   70/102 - Train Accuracy: 0.9828, Validation Accuracy: 0.8861, Loss: 0.0661\n",
            "Epoch  28 Batch   80/102 - Train Accuracy: 0.9515, Validation Accuracy: 0.8953, Loss: 0.1103\n",
            "Epoch  28 Batch   90/102 - Train Accuracy: 0.9711, Validation Accuracy: 0.8866, Loss: 0.0802\n",
            "Epoch  28 Batch  100/102 - Train Accuracy: 0.9703, Validation Accuracy: 0.8949, Loss: 0.1092\n",
            "Epoch  29 Batch   10/102 - Train Accuracy: 0.9619, Validation Accuracy: 0.8885, Loss: 0.0868\n",
            "Epoch  29 Batch   20/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8908, Loss: 0.0875\n",
            "Epoch  29 Batch   30/102 - Train Accuracy: 0.9630, Validation Accuracy: 0.8968, Loss: 0.0804\n",
            "Epoch  29 Batch   40/102 - Train Accuracy: 0.9648, Validation Accuracy: 0.8867, Loss: 0.0781\n",
            "Epoch  29 Batch   50/102 - Train Accuracy: 0.9618, Validation Accuracy: 0.8906, Loss: 0.0847\n",
            "Epoch  29 Batch   60/102 - Train Accuracy: 0.9718, Validation Accuracy: 0.8957, Loss: 0.0810\n",
            "Epoch  29 Batch   70/102 - Train Accuracy: 0.9792, Validation Accuracy: 0.8966, Loss: 0.0623\n",
            "Epoch  29 Batch   80/102 - Train Accuracy: 0.9530, Validation Accuracy: 0.8926, Loss: 0.0870\n",
            "Epoch  29 Batch   90/102 - Train Accuracy: 0.9673, Validation Accuracy: 0.8932, Loss: 0.0787\n",
            "Epoch  29 Batch  100/102 - Train Accuracy: 0.9731, Validation Accuracy: 0.8947, Loss: 0.1036\n",
            "Epoch  30 Batch   10/102 - Train Accuracy: 0.9639, Validation Accuracy: 0.8864, Loss: 0.0876\n",
            "Epoch  30 Batch   20/102 - Train Accuracy: 0.9578, Validation Accuracy: 0.8861, Loss: 0.0921\n",
            "Epoch  30 Batch   30/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8880, Loss: 0.0807\n",
            "Epoch  30 Batch   40/102 - Train Accuracy: 0.9784, Validation Accuracy: 0.8849, Loss: 0.0702\n",
            "Epoch  30 Batch   50/102 - Train Accuracy: 0.9685, Validation Accuracy: 0.8893, Loss: 0.0747\n",
            "Epoch  30 Batch   60/102 - Train Accuracy: 0.9718, Validation Accuracy: 0.8903, Loss: 0.0721\n",
            "Epoch  30 Batch   70/102 - Train Accuracy: 0.9781, Validation Accuracy: 0.8921, Loss: 0.0643\n",
            "Epoch  30 Batch   80/102 - Train Accuracy: 0.9571, Validation Accuracy: 0.8879, Loss: 0.0911\n",
            "Epoch  30 Batch   90/102 - Train Accuracy: 0.9711, Validation Accuracy: 0.8836, Loss: 0.0653\n",
            "Epoch  30 Batch  100/102 - Train Accuracy: 0.9734, Validation Accuracy: 0.8919, Loss: 0.0950\n",
            "Epoch  31 Batch   10/102 - Train Accuracy: 0.9663, Validation Accuracy: 0.8893, Loss: 0.0763\n",
            "Epoch  31 Batch   20/102 - Train Accuracy: 0.9695, Validation Accuracy: 0.8923, Loss: 0.0908\n",
            "Epoch  31 Batch   30/102 - Train Accuracy: 0.9690, Validation Accuracy: 0.8867, Loss: 0.0812\n",
            "Epoch  31 Batch   40/102 - Train Accuracy: 0.9784, Validation Accuracy: 0.8890, Loss: 0.0782\n",
            "Epoch  31 Batch   50/102 - Train Accuracy: 0.9699, Validation Accuracy: 0.8848, Loss: 0.0747\n",
            "Epoch  31 Batch   60/102 - Train Accuracy: 0.9761, Validation Accuracy: 0.8913, Loss: 0.0713\n",
            "Epoch  31 Batch   70/102 - Train Accuracy: 0.9898, Validation Accuracy: 0.8890, Loss: 0.0524\n",
            "Epoch  31 Batch   80/102 - Train Accuracy: 0.9657, Validation Accuracy: 0.8872, Loss: 0.0852\n",
            "Epoch  31 Batch   90/102 - Train Accuracy: 0.9674, Validation Accuracy: 0.8823, Loss: 0.0605\n",
            "Epoch  31 Batch  100/102 - Train Accuracy: 0.9816, Validation Accuracy: 0.8856, Loss: 0.0856\n",
            "Epoch  32 Batch   10/102 - Train Accuracy: 0.9714, Validation Accuracy: 0.8883, Loss: 0.0730\n",
            "Epoch  32 Batch   20/102 - Train Accuracy: 0.9664, Validation Accuracy: 0.8892, Loss: 0.0731\n",
            "Epoch  32 Batch   30/102 - Train Accuracy: 0.9669, Validation Accuracy: 0.8875, Loss: 0.0641\n",
            "Epoch  32 Batch   40/102 - Train Accuracy: 0.9814, Validation Accuracy: 0.8888, Loss: 0.0785\n",
            "Epoch  32 Batch   50/102 - Train Accuracy: 0.9679, Validation Accuracy: 0.8895, Loss: 0.0751\n",
            "Epoch  32 Batch   60/102 - Train Accuracy: 0.9673, Validation Accuracy: 0.8825, Loss: 0.0670\n",
            "Epoch  32 Batch   70/102 - Train Accuracy: 0.9872, Validation Accuracy: 0.8843, Loss: 0.0552\n",
            "Epoch  32 Batch   80/102 - Train Accuracy: 0.9589, Validation Accuracy: 0.8892, Loss: 0.0819\n",
            "Epoch  32 Batch   90/102 - Train Accuracy: 0.9714, Validation Accuracy: 0.8859, Loss: 0.0616\n",
            "Epoch  32 Batch  100/102 - Train Accuracy: 0.9800, Validation Accuracy: 0.8900, Loss: 0.0748\n",
            "Epoch  33 Batch   10/102 - Train Accuracy: 0.9762, Validation Accuracy: 0.8900, Loss: 0.0682\n",
            "Epoch  33 Batch   20/102 - Train Accuracy: 0.9740, Validation Accuracy: 0.8885, Loss: 0.0827\n",
            "Epoch  33 Batch   30/102 - Train Accuracy: 0.9706, Validation Accuracy: 0.8905, Loss: 0.0670\n",
            "Epoch  33 Batch   40/102 - Train Accuracy: 0.9865, Validation Accuracy: 0.8918, Loss: 0.0686\n",
            "Epoch  33 Batch   50/102 - Train Accuracy: 0.9795, Validation Accuracy: 0.8934, Loss: 0.0714\n",
            "Epoch  33 Batch   60/102 - Train Accuracy: 0.9740, Validation Accuracy: 0.8893, Loss: 0.0601\n",
            "Epoch  33 Batch   70/102 - Train Accuracy: 0.9802, Validation Accuracy: 0.8908, Loss: 0.0466\n",
            "Epoch  33 Batch   80/102 - Train Accuracy: 0.9644, Validation Accuracy: 0.8893, Loss: 0.0714\n",
            "Epoch  33 Batch   90/102 - Train Accuracy: 0.9746, Validation Accuracy: 0.8906, Loss: 0.0586\n",
            "Epoch  33 Batch  100/102 - Train Accuracy: 0.9844, Validation Accuracy: 0.8926, Loss: 0.0725\n",
            "Epoch  34 Batch   10/102 - Train Accuracy: 0.9769, Validation Accuracy: 0.8924, Loss: 0.0602\n",
            "Epoch  34 Batch   20/102 - Train Accuracy: 0.9703, Validation Accuracy: 0.8921, Loss: 0.0763\n",
            "Epoch  34 Batch   30/102 - Train Accuracy: 0.9754, Validation Accuracy: 0.8898, Loss: 0.0600\n",
            "Epoch  34 Batch   40/102 - Train Accuracy: 0.9871, Validation Accuracy: 0.8879, Loss: 0.0651\n",
            "Epoch  34 Batch   50/102 - Train Accuracy: 0.9779, Validation Accuracy: 0.8841, Loss: 0.0730\n",
            "Epoch  34 Batch   60/102 - Train Accuracy: 0.9762, Validation Accuracy: 0.8921, Loss: 0.0647\n",
            "Epoch  34 Batch   70/102 - Train Accuracy: 0.9927, Validation Accuracy: 0.8846, Loss: 0.0434\n",
            "Epoch  34 Batch   80/102 - Train Accuracy: 0.9689, Validation Accuracy: 0.8874, Loss: 0.0756\n",
            "Epoch  34 Batch   90/102 - Train Accuracy: 0.9729, Validation Accuracy: 0.8859, Loss: 0.0603\n",
            "Epoch  34 Batch  100/102 - Train Accuracy: 0.9862, Validation Accuracy: 0.8875, Loss: 0.0767\n",
            "Epoch  35 Batch   10/102 - Train Accuracy: 0.9758, Validation Accuracy: 0.8849, Loss: 0.0602\n",
            "Epoch  35 Batch   20/102 - Train Accuracy: 0.9781, Validation Accuracy: 0.8859, Loss: 0.0676\n",
            "Epoch  35 Batch   30/102 - Train Accuracy: 0.9805, Validation Accuracy: 0.8856, Loss: 0.0653\n",
            "Epoch  35 Batch   40/102 - Train Accuracy: 0.9877, Validation Accuracy: 0.8882, Loss: 0.0540\n",
            "Epoch  35 Batch   50/102 - Train Accuracy: 0.9739, Validation Accuracy: 0.8826, Loss: 0.0611\n",
            "Epoch  35 Batch   60/102 - Train Accuracy: 0.9751, Validation Accuracy: 0.8910, Loss: 0.0544\n",
            "Epoch  35 Batch   70/102 - Train Accuracy: 0.9904, Validation Accuracy: 0.8903, Loss: 0.0441\n",
            "Epoch  35 Batch   80/102 - Train Accuracy: 0.9711, Validation Accuracy: 0.8905, Loss: 0.0674\n",
            "Epoch  35 Batch   90/102 - Train Accuracy: 0.9802, Validation Accuracy: 0.8885, Loss: 0.0566\n",
            "Epoch  35 Batch  100/102 - Train Accuracy: 0.9828, Validation Accuracy: 0.8867, Loss: 0.0675\n",
            "Epoch  36 Batch   10/102 - Train Accuracy: 0.9760, Validation Accuracy: 0.8877, Loss: 0.0627\n",
            "Epoch  36 Batch   20/102 - Train Accuracy: 0.9742, Validation Accuracy: 0.8936, Loss: 0.0637\n",
            "Epoch  36 Batch   30/102 - Train Accuracy: 0.9733, Validation Accuracy: 0.8830, Loss: 0.0565\n",
            "Epoch  36 Batch   40/102 - Train Accuracy: 0.9883, Validation Accuracy: 0.8916, Loss: 0.0555\n",
            "Epoch  36 Batch   50/102 - Train Accuracy: 0.9842, Validation Accuracy: 0.8888, Loss: 0.0548\n",
            "Epoch  36 Batch   60/102 - Train Accuracy: 0.9826, Validation Accuracy: 0.8883, Loss: 0.0601\n",
            "Epoch  36 Batch   70/102 - Train Accuracy: 0.9906, Validation Accuracy: 0.8875, Loss: 0.0369\n",
            "Epoch  36 Batch   80/102 - Train Accuracy: 0.9691, Validation Accuracy: 0.8931, Loss: 0.0666\n",
            "Epoch  36 Batch   90/102 - Train Accuracy: 0.9824, Validation Accuracy: 0.8862, Loss: 0.0482\n",
            "Epoch  36 Batch  100/102 - Train Accuracy: 0.9838, Validation Accuracy: 0.8879, Loss: 0.0783\n",
            "Epoch  37 Batch   10/102 - Train Accuracy: 0.9792, Validation Accuracy: 0.8882, Loss: 0.0555\n",
            "Epoch  37 Batch   20/102 - Train Accuracy: 0.9849, Validation Accuracy: 0.8854, Loss: 0.0650\n",
            "Epoch  37 Batch   30/102 - Train Accuracy: 0.9793, Validation Accuracy: 0.8879, Loss: 0.0577\n",
            "Epoch  37 Batch   40/102 - Train Accuracy: 0.9919, Validation Accuracy: 0.8934, Loss: 0.0421\n",
            "Epoch  37 Batch   50/102 - Train Accuracy: 0.9810, Validation Accuracy: 0.8913, Loss: 0.0486\n",
            "Epoch  37 Batch   60/102 - Train Accuracy: 0.9792, Validation Accuracy: 0.8906, Loss: 0.0578\n",
            "Epoch  37 Batch   70/102 - Train Accuracy: 0.9935, Validation Accuracy: 0.8833, Loss: 0.0419\n",
            "Epoch  37 Batch   80/102 - Train Accuracy: 0.9708, Validation Accuracy: 0.8854, Loss: 0.0629\n",
            "Epoch  37 Batch   90/102 - Train Accuracy: 0.9852, Validation Accuracy: 0.8923, Loss: 0.0487\n",
            "Epoch  37 Batch  100/102 - Train Accuracy: 0.9850, Validation Accuracy: 0.8820, Loss: 0.0662\n",
            "Epoch  38 Batch   10/102 - Train Accuracy: 0.9727, Validation Accuracy: 0.8895, Loss: 0.0640\n",
            "Epoch  38 Batch   20/102 - Train Accuracy: 0.9828, Validation Accuracy: 0.8861, Loss: 0.0680\n",
            "Epoch  38 Batch   30/102 - Train Accuracy: 0.9770, Validation Accuracy: 0.8879, Loss: 0.0648\n",
            "Epoch  38 Batch   40/102 - Train Accuracy: 0.9898, Validation Accuracy: 0.8885, Loss: 0.0519\n",
            "Epoch  38 Batch   50/102 - Train Accuracy: 0.9797, Validation Accuracy: 0.8898, Loss: 0.0583\n",
            "Epoch  38 Batch   60/102 - Train Accuracy: 0.9788, Validation Accuracy: 0.8836, Loss: 0.0539\n",
            "Epoch  38 Batch   70/102 - Train Accuracy: 0.9914, Validation Accuracy: 0.8848, Loss: 0.0378\n",
            "Epoch  38 Batch   80/102 - Train Accuracy: 0.9778, Validation Accuracy: 0.8893, Loss: 0.0540\n",
            "Epoch  38 Batch   90/102 - Train Accuracy: 0.9829, Validation Accuracy: 0.8809, Loss: 0.0449\n",
            "Epoch  38 Batch  100/102 - Train Accuracy: 0.9916, Validation Accuracy: 0.8851, Loss: 0.0645\n",
            "Epoch  39 Batch   10/102 - Train Accuracy: 0.9818, Validation Accuracy: 0.8908, Loss: 0.0503\n",
            "Epoch  39 Batch   20/102 - Train Accuracy: 0.9810, Validation Accuracy: 0.8844, Loss: 0.0592\n",
            "Epoch  39 Batch   30/102 - Train Accuracy: 0.9798, Validation Accuracy: 0.8901, Loss: 0.0499\n",
            "Epoch  39 Batch   40/102 - Train Accuracy: 0.9832, Validation Accuracy: 0.8864, Loss: 0.0567\n",
            "Epoch  39 Batch   50/102 - Train Accuracy: 0.9824, Validation Accuracy: 0.8870, Loss: 0.0579\n",
            "Epoch  39 Batch   60/102 - Train Accuracy: 0.9788, Validation Accuracy: 0.8862, Loss: 0.0430\n",
            "Epoch  39 Batch   70/102 - Train Accuracy: 0.9953, Validation Accuracy: 0.8888, Loss: 0.0375\n",
            "Epoch  39 Batch   80/102 - Train Accuracy: 0.9665, Validation Accuracy: 0.8882, Loss: 0.0569\n",
            "Epoch  39 Batch   90/102 - Train Accuracy: 0.9875, Validation Accuracy: 0.8903, Loss: 0.0412\n",
            "Epoch  39 Batch  100/102 - Train Accuracy: 0.9931, Validation Accuracy: 0.8910, Loss: 0.0562\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KmC3yrPfq9yw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W6ObRf6kq-eW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z0A6eJGMXqj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_params(params):\n",
        "    with open('params.p', 'wb') as out_file:\n",
        "        pickle.dump(params, out_file)\n",
        "\n",
        "\n",
        "def load_params():\n",
        "    with open('params.p', mode='rb') as in_file:\n",
        "        return pickle.load(in_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xk3fJy1tqq5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save parameters for checkpoint\n",
        "import pickle\n",
        "save_params(save_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UvL0vd9NquFV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #Checkpoint\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# import problem_unittests as tests\n",
        "\n",
        "# _, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
        "load_path = load_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e5xUWHXvVHC9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EzqUfYmIq0mA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Transliterate the word\n",
        "def word_to_seq(word, vocab_to_int):\n",
        "    results = []\n",
        "    for char in word.split(\" \"):\n",
        "        if char in vocab_to_int:\n",
        "            results.append(vocab_to_int[char])\n",
        "        else:\n",
        "            results.append(vocab_to_int['<UNK>'])\n",
        "            \n",
        "    return results\n",
        "\n",
        "translate_word = \"A M E R I C A N\"\n",
        "\n",
        "translate_word = word_to_seq(translate_word, source_vocab_to_int)\n",
        "\n",
        "# translate_sentence = train['ENG'][0]\n",
        "\n",
        "# translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
        "\n",
        "# for i in range(len(train['ENG'])):\n",
        "#   #translate_sentence = sentence_to_seq(train['ENG'][i], source_vocab_to_int)\n",
        "#   translate_sentence = train['ENG'][i]\n",
        "#   if i == 0:\n",
        "#     translated_sentence = np.vstack([translate_sentence, translate_sentence])\n",
        "#   else:\n",
        "#     translated_sentence = np.vstack([translated_sentence, translate_sentence])\n",
        "  \n",
        "# translated_sentence = translated_sentence[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V_SxQ7_bX4sz",
        "colab_type": "code",
        "outputId": "9e01d1f3-af5d-40d9-9126-11b18a92c9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "translate_word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16, 28, 20, 33, 24, 18, 16, 29]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "bdBLcYhGq0dr",
        "colab_type": "code",
        "outputId": "83e0fa2f-7e37-4b27-96a8-d552854afac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "loaded_graph = tf.Graph()\n",
        "counter = 0\n",
        "\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
        "    loader.restore(sess, load_path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "    str_vec = []\n",
        "#     for i in range(len(test[\"ENG\"])):\n",
        "#       str_vec.append(\" \")\n",
        "#     xx = pd.DataFrame({'hin' : str_vec})\n",
        "    \n",
        "    for it, s in enumerate(test['ENG']):\n",
        "      translate_word = word_to_seq(s, source_vocab_to_int)\n",
        "      \n",
        "      translate_logits = sess.run(logits, {input_data: [translate_word]*batch_size,\n",
        "                                         target_sequence_length: [len(translate_word)*2]*batch_size,\n",
        "                                         keep_prob: 1.0})[0]\n",
        "      translate_logits = \" \".join([target_int_to_vocab[i] for i in translate_logits ])\n",
        "      \n",
        "#       xx['hin'][it] = str(translate_logits)\n",
        "#       if it == 0:\n",
        "#         final_trans = np.vstack([translate_logits, translate_logits])\n",
        "#       else:\n",
        "#         final_trans = np.vstack([final_trans, translate_logits])\n",
        "      str_vec.append(translate_logits)    \n",
        "    \n",
        "# print('Input')\n",
        "# print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
        "# print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
        "\n",
        "# print('\\nPrediction')\n",
        "# print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
        "# print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/checkpoints/dev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cCVanQsqjkow",
        "colab_type": "code",
        "outputId": "d07e4866-24cd-4b00-d256-0fa63d180ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "str_vec[1].split()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['आ', 'क', 'श', '<EOS>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "Hwc9Irw10AIg",
        "colab_type": "code",
        "outputId": "354ff867-8227-4d28-d673-3b99e20eae48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "train['HIN'][0].split()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['र', 'ा', 'स', 'व', 'ि', 'ह', 'ा', 'र', 'ी']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "wlLchkmtmgEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "str_vec1 = []\n",
        "for i in str_vec:\n",
        "  str_vec1.append(i.split('<')[0])\n",
        "\n",
        "for i in range(len(str_vec1)):\n",
        "  str_vec1[i] = str_vec1[i][:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W1ODyNHq8LQV",
        "colab_type": "code",
        "outputId": "0097eef9-24f4-4829-bbc8-25aeabd7232c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "str_vec1[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'आ ध ि य ा ँ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "iczjBAIBKadM",
        "colab_type": "code",
        "outputId": "0b958c66-ae38-4362-a4de-4523af13be25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "cell_type": "code",
      "source": [
        "loaded_graph = train_graph#tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
        "    loader.restore(sess, load_path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "    translate_logits = sess.run(logits, {input_data: [translate_word]*batch_size,\n",
        "                                         target_sequence_length: [len(translate_word)*2]*batch_size,\n",
        "                                         keep_prob: 1.0})[0]\n",
        "\n",
        "print('Input')\n",
        "print('  Word Ids:      {}'.format([i for i in translate_word]))\n",
        "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_word]))\n",
        "\n",
        "print('\\nPrediction')\n",
        "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
        "print('  Hindi Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/checkpoints/dev\n",
            "Input\n",
            "  Word Ids:      [41, 24, 30, 29]\n",
            "  English Words: ['Z', 'I', 'O', 'N']\n",
            "\n",
            "Prediction\n",
            "  Word Ids:      [38, 64, 66, 56, 50, 75, 50, 1]\n",
            "  Hindi Words: ज ़ ि य न ो न <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lNMkv-ZU8J84",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQfgef6W1WSh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert(s): \n",
        "  \n",
        "    # initialization of string to \"\" \n",
        "    new = \"\" \n",
        "  \n",
        "    # traverse in the string  \n",
        "    for x in s: \n",
        "        new += x  \n",
        "  \n",
        "    # return string  \n",
        "    return new \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEC9bPFL7pYI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yy = copy.deepcopy(str_vec1)\n",
        "for i in range(len(str_vec1)):\n",
        "  \n",
        "  yy[i] = convert(str_vec1[i].split())\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XVWWLSSKRpiX",
        "colab_type": "code",
        "outputId": "f96d7a30-44b4-47f7-d841-c81858b80fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "yy[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'आधियाँ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "IvrErAD9iRQz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pred_s = pd.DataFrame({'hin' : str_vec1})  \n",
        "pred_s = pd.DataFrame({'id':test['id'], 'HIN': str_vec1}, columns = ['id','HIN'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itpVlrBgpbiO",
        "colab_type": "code",
        "outputId": "2baf6158-af66-461c-8570-852b52a48eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1992
        }
      },
      "cell_type": "code",
      "source": [
        "pred_s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>HIN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>आ ध ि य ा ँ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>आ क श</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>आ प ् त ा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>आ य श ा ह</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>आ ई</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>अ ब ् द ु ह ् ष</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>अ ब र र ा थ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>अ ब ी य</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>अ ब ू उ न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>अ ब ् र ि य ा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>ए क ड े म ि य</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>ए क म ो ं क ो र ् श न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>ए क र न ो</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>ए ड म ् स ् क ो न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>अ ड ् ग ो न ं</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>ऑ ड ल ब ् स ् द</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>अ ग र ो ह</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>आ ह ् ल ा द ् ण</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>ए र ि ड</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>ऐ स ि न ् ज ़</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>अ ज ी</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>अ झ ा य े</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>अ ज ि ठ ा ं</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>अ क र म म</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>ए क े ल े</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>ख ृ क़ ़ र ी )</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>अ क ो ड ि य ा ब</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>प क ु र द ि य</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>अ ल स ् त ा ह</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>अ ल ् ब न ी य ा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>970</td>\n",
              "      <td>व ् र े ल ि स ् ल े ग</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>971</th>\n",
              "      <td>971</td>\n",
              "      <td>व ा ह ि ड ु ल ् ल</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>972</td>\n",
              "      <td>व ल ा न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>973</td>\n",
              "      <td>व ा ल ा ह</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>974</td>\n",
              "      <td>व ै ल ि ं ग ् ट न न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>975</td>\n",
              "      <td>व ं द ा ण</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>976</td>\n",
              "      <td>व ि स म ् म ि ड</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977</th>\n",
              "      <td>977</td>\n",
              "      <td>व ि क ् स</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>978</th>\n",
              "      <td>978</td>\n",
              "      <td>व ि थ ि ं ग ं ग</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>979</th>\n",
              "      <td>979</td>\n",
              "      <td>व ि भ ट न म</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>980</th>\n",
              "      <td>980</td>\n",
              "      <td>व ि ट े न प न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>981</th>\n",
              "      <td>981</td>\n",
              "      <td>व ू प ो द ् म</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>982</td>\n",
              "      <td>व ि ज ़ स ् र ि य ा र</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>983</td>\n",
              "      <td>व ि ल ि य म ् स ो न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>984</td>\n",
              "      <td>व ि श र ् ट ् य ो</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>985</td>\n",
              "      <td>व ि स न ् स ि क ी</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>986</td>\n",
              "      <td>व ि थ ा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>987</td>\n",
              "      <td>व ो ह ा ट</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>988</td>\n",
              "      <td>व ो ल ् भ र ै ं ड</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>989</td>\n",
              "      <td>व ू ल े</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>990</td>\n",
              "      <td>व ् ट म ै ग ् र ब ा ग</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>991</td>\n",
              "      <td>य ड ो न ् न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>992</td>\n",
              "      <td>य श ो ध र</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>993</td>\n",
              "      <td>य ो ग ि न ् द ् र</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>994</td>\n",
              "      <td>ज ़ ब र ् ब</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>995</td>\n",
              "      <td>ज़ ह र ी</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>996</td>\n",
              "      <td>ज़ े ल ् व े ग र</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>997</td>\n",
              "      <td>ज ़ ़ प ् र े ल ि न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>998</td>\n",
              "      <td>ज़ ि न द ग ि य ा ं द</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>999</td>\n",
              "      <td>ज ़ ि य न ो न</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                    HIN\n",
              "0      0            आ ध ि य ा ँ\n",
              "1      1                  आ क श\n",
              "2      2             आ प ् त ा \n",
              "3      3              आ य श ा ह\n",
              "4      4                    आ ई\n",
              "5      5        अ ब ् द ु ह ् ष\n",
              "6      6            अ ब र र ा थ\n",
              "7      7                अ ब ी य\n",
              "8      8              अ ब ू उ न\n",
              "9      9         अ ब ् र ि य ा \n",
              "10    10          ए क ड े म ि य\n",
              "11    11  ए क म ो ं क ो र ् श न\n",
              "12    12              ए क र न ो\n",
              "13    13      ए ड म ् स ् क ो न\n",
              "14    14          अ ड ् ग ो न ं\n",
              "15    15        ऑ ड ल ब ् स ् द\n",
              "16    16              अ ग र ो ह\n",
              "17    17        आ ह ् ल ा द ् ण\n",
              "18    18                ए र ि ड\n",
              "19    19          ऐ स ि न ् ज ़\n",
              "20    20                  अ ज ी\n",
              "21    21              अ झ ा य े\n",
              "22    22            अ ज ि ठ ा ं\n",
              "23    23              अ क र म म\n",
              "24    24              ए क े ल े\n",
              "25    25          ख ृ क़ ़ र ी )\n",
              "26    26        अ क ो ड ि य ा ब\n",
              "27    27          प क ु र द ि य\n",
              "28    28          अ ल स ् त ा ह\n",
              "29    29        अ ल ् ब न ी य ा\n",
              "..   ...                    ...\n",
              "970  970  व ् र े ल ि स ् ल े ग\n",
              "971  971      व ा ह ि ड ु ल ् ल\n",
              "972  972                व ल ा न\n",
              "973  973              व ा ल ा ह\n",
              "974  974    व ै ल ि ं ग ् ट न न\n",
              "975  975              व ं द ा ण\n",
              "976  976        व ि स म ् म ि ड\n",
              "977  977             व ि क ् स \n",
              "978  978        व ि थ ि ं ग ं ग\n",
              "979  979            व ि भ ट न म\n",
              "980  980          व ि ट े न प न\n",
              "981  981          व ू प ो द ् म\n",
              "982  982  व ि ज ़ स ् र ि य ा र\n",
              "983  983    व ि ल ि य म ् स ो न\n",
              "984  984      व ि श र ् ट ् य ो\n",
              "985  985      व ि स न ् स ि क ी\n",
              "986  986                व ि थ ा\n",
              "987  987              व ो ह ा ट\n",
              "988  988      व ो ल ् भ र ै ं ड\n",
              "989  989                व ू ल े\n",
              "990  990  व ् ट म ै ग ् र ब ा ग\n",
              "991  991            य ड ो न ् न\n",
              "992  992              य श ो ध र\n",
              "993  993      य ो ग ि न ् द ् र\n",
              "994  994            ज ़ ब र ् ब\n",
              "995  995                ज़ ह र ी\n",
              "996  996        ज़ े ल ् व े ग र\n",
              "997  997    ज ़ ़ प ् र े ल ि न\n",
              "998  998    ज़ ि न द ग ि य ा ं द\n",
              "999  999          ज ़ ि य न ो न\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "4m1GIIis-i0E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "pred_s.to_csv('/content/gdrive/My Drive/submission_pa3.csv', index = False)\n",
        "files.download('/content/gdrive/My Drive/submission_pa3.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5tubxZ3jIts",
        "colab_type": "code",
        "outputId": "f73d2c47-fa7a-4d22-9d72-1b85d69bfd43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        " train['HIN'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'र ा स व ि ह ा र ी'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "AuSHwqsKek9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}