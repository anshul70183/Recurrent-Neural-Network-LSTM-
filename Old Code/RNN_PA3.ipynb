{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_PA3",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "imXc7RCu-at0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from collections import OrderedDict\n",
        "from itertools import chain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3Uyb4HA-qKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe6bda99-8572-4bed-ebbf-1a58913d3ef4"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9qiGRxGP-r_R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('drive/My Drive/data_PA3/train.csv')\n",
        "valid= pd.read_csv('drive/My Drive/data_PA3/valid.csv')\n",
        "test = pd.read_csv('drive/My Drive/data_PA3/partial_test_400.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_TD-gK-grSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "923b6edd-bd10-4684-dec0-11fbc4888576"
      },
      "cell_type": "code",
      "source": [
        "#finding volcaublary size for English from the training data\n",
        "train_eng = train[\"ENG\"]\n",
        "# print(train_temp)\n",
        "train_eng = np.array(train_eng)\n",
        "print(train_eng.shape)\n",
        "\n",
        "p = list(OrderedDict.fromkeys(chain.from_iterable(train_eng)))\n",
        "input_vocab_size = len(p)+2\n",
        "print(input_vocab_size)\n",
        "print(p)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13122,)\n",
            "46\n",
            "['R', ' ', 'A', 'S', 'V', 'I', 'H', 'E', 'D', 'O', 'G', 'N', '_', 'T', 'U', 'M', 'J', 'B', 'L', 'C', 'Y', 'K', 'Z', 'F', '-', 'W', 'P', 'Q', \"'\", 'X', '^', '/', '6', '(', ')', ',', 'É', '2', '.', '?', '4', 'Á', 'È', '1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UVyNT9Py55BD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "7d1e0568-5b88-4499-cf4b-6650e27f24d1"
      },
      "cell_type": "code",
      "source": [
        "#finding volcaublary size for Hindi from the training data\n",
        "train_hin = train[\"HIN\"]\n",
        "# print(train_temp)\n",
        "train_hin = np.array(train_hin)\n",
        "print(train_hin.shape)\n",
        "\n",
        "p = list(OrderedDict.fromkeys(chain.from_iterable(train_hin)))\n",
        "output_vocab_size = len(p)+2\n",
        "print(output_vocab_size)\n",
        "print(p)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13122,)\n",
            "86\n",
            "['र', ' ', 'ा', 'स', 'व', 'ि', 'ह', 'ी', 'द', 'े', 'ग', 'न', '_', 'ो', 'ड', 'श', 'त', '्', 'ु', 'म', 'ज', 'ब', 'ै', 'ल', 'क', 'ॉ', '़', 'य', 'फ', 'ट', 'इ', 'ज़', 'ऑ', 'ं', 'थ', '-', 'ू', 'अ', 'ध', 'प', '\\u200d', 'छ', 'च', 'औ', 'ई', 'ॅ', 'आ', 'ख', 'ढ', 'ढ़', 'उ', 'झ', 'ँ', 'भ', 'ौ', 'ष', 'ण', 'घ', 'ठ', 'ए', 'ृ', 'क़', 'ऋ', 'ओ', 'ऐ', '/', 'ड़', '6', '(', ')', ',', 'ञ', \"'\", '2', '.', 'ऊ', '?', 'फ़', ':', '4', 'ख़', 'ग़', '1', '॥']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Aa7evzVURFL0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inembsize = 256\n",
        "# embedding_matrix = tf.Variable(tf.random.normal([vocab_size,inembsize],-1.0,1.0)) ##embedding matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0Bc0nZP-zu2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# embedding_matrix is a tensor of shape [vocabulary_size, embedding size]\n",
        "# word_embeddings = tf.nn.embedding_lookup(embed_matrix, [3])\n",
        "# print(word_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NufNFh05eSnS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5rI9EU-qc_su",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oi5V1tXSdDEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EPkld2qwnGP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "building encoder\n"
      ]
    },
    {
      "metadata": {
        "id": "2we3U3EanIW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build RNN cell\n",
        "# encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zAg2bE-P4dXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "01a3e736-7a7f-458b-a0a9-b49240fc669b"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "num_batches = len(train)/batch_size\n",
        "num_epochs = 3\n",
        "\n",
        "X_MAX_LENGTH = max([len(train_eng[_]) for _ in range(len(train_eng))])\n",
        "Y_MAX_LENGTH = max([len(train_hin[_]) for _ in range(len(train_hin))])\n",
        "\n",
        "num_epochs = 10\n",
        "state_size = 512  #64, 128, 256 was a good number for linux OS!\n",
        "encoder_layers = 1\n",
        "decoder_layers = 1\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0.1\n",
        "\n",
        "\n",
        "state_size = 512\n",
        "\n",
        "encoder_x = tf.placeholder(dtype=tf.int32, shape=[batch_size, X_MAX_LENGTH]) #[batch_size, X_MAX_LENGTH]\n",
        "decoder_x = tf.placeholder(dtype=tf.int32, shape=[batch_size, Y_MAX_LENGTH, output_vocab_size]) #[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "y = tf.placeholder(dtype=tf.float32, shape=[batch_size, Y_MAX_LENGTH, output_vocab_size])#[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "init_state = tf.placeholder(tf.float32, [encoder_layers, 2, batch_size, state_size])\n",
        "\n",
        "print('[CONFIG] LSTM Sequence-to-Sequence Translation Model Configuration:')\n",
        "print('[CONFIG] Batch size: {}'.format(batch_size))\n",
        "print('[CONFIG] Number of Hidden Layers (Encoder/Decoder): {}/{}'.format(encoder_layers, decoder_layers))\n",
        "print('[CONFIG] Hidden Layer State Size: {}'.format(state_size))\n",
        "print('[CONFIG] X Placeholder Shape: [{}, {}]'.format('None', 'None'))\n",
        "print('[CONFIG] Y Placeholder Shape: [{}, {}, {}]\\n'.format('None', 1, output_vocab_size))\n",
        "print(X_MAX_LENGTH)\n",
        "print(Y_MAX_LENGTH)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CONFIG] LSTM Sequence-to-Sequence Translation Model Configuration:\n",
            "[CONFIG] Batch size: 8\n",
            "[CONFIG] Number of Hidden Layers (Encoder/Decoder): 1/1\n",
            "[CONFIG] Hidden Layer State Size: 512\n",
            "[CONFIG] X Placeholder Shape: [None, None]\n",
            "[CONFIG] Y Placeholder Shape: [None, 1, 86]\n",
            "\n",
            "121\n",
            "123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rtmkDkmzyWoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########\n",
        "#\n",
        "# METHOD: vectorize_data()\n",
        "# DESCRIPTION: Transform sequences from word_id form into one_hot form.\n",
        "# PARAMS:\n",
        "#           word_sentences: sequences of word_ids\n",
        "#           max_length: MAX_LENGTH of sentences w/ padding.\n",
        "#           word_to_idx: data structure containing word mappings to their id representation.\n",
        "# RETURNS:\n",
        "#           sequences: Batch of one_hot sequences.\n",
        "#\n",
        "########\n",
        "def vectorize_data(word_sentences, max_length, word_to_idx):\n",
        "    sequences = np.zeros((len(word_sentences), max_length, len(word_to_idx)), dtype=float)\n",
        "    for i, sentence in enumerate(word_sentences):\n",
        "        for j, word in enumerate(sentence):\n",
        "            sequences[i, j, word] = 1.\n",
        "    return sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SeZ4DeG9yaf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########\n",
        "#\n",
        "# METHOD: generate_text()\n",
        "# DESCRIPTION: Translate LSTM output probabilities into target vocabulary using predefined dictionary.\n",
        "#              *Avoid printing 'UNK' characters!*\n",
        "# PARAMS:\n",
        "#           prediction: array of probabilistic values that correspond to LSTM estimation outputs.\n",
        "#           batch_size: number of parallel sequences processed.\n",
        "#           length: MAX_LENGTH of each sequence w/ padding.\n",
        "#           vocab_size: Size of target vocabulary - corresponds to number of classes within network estimations.\n",
        "#           idx_to_word: Word ids mapped to corresponding words for translation.\n",
        "# RETURNS:\n",
        "#           batch_sequence: Batch of translated sentences.\n",
        "#\n",
        "########\n",
        "def generate_text(prediction, batch_size, length, vocab_size, idx_to_word):\n",
        "\n",
        "    batch_softmax = np.reshape(prediction, [batch_size, length, vocab_size])\n",
        "    batch_sentence = []\n",
        "\n",
        "    for sequence in batch_softmax:\n",
        "        word_sequence = ''\n",
        "        for char in sequence:\n",
        "            vector_position = np.argmax(char)\n",
        "            y_word = idx_to_word[vector_position]\n",
        "            if y_word != 'ZERO':\n",
        "                word_sequence = word_sequence + y_word + ' '\n",
        "            else:\n",
        "                word_sequence = word_sequence + ''\n",
        "        batch_sentence.append(word_sequence)\n",
        "\n",
        "    return batch_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HvqFl_c8Buw2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def variable_summaries(var):\n",
        "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "  with tf.name_scope('summaries'):\n",
        "    mean = tf.reduce_mean(var)\n",
        "    tf.summary.scalar('mean', mean)\n",
        "    with tf.name_scope('stddev'):\n",
        "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "    tf.summary.scalar('stddev', stddev)\n",
        "    tf.summary.scalar('max', tf.reduce_max(var))\n",
        "    tf.summary.scalar('min', tf.reduce_min(var))\n",
        "    tf.summary.histogram('histogram', var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pKWF3Y0I4ddt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_model(data):\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "\n",
        "        with tf.variable_scope('encoder_word_embeddings'):\n",
        "\n",
        "            word_embeddings = tf.get_variable('encoder_word_embeddings', [input_vocab_size, inembsize])\n",
        "            encoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, encoder_x)\n",
        "            encoder_embedded_word_ids = tf.reshape(encoder_embedded_word_ids, [-1, X_MAX_LENGTH, inembsize])\n",
        "\n",
        "        with tf.variable_scope('encoder'):\n",
        "            ####\n",
        "            #\n",
        "            # LSTM ENCODER\n",
        "            #\n",
        "            ####\n",
        "            # Forward passes\n",
        "            state_per_layer_list = tf.unstack(init_state, axis=0)\n",
        "            rnn_tuple_state = tuple(\n",
        "                [tf.nn.rnn_cell.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
        "                for idx in range(encoder_layers)])\n",
        "\n",
        "            encoder_stacked_cell = []\n",
        "\n",
        "            for _ in range(encoder_layers):\n",
        "                encoder_single_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "                if _ == 0:\n",
        "                    with tf.name_scope('encoder_dropout') as scope:\n",
        "                        encoder_single_cell = tf.nn.rnn_cell.DropoutWrapper(encoder_single_cell,\n",
        "                                                        output_keep_prob=0.75)  # add dropout to first LSTM layer only.\n",
        "                encoder_stacked_cell.append(encoder_single_cell)\n",
        "\n",
        "            encoder_cell = tf.nn.rnn_cell.MultiRNNCell(encoder_stacked_cell, state_is_tuple=True)\n",
        "\n",
        "            encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell=encoder_cell,\n",
        "                                                             inputs=encoder_embedded_word_ids,\n",
        "                                                             initial_state=rnn_tuple_state)\n",
        "\n",
        "            del encoder_outputs, encoder_cell, state_per_layer_list, encoder_stacked_cell\n",
        "            \n",
        "            print(\"HIHIHIHI\")\n",
        "            print(encoder_final_state)\n",
        "            \n",
        "#             print(\"HIHIHIHI\")\n",
        "#             print(encoder_final_state.shape)\n",
        "            \n",
        "\n",
        "        with tf.variable_scope('decoder_word_embeddings'):\n",
        "\n",
        "            word_embeddings = tf.get_variable('decoder_word_embeddings', [output_vocab_size, inembsize])\n",
        "            decoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, decoder_x)\n",
        "            decoder_embedded_word_ids = tf.reshape(decoder_embedded_word_ids,\n",
        "                                                        [-1, Y_MAX_LENGTH, output_vocab_size * inembsize])\n",
        "\n",
        "        with tf.variable_scope('decoder'):\n",
        "            ####\n",
        "            #\n",
        "            # LSTM DECODER\n",
        "            #\n",
        "            ####\n",
        "            decoder_stacked_cell = []\n",
        "            for _ in range(decoder_layers):\n",
        "                decoder_single_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "                if _ == 0:\n",
        "                    with tf.name_scope('decoder_dropout') as scope:\n",
        "                        decoder_single_cell = tf.nn.rnn_cell.DropoutWrapper(decoder_single_cell,\n",
        "                                                    output_keep_prob=0.75)  # add dropout to first LSTM layer only.\n",
        "                decoder_stacked_cell.append(decoder_single_cell)\n",
        "\n",
        "            decoder_cell = tf.nn.rnn_cell.MultiRNNCell(decoder_stacked_cell, state_is_tuple=True)\n",
        "\n",
        "            decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(cell=decoder_cell,\n",
        "                                                             inputs=decoder_embedded_word_ids,\n",
        "                                                             initial_state=encoder_final_state)\n",
        "\n",
        "            outputs = tf.reshape(decoder_outputs, [-1, state_size])\n",
        "\n",
        "            with tf.name_scope('decoder_hidden_states') as scope:\n",
        "                W2 = tf.Variable(tf.random_normal([state_size, output_vocab_size]), dtype=tf.float32)\n",
        "                variable_summaries(W2)\n",
        "                b2 = tf.Variable(tf.zeros([1, output_vocab_size]), dtype=tf.float32)\n",
        "                variable_summaries(b2)\n",
        "\n",
        "                logits = tf.matmul(outputs, W2) + b2 # Broadcasted addition\n",
        "                tf.summary.histogram('pre_activations', logits)\n",
        "                prediction = tf.nn.softmax(logits)\n",
        "                tf.summary.histogram('activations', prediction)\n",
        "            labels = y\n",
        "\n",
        "    return decoder_final_state, labels , logits, prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bn9keAIChKrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(inputs = train_eng, labels = train_hin, vocabulary_size = 100):\n",
        "\n",
        "    print('[INFO] Importing RAW Inputs and Labels...')\n",
        "    x_data_source = open(inputs, 'r')\n",
        "    y_data_source = open(labels, 'r')\n",
        "\n",
        "    # Feed input data in backwards for better translation performance.\n",
        "    x_data = (tf.compat.as_str(x_data_source.read()).lower().split(' '))\n",
        "    x_data = [x_data[_].split(' ')[::-1] for _ in range(len(x_data))]\n",
        "    X_DATA_SIZE = len(x_data)\n",
        "    print('[NOTIFICATION] Here is a snippet of the Inputs!')\n",
        "    print(x_data[0:2])\n",
        "\n",
        "    y_data = (tf.compat.as_str(y_data_source.read()).lower().split(' '))\n",
        "    y_data = [y_data[_].split(' ') for _ in range(len(y_data))]\n",
        "    Y_DATA_SIZE = len(y_data)\n",
        "    print('[NOTIFICATION] Here is a snippet of the Labels!')\n",
        "    print(y_data[0:2])\n",
        "\n",
        "\n",
        "    print('[INFO] Creating Dictionary Indexes...')\n",
        "    #Word Dictionary\n",
        "    x_word_list = []\n",
        "    y_word_list = []\n",
        "    [x_word_list.extend(x_data[_]) for _ in range(len(x_data))]\n",
        "    [y_word_list.extend(y_data[_]) for _ in range(len(y_data))]\n",
        "\n",
        "    x_word_dictionary = []\n",
        "    y_word_dictionary = []\n",
        "    print('[INFO] Finding most common vocabulary...')\n",
        "    print('[INFO] The common vocabulary threshold is set to: {}'.format(vocabulary_size))\n",
        "    x_word_dictionary.extend(collections.Counter(x_word_list).most_common(44))\n",
        "    y_word_dictionary.extend(collections.Counter(y_word_list).most_common(84))\n",
        "\n",
        "    print('[INFO] Clearing up memory...')\n",
        "    del x_word_list, y_word_list\n",
        "\n",
        "    print('[INFO] Mapping vocabulary to idx...')\n",
        "    #word/idx mapping\n",
        "    x_idx_to_word = [word[0] for idx, word in enumerate(x_word_dictionary)]\n",
        "    x_idx_to_word.insert(0, '#')\n",
        "    x_idx_to_word.append('$')\n",
        "    print('[NOTIFICATION] Here is a snippet of the Input Dictionary!')\n",
        "    print(x_idx_to_word[0:20])\n",
        "\n",
        "    y_idx_to_word = [word[0] for idx, word in enumerate(y_word_dictionary)]\n",
        "    y_idx_to_word.insert(0, '#') #at the first \n",
        "    y_idx_to_word.append('$') #at the last\n",
        "    print('[NOTIFICATION] Here is a snippet of the Label Dictionary!')\n",
        "    print(y_idx_to_word[0:20])\n",
        "\n",
        "    x_word_to_idx = {word:ix for ix, word in enumerate(x_idx_to_word)}\n",
        "    y_word_to_idx = {word: ix for ix, word in enumerate(y_idx_to_word)}\n",
        "\n",
        "    X_VOCAB_SIZE = len(x_word_dictionary) + 2\n",
        "    Y_VOCAB_SIZE = len(y_word_dictionary) + 2\n",
        "\n",
        "    print('[INFO] Converting vocabulary to index value...')\n",
        "    # Converting each character to its index value\n",
        "    for i, word in enumerate(x_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in x_word_to_idx:\n",
        "                x_data[i][j] = x_word_to_idx[charac]\n",
        "            else:\n",
        "                x_data[i][j] = x_word_to_idx['$']\n",
        "    print('[INFO] Input conversion complete!')\n",
        "    print('[NOTIFICATION] Here is a snippet of the Input Index Values!')\n",
        "    print(x_data[0:2])\n",
        "\n",
        "    for i, word in enumerate(y_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in y_word_to_idx:\n",
        "                y_data[i][j] = y_word_to_idx[charac]\n",
        "            else:\n",
        "                y_data[i][j] = y_word_to_idx['$']\n",
        "    print('[INFO] Label conversion complete!')\n",
        "    print('[NOTIFICATION] Here is a snippet of the Label Index Values!')\n",
        "    print(y_data[0:2])\n",
        "\n",
        "    X_MAX_LENGTH = max([len(x_data[_]) for _ in range(len(x_data))])\n",
        "    Y_MAX_LENGTH = max([len(y_data[_]) for _ in range(len(y_data))])\n",
        "\n",
        "    del x_data, y_data\n",
        "\n",
        "    return X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpmDEaRYxFcZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data_by_batch(batch_number, x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin,\n",
        "                       batch_size = batch_size):\n",
        "\n",
        "    start_batch = batch_number * batch_size\n",
        "    end_batch = start_batch + batch_size\n",
        "\n",
        "    print('\\n[INFO] Importing RAW Inputs and Labels for batch...')\n",
        "    x_data_source = open(inputs, 'r')\n",
        "    y_data_source = open(labels, 'r')\n",
        "\n",
        "    #Feed input data in backwards for better translation performance.\n",
        "    x_data = (tf.compat.as_str(x_data_source.read()).lower().split(' '))\n",
        "    x_data = [x_data[_].split(' ')[::-1] for _ in range(len(x_data))]\n",
        "    x_data = x_data[start_batch:end_batch]\n",
        "\n",
        "    y_data = (tf.compat.as_str(y_data_source.read()).lower().split(' '))\n",
        "    y_data = [y_data[_].split(' ') for _ in range(len(y_data))]\n",
        "    y_data = y_data[start_batch:end_batch]\n",
        "\n",
        "    # Converting each character to its index value\n",
        "    for i, word in enumerate(x_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in x_word_to_idx:\n",
        "                x_data[i][j] = x_word_to_idx[charac]\n",
        "            else:\n",
        "                x_data[i][j] = x_word_to_idx['$']\n",
        "\n",
        "    for i, word in enumerate(y_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in y_word_to_idx:\n",
        "                y_data[i][j] = y_word_to_idx[charac]\n",
        "            else:\n",
        "                y_data[i][j] = y_word_to_idx['$']\n",
        "\n",
        "    return x_data, y_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHB0cjBeCG_S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "restore = False\n",
        "ckpt_model_directory = 'drive/My Drive/data_files/training/saved_models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hKX06qd1BCNV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_train():\n",
        "\n",
        "    #Initialize CPU config..\n",
        "#     config = config_tensorflow_hardware()\n",
        "\n",
        "    current_state, labels, logits, prediction = lstm_model(encoder_x)\n",
        "\n",
        "    with tf.name_scope('cross_entropy') as scope:\n",
        "        cost = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
        "        total_cost = tf.reduce_mean(cost)\n",
        "        tf.summary.scalar('cross-entropy', total_cost)\n",
        "\n",
        "    with tf.name_scope('optimizer') as scope:\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(total_cost)\n",
        "\n",
        "    #Keep previous 30 model checkpoints..\n",
        "    saver = tf.train.Saver(max_to_keep=30)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        merged = tf.summary.merge_all()\n",
        "#         writer = tf.summary.FileWriter(logs_path, graph=sess.graph)\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        epoch = 0\n",
        "\n",
        "#         if restore:\n",
        "#             print('Loading variables from {}'.format(ckpt_model_directory + nn_config.meta_file + nn_config.meta_num + nn_config.meta_ext))\n",
        "#             saver.restore(sess, nn_config.ckpt_model_directory + nn_config.meta_file + nn_config.meta_num)\n",
        "\n",
        "        _current_state = np.zeros((encoder_layers, 2, batch_size, state_size))\n",
        "\n",
        "        while True: #Unlimited epochs for Training\n",
        "\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for batch in range(num_batches):\n",
        "\n",
        "                batch_x, batch_y = load_data_by_batch(batch, x_word_to_idx, y_word_to_idx)\n",
        "\n",
        "                batch_loss = 0\n",
        "\n",
        "                print('[INFO] Zero padding RAW batch data...'.format(batch))\n",
        "                for _ in range(len(batch_x)):\n",
        "                    x_length = len(batch_x[_])\n",
        "                    y_length = len(batch_y[_])\n",
        "\n",
        "                    batch_x[_].extend(np.zeros([X_MAX_LENGTH - x_length], dtype=int))\n",
        "                    batch_y[_].extend(np.zeros([Y_MAX_LENGTH - y_length], dtype=int))\n",
        "\n",
        "                y_one_hot = vectorize_data(batch_y, Y_MAX_LENGTH, y_word_to_idx)\n",
        "\n",
        "                del batch_y\n",
        "\n",
        "                print('[INFO] Passing batch values into Encoder/Decoder for LSTM network optimization...'.format(batch))\n",
        "\n",
        "                summary, _total_cost, _train_step, _current_state, _prediction_series = sess.run(\n",
        "                [merged, total_cost, optimizer, current_state, prediction],\n",
        "                feed_dict={\n",
        "                        encoder_x: batch_x,\n",
        "                        decoder_x: y_one_hot,\n",
        "                        y: y_one_hot,\n",
        "                        init_state: _current_state\n",
        "                    })\n",
        "\n",
        "#                 writer.add_summary(summary, epoch)\n",
        "\n",
        "                if batch % 2 == 0:\n",
        "\n",
        "                    print('[INFO] Translating Softmax Probabilities...')\n",
        "\n",
        "                    text = generate_text(_prediction_series, batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE, y_idx_to_word)\n",
        "\n",
        "                    if epoch % 5 == 0:\n",
        "                        save_output_path = 'data_files/training/encoder-decoder-output/TRAINING_TRANSLATION_OUTPUT_EPOCH_{}_STATE-SIZE_{}_NUM-LAYERS_{}_LEARNING-RATE{}_EMBEDDING-SIZE_{}.txt'.format(epoch, state_size, decoder_layers, nn_config.learning_rate, nn_config.embedding_size)\n",
        "\n",
        "                        with open(save_output_path,'wb') as f:\n",
        "                         for item in text:\n",
        "                             f.write(\"%s\\n\" % item)\n",
        "                        print('[NOTIFICATION] Translation file saved to: {}'.format(save_output_path))\n",
        "\n",
        "                    print('[INFO]: Batch {} optimized output for Epoch {}:'.format(batch, epoch))\n",
        "                    for _ in range(batch_size):\n",
        "                        print(text[_])\n",
        "\n",
        "                batch_loss += _total_cost\n",
        "                epoch_loss += _total_cost\n",
        "\n",
        "                print('\\n[STATUS] Batch {}/{} complete! Batch Loss: {}'.format(batch, num_batches, batch_loss))\n",
        "\n",
        "            if epoch % 1 == 0:\n",
        "\n",
        "                save_model_path = \"data_files/training/saved_models/translation_model\"\n",
        "                if epoch == 0:\n",
        "                    save_path = saver.save(sess, save_model_path, epoch)\n",
        "                else:\n",
        "                    save_path = saver.save(sess, save_model_path, epoch, write_meta_graph=False)\n",
        "\n",
        "                print(\"[STATUS] Model ckpt saved in file: %s\" % save_path)\n",
        "\n",
        "            print('[STATUS] Epoch {} complete! Epoch Loss: {}'.format(epoch, epoch_loss))\n",
        "\n",
        "            saver.save(sess, logs_path + '/model.ckpt', epoch)\n",
        "            epoch+=1\n",
        "\n",
        "# lstm_train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Ikyz0rp0Aid",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "outputId": "d1d30ac6-3045-4cae-d3ae-3ef058eed758"
      },
      "cell_type": "code",
      "source": [
        "lstm_train()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-12-c311c303a56d>:26: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-12-c311c303a56d>:33: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-12-c311c303a56d>:37: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "HIHIHIHI\n",
            "(LSTMStateTuple(c=<tf.Tensor 'encoder/rnn/while/Exit_3:0' shape=(8, 512) dtype=float32>, h=<tf.Tensor 'encoder/rnn/while/Exit_4:0' shape=(8, 512) dtype=float32>),)\n",
            "WARNING:tensorflow:From <ipython-input-16-5c7a2e59966b>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-232fa46a8724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-5c7a2e59966b>\u001b[0m in \u001b[0;36mlstm_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_by_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_word_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_word_to_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JBk-gXZ60BZm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}