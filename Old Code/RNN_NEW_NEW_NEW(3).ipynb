{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_NEW_NEW_NEW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "L9ixfY61Bege",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "imXc7RCu-at0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from collections import OrderedDict\n",
        "from itertools import chain\n",
        "import collections\n",
        "from tensorflow.contrib import rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3Uyb4HA-qKs",
        "colab_type": "code",
        "outputId": "7dc2e80c-9c8a-4a80-d5a3-8b024dc36008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EPkld2qwnGP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "building encoder\n"
      ]
    },
    {
      "metadata": {
        "id": "GW0tXdQqntUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "batch_size = 128\n",
        "init = 1\n",
        "dropout_prob= 0\n",
        "decode_method=1\n",
        "beam_width = 10\n",
        "save_dir = 'drive/My Drive/data_PA3/'\n",
        "num_epochs = 1\n",
        "train_path = 'drive/My Drive/data_PA3/train.csv'\n",
        "valid_path = 'drive/My Drive/data_PA3/valid.csv'\n",
        "test_path = 'drive/My Drive/data_PA3/partial_test_400.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qiGRxGP-r_R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(train_path)\n",
        "valid= pd.read_csv(valid_path)\n",
        "test = pd.read_csv(test_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zAg2bE-P4dXH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inembsize = 256\n",
        "outembsize = 256\n",
        "num_batches = len(train)//batch_size\n",
        "encoder_layers = 1\n",
        "decoder_layers = 2\n",
        "state_size = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GSxI6R1Zdc2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_eng = train['ENG']\n",
        "train_hin = train[\"HIN\"]\n",
        "\n",
        "valid_eng = valid[\"ENG\"]\n",
        "valid_hin = valid[\"HIN\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtmkDkmzyWoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot(batch_word, max_length, word_to_idx):\n",
        "    vec_word = np.zeros((len(batch_word), max_length, len(word_to_idx)), dtype=float)\n",
        "    for i, word in enumerate(batch_word):\n",
        "        for j, char in enumerate(word):\n",
        "            vec_word[i, j, char] = 1.\n",
        "    return vec_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SeZ4DeG9yaf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(prediction, batch_size, length, vocab_size, idx_to_word):\n",
        "\n",
        "    batch_softmax = np.reshape(prediction, [batch_size, length, vocab_size])\n",
        "    batch_word = []\n",
        "\n",
        "    for word in batch_softmax:\n",
        "        new_word = ''\n",
        "        for char in word:\n",
        "            vector_position = np.argmax(char)\n",
        "            y_word = idx_to_word[vector_position]\n",
        "            if y_word != 'PAD':\n",
        "                new_word = new_word + y_word + ' '\n",
        "            else:\n",
        "                new_word = new_word + ''\n",
        "        batch_word.append(new_word)\n",
        "\n",
        "    return batch_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HvqFl_c8Buw2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## To remove later\n",
        "\n",
        "# def variable_summaries(var):\n",
        "#   \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "#   with tf.name_scope('summaries'):\n",
        "#     mean = tf.reduce_mean(var)\n",
        "#     tf.summary.scalar('mean', mean)\n",
        "#     with tf.name_scope('stddev'):\n",
        "#       stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "#     tf.summary.scalar('stddev', stddev)\n",
        "#     tf.summary.scalar('max', tf.reduce_max(var))\n",
        "#     tf.summary.scalar('min', tf.reduce_min(var))\n",
        "#     tf.summary.histogram('histogram', var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bn9keAIChKrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(train_eng, train_hin):\n",
        "    \n",
        "    # Feed input data in backwards for better translation performance.\n",
        "    x_data = [(tf.compat.as_str(train_eng[_]).lower().split(' ')) for _ in range(len(train_eng))]\n",
        "    x_data = [list(x_data[_]) for _ in range(len(x_data))]\n",
        "    X_DATA_SIZE = len(x_data)\n",
        "\n",
        "    y_data = [(tf.compat.as_str(train_hin[_]).lower().split(' ')) for _ in range(len(train_hin))]\n",
        "    y_data = [list(y_data[_]) for _ in range(len(y_data))]\n",
        "    Y_DATA_SIZE = len(y_data)\n",
        "\n",
        "\n",
        "    x_word_list = []\n",
        "    y_word_list = []\n",
        "    [x_word_list.extend(x_data[_]) for _ in range(len(x_data))]\n",
        "    [y_word_list.extend(y_data[_]) for _ in range(len(y_data))]\n",
        "\n",
        "    x_word_dictionary = []\n",
        "    y_word_dictionary = []\n",
        "    x_word_dictionary.extend(collections.Counter(x_word_list).most_common(44))\n",
        "    y_word_dictionary.extend(collections.Counter(y_word_list).most_common(84))\n",
        "    \n",
        "\n",
        "    x_idx_to_word = [word[0] for idx, word in enumerate(x_word_dictionary)]\n",
        "    x_idx_to_word.insert(0, 'GO')\n",
        "    x_idx_to_word.insert(1, 'EOS')\n",
        "    x_idx_to_word.insert(2, 'PAD')\n",
        "    x_idx_to_word.insert(3, 'UK')\n",
        "\n",
        "\n",
        "    y_idx_to_word = [word[0] for idx, word in enumerate(y_word_dictionary)]\n",
        "    y_idx_to_word.insert(0, 'GO')\n",
        "    y_idx_to_word.insert(1, 'EOS')\n",
        "    y_idx_to_word.insert(2, 'PAD')\n",
        "    y_idx_to_word.insert(3, 'UK')\n",
        "\n",
        "    x_word_to_idx = {word:ix for ix, word in enumerate(x_idx_to_word)}\n",
        "    y_word_to_idx = {word: ix for ix, word in enumerate(y_idx_to_word)}\n",
        "\n",
        "    X_VOCAB_SIZE = len(x_word_dictionary) + 4\n",
        "    Y_VOCAB_SIZE = len(y_word_dictionary) + 4\n",
        "\n",
        "    X_MAX_LENGTH = max([len(x_data[_]) for _ in range(len(x_data))])\n",
        "    Y_MAX_LENGTH = max([len(y_data[_]) for _ in range(len(y_data))])\n",
        "\n",
        "    return X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOPM5vzBQtTF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_VOCAB_SIZE, Y_VOCAB_SIZE, x_idx_to_word, x_word_to_idx, y_idx_to_word, y_word_to_idx, X_MAX_LENGTH, Y_MAX_LENGTH, X_DATA_SIZE = load_data(train_eng, train_hin)                                                                                                                   \n",
        "\n",
        "X_VOCAB_SIZE_VALID, Y_VOCAB_SIZE_VALID, x_idx_to_word_VALID, x_word_to_idx_VALID, y_idx_to_word_VALID, y_word_to_idx_VALID, X_MAX_LENGTH_VALID, Y_MAX_LENGTH_VALID, X_DATA_SIZE_VALID = load_data(valid_eng, valid_hin)                                                                                                                   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpmDEaRYxFcZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin):\n",
        "\n",
        "\n",
        "    x_data = inputs\n",
        "    y_data = labels\n",
        "\n",
        "    x_data = [(tf.compat.as_str(x_data[_]).lower().split(' ')) for _ in range(len(x_data))]\n",
        "    x_data = [list(x_data[_]) for _ in range(len(x_data))]\n",
        "       \n",
        "    y_data = [(tf.compat.as_str(y_data[_]).lower().split(' ')) for _ in range(len(y_data))]\n",
        "    y_data = [list(y_data[_]) for _ in range(len(y_data))]\n",
        "\n",
        "    for i, word in enumerate(x_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in x_word_to_idx:\n",
        "                x_data[i][j] = x_word_to_idx[charac]\n",
        "            else:\n",
        "                x_data[i][j] = x_word_to_idx['UK']\n",
        "\n",
        "    for i, word in enumerate(y_data):\n",
        "        for j, charac in enumerate(word):\n",
        "            if charac in y_word_to_idx:\n",
        "                y_data[i][j] = y_word_to_idx[charac]\n",
        "            else:\n",
        "                y_data[i][j] = y_word_to_idx['UK']\n",
        "                \n",
        "    return x_data, y_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZFb7VoMHoI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data , y_data = get_data( x_word_to_idx, y_word_to_idx, inputs = train_eng, labels = train_hin)                                              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFNbGZXhgJ-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data_val , y_data_val = get_data(x_word_to_idx, y_word_to_idx, inputs = valid_eng, labels = valid_hin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a13EGvTatGim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding(encoder_x):\n",
        "  \n",
        "      word_embeddings = tf.get_variable('encoder_word_embeddings', [X_VOCAB_SIZE, inembsize])\n",
        "      encoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, encoder_x)\n",
        "      encoder_embedded_word_ids = tf.reshape(encoder_embedded_word_ids, [-1, X_MAX_LENGTH, inembsize])\n",
        "      \n",
        "      encoder_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "      \n",
        "#       def attention():\n",
        "#             attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units = state_size, \n",
        "#                                                                     memory = encoder_embedded_word_ids)\n",
        "          \n",
        "#             return tf.contrib.seq2seq.AttentionWrapper(cell = encoder_cell, \n",
        "#                                                         attention_mechanism = attention_mechanism,\n",
        "#                                                         attention_layer_size = size_layer//2)\n",
        "      \n",
        "      for n in range(encoder_layers): \n",
        "\n",
        "                (out_fw, out_bw), encoder_state  = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = encoder_cell,  ## initializer, reuse to be set\n",
        "                cell_bw = encoder_cell,  ## initializer, reuse to be set\n",
        "                inputs = encoder_embedded_word_ids,\n",
        "                dtype = tf.float32)\n",
        "        \n",
        "#       print(tf.shape(encoder_out))\n",
        "#       tf.concat([encoder_out[0],encoder_out[1]], 1)  \n",
        "#       print(tf.shape(encoder_out))\n",
        "      \n",
        "#       tf.concat([encoder_out[0], encoder_out[1]],0)\n",
        "      \n",
        "#       print(tf.shape(encoder_out))\n",
        "\n",
        "#       tf.reshape(encoder_state, [-1, batch_size,Y_MAX_LENGTH, state_size])\n",
        "      \n",
        "  \n",
        "#       bi_state_c = tf.concat((out_fw.c, out_bw.c), -1)\n",
        "#       bi_state_h = tf.concat((out_fw.h, out_bw.h), -1)\n",
        "#       bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
        "#       encoder_output = tuple([bi_lstm_state] * num_layers)\n",
        "      encoder_output = tf.concat((out_fw, out_bw), 2)\n",
        "      #print(tf.shape(encoder_out[0])) \n",
        "      \n",
        "      return encoder_state, encoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XD2T1sQZYNwe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_decoder_input(y_data):\n",
        "\n",
        "    id = y_word_to_idx['GO']\n",
        "    \n",
        "    after_slice = tf.strided_slice(y_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    y_data_new = tf.concat( [tf.fill([batch_size, 1], id), after_slice], 1)\n",
        "    \n",
        "    return y_data_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PEcg7uIMr7qF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "attention = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rqsPIfaLuq4G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_train(decoder_embedded_word_ids, decoder_cell, encoder_state, encoder_output, dense_layer, train_sequence_length):\n",
        "  \n",
        "      training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embedded_word_ids, sequence_length = train_sequence_length)\n",
        "\n",
        "      if attention == 1:\n",
        "        # Define attention mechanism\n",
        "#         attention_states = tf.transpose(encoder_output, [1,0, 2])\n",
        "#         print(encoder_state)\n",
        "        attn_mech = tf.contrib.seq2seq.LuongAttention(\n",
        "        num_units = state_size, memory = encoder_output,\n",
        "           memory_sequence_length= train_sequence_length)\n",
        "    \n",
        "    \n",
        "#     attention_mechanism = tf.contrib.seq2seq.LuongAttention(decSize, attention_states, memory_sequence_length=None)\n",
        "#             decoder1_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=decSize)\n",
        "#             initial_state = decoder1_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
        "#             decoder1 = tf.contrib.seq2seq.BasicDecoder(decoder1_cell, helper, initial_state, output_layer=projection_layer)\n",
        "#             decoder1_outputs, decoder1_state, decoder1_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(decoder1)\n",
        "          \n",
        "    \n",
        "        # Define attention cell (Attention wrapper)\n",
        "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "        cell = decoder_cell, attention_mechanism = attn_mech, attention_layer_size =state_size,\n",
        "        alignment_history=True)\n",
        "        \n",
        "        #define initial state of the attention layer\n",
        "        initial_state = attn_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
        "        \n",
        "        #define decoder and feed in the cell as the attention wrapped cell\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "              cell = attn_cell,\n",
        "              helper = training_helper,\n",
        "              initial_state = initial_state,\n",
        "              output_layer = dense_layer)\n",
        "\n",
        "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "              decoder = training_decoder,\n",
        "              impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further\n",
        "              maximum_iterations = Y_MAX_LENGTH)   \n",
        "      \n",
        "        training_logits = training_decoder_output.rnn_output\n",
        "        training_logits_max = training_decoder_output.sample_id\n",
        "        \n",
        "      else:\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "              cell = decoder_cell,\n",
        "              helper = training_helper,\n",
        "              initial_state = encoder_state,\n",
        "              output_layer = dense_layer)\n",
        "\n",
        "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "              decoder = training_decoder,\n",
        "              impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further\n",
        "              maximum_iterations = Y_MAX_LENGTH)   \n",
        "      \n",
        "        training_logits = training_decoder_output.rnn_output\n",
        "        training_logits_max = training_decoder_output.sample_id\n",
        "      \n",
        "      return training_logits, training_logits_max"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jooQgWLFvtTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_infer(word_embeddings, decoder_cell, encoder_state, dense_layer):\n",
        "     \n",
        "      #decoder for test data\n",
        "          predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding = word_embeddings,\n",
        "                  start_tokens = tf.fill([batch_size], y_word_to_idx['GO']), end_token = y_word_to_idx['EOS'])\n",
        "            \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                  cell = decoder_cell,\n",
        "                  helper = predicting_helper,\n",
        "                  initial_state = encoder_state,\n",
        "                  output_layer = dense_layer)\n",
        "\n",
        "          predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                  decoder = predicting_decoder,\n",
        "                  impute_finished = True,\n",
        "                  maximum_iterations = Y_MAX_LENGTH)\n",
        "          \n",
        "          \n",
        "          predicting_ids = predicting_decoder_output.sample_id\n",
        "          \n",
        "          return predicting_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zt7y4-GYv-hL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding(decoder_x, encoder_state,encoder_output, train_sequence_length):\n",
        "      word_embeddings = tf.get_variable('decoder_word_embeddings', [Y_VOCAB_SIZE, outembsize])\n",
        "      decoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, decoder_x)\n",
        "      decoder_embedded_word_ids = tf.reshape(decoder_embedded_word_ids,  [-1, Y_MAX_LENGTH,  outembsize])\n",
        "      \n",
        "      decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(state_size) for _ in range(decoder_layers)])\n",
        "      dense_layer = tf.layers.Dense(Y_VOCAB_SIZE)\n",
        "      \n",
        "      with tf.variable_scope(\"decode\"):\n",
        "            train_output, train_output_max = decoding_train(decoder_embedded_word_ids, decoder_cell, encoder_state, encoder_output, dense_layer, train_sequence_length)\n",
        "          \n",
        "      \n",
        "      with tf.variable_scope(\"decode\", reuse=True):       \n",
        "            infer_output = decoding_infer(word_embeddings, decoder_cell, encoder_state, dense_layer)\n",
        "        \n",
        "      return train_output,  train_output_max, infer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mjDQz_ZQodIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(logits, labels):\n",
        "  \n",
        "    max_seq = Y_MAX_LENGTH\n",
        "    \n",
        "    for i in range(len(logits)):\n",
        "       if len(logits[i])<max_seq:\n",
        "          logits[i] = np.pad(\n",
        "            logits[i],\n",
        "            [(0,0),(0,max_seq - len(logits[i]))],\n",
        "            'constant')\n",
        "          \n",
        "    for i in range(len(labels)):\n",
        "       if len(labels[i])<max_seq:\n",
        "          labels[i] = np.pad(\n",
        "            labels[i],\n",
        "            [(0,0),(0,max_seq - len(labels[i]))],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(logits, labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gfjzdjVLUbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad(sentence_batch, pad_int, max_sentence):\n",
        "    \n",
        "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5il21DoqK03y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(x_data, y_data, batch_size, x_pad_int, y_pad_int):\n",
        "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(x_data)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "\n",
        "        # Slice the right amount for the batch\n",
        "        x_data_batch = x_data[start_i:start_i + batch_size]\n",
        "        y_data_batch = y_data[start_i:start_i + batch_size]\n",
        "\n",
        "        # Pad\n",
        "        pad_x_batch = np.array(pad(x_data_batch, x_pad_int, X_MAX_LENGTH))\n",
        "        pad_y_batch = np.array(pad(y_data_batch, y_pad_int, Y_MAX_LENGTH))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_y_lengths = []\n",
        "        for y in pad_y_batch:\n",
        "            pad_y_lengths.append(len(y))\n",
        "\n",
        "        pad_x_lengths = []\n",
        "        for x in pad_x_batch:\n",
        "            pad_x_lengths.append(len(x))\n",
        "\n",
        "        yield pad_x_batch, pad_y_batch, pad_x_lengths, pad_y_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T99fng0AzbnE",
        "colab_type": "code",
        "outputId": "c4c5f41a-2e82-4471-fe45-408c3aece112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3497
        }
      },
      "cell_type": "code",
      "source": [
        "tf_graph = tf.Graph()\n",
        "with tf_graph.as_default():\n",
        "  \n",
        "      encoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) \n",
        "      decoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) \n",
        "      #y = tf.placeholder(dtype=tf.float32, shape=[None, None, None])\n",
        "      y_sequence_length =  tf.placeholder(dtype=tf.int32, shape=[None]) \n",
        "      \n",
        "      encoder_state, encoder_output =  encoding(encoder_x)      \n",
        "      _decoder_x  = process_decoder_input(decoder_x)\n",
        "      \n",
        "      train_logits, train_logits_max,  infer_output = decoding(_decoder_x, encoder_state, encoder_output, y_sequence_length)\n",
        "      \n",
        "      \n",
        "      masks = tf.sequence_mask(y_sequence_length, Y_MAX_LENGTH, dtype=tf.float32)\n",
        "      \n",
        "      softmax_out = tf.nn.softmax(train_logits)\n",
        "      \n",
        "      #cost =  tf.nn.softmax_cross_entropy_with_logits(logits=train_logits, labels=y)\n",
        "      cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            train_logits,\n",
        "            decoder_x,\n",
        "            masks)\n",
        "      \n",
        "\n",
        "      total_cost = tf.reduce_mean(cost)\n",
        "      optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost)\n",
        "      \n",
        "    \n",
        "     # accuracy = get_accuracy(train_logits, decoder_x)\n",
        "      \n",
        "#       y_t = tf.argmax(softmax_out,axis=2)\n",
        "#       y_t = tf.cast(y_t, tf.int32)\n",
        "\n",
        "#       prediction = y_t\n",
        "#       mask_label = decoder_x\n",
        "\n",
        "#       correct_pred = tf.equal(prediction, mask_label)\n",
        "#       accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 768 and 1280 for 'decode/decoder/while/BasicDecoderStep/decoder/attention_wrapper/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,768], [1280,2048].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-fd9cd999bbaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0m_decoder_x\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mprocess_decoder_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mtrain_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_logits_max\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0minfer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-150-f81b3f8fe7cc>\u001b[0m in \u001b[0;36mdecoding\u001b[0;34m(decoder_x, encoder_state, encoder_output, train_sequence_length)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0minfer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoding_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrain_output_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-cc19924ee6cc>\u001b[0m in \u001b[0;36mdecoding_infer\u001b[0;34m(word_embeddings, decoder_cell, encoder_state, dense_layer)\u001b[0m\n\u001b[1;32m     22\u001b[0m                   \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicting_decoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                   \u001b[0mimpute_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                   maximum_iterations = Y_MAX_LENGTH)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\u001b[0m in \u001b[0;36mdynamic_decode\u001b[0;34m(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mfinal_outputs_ta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3554\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3555\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 3556\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   3557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3558\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   3085\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3086\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 3087\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   3088\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3089\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   3020\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   3021\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3022\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3023\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3523\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3524\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\u001b[0m in \u001b[0;36mbody\u001b[0;34m(time, outputs_ta, state, inputs, finished, sequence_lengths)\u001b[0m\n\u001b[1;32m    263\u001b[0m       \"\"\"\n\u001b[1;32m    264\u001b[0m       (next_outputs, decoder_state, next_inputs,\n\u001b[0;32m--> 265\u001b[0;31m        decoder_finished) = decoder.step(time, inputs, state)\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracks_own_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mnext_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, time, inputs, state, name)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BasicDecoderStep\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m       \u001b[0mcell_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mcell_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# In graph mode, failure to build the layer's graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;31m# implies a user-side bug. We don't catch exceptions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m   1513\u001b[0m                                       [-1, cell.state_size])\n\u001b[1;32m   1514\u001b[0m           \u001b[0mcur_state_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         \u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1516\u001b[0m         \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;31m# method.  See the class docstring for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[0;32m--> 371\u001b[0;31m                                      *args, **kwargs)\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# In graph mode, failure to build the layer's graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;31m# implies a user-side bug. We don't catch exceptions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;31m# i = input_gate, j = new_input, f = forget_gate, o = output_gate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m     lstm_matrix = math_ops.matmul(\n\u001b[0;32m--> 997\u001b[0;31m         array_ops.concat([inputs, m_prev], 1), self._kernel)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0mlstm_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2453\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2455\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5331\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5332\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5333\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   5334\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5335\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 768 and 1280 for 'decode/decoder/while/BasicDecoderStep/decoder/attention_wrapper/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,768], [1280,2048]."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hKX06qd1BCNV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=tf_graph) as sess:\n",
        "\n",
        "    merged = tf.summary.merge_all()\n",
        "    saver = tf.train.Saver(max_to_keep=30)\n",
        "    \n",
        "    batch_x_val, batch_y_val, batch_x_sequence_length_val, batch_y_sequence_length_val = next(get_batches(x_data_val, y_data_val, batch_size, 2,2))\n",
        " \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch< num_epochs: \n",
        "        epoch_loss = 0\n",
        "        epoch_loss_val =0\n",
        "        epoch_acc_agg =0\n",
        "        epoch_acc_agg_val =0\n",
        "        n_steps=0\n",
        "        \n",
        "        for batch, (batch_x, batch_y, batch_x_sequence_length, batch_y_sequence_length) in enumerate(get_batches(x_data, y_data, batch_size, 2,2)):\n",
        "            \n",
        "            opt = sess.run(optimizer, feed_dict={encoder_x:batch_x,  decoder_x:batch_y,   y_sequence_length: batch_y_sequence_length })\n",
        "            batch_loss  = sess.run([total_cost], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,   y_sequence_length: batch_y_sequence_length })     \n",
        "            logits, logits_max = sess.run([train_logits, train_logits_max], feed_dict={encoder_x: batch_x,  decoder_x: batch_y,   y_sequence_length: batch_y_sequence_length })    \n",
        "            #_logits = np.argmax(logits, axis=2)\n",
        "            \n",
        "            \n",
        "            accuracy = get_accuracy(logits_max, batch_y)\n",
        "            #print(accuracy)\n",
        "            \n",
        "            \n",
        "            \n",
        "            #opt = sess.run(optimizer, feed_dict={encoder_x: x_data_valid, decoder_x: y_data_valid, y: y_one_hot_valid})\n",
        "            batch_loss_val = sess.run([total_cost], feed_dict={encoder_x:  batch_x_val, decoder_x:  batch_y_val, y_sequence_length: batch_y_sequence_length_val })     \n",
        "            logits_val = sess.run([infer_output], feed_dict={encoder_x:  batch_x_val, decoder_x:  batch_y_val, y_sequence_length: batch_y_sequence_length_val })\n",
        "            #print(np.shape(logits_val))\n",
        "            \n",
        "            #print(np.shape(logits_val))\n",
        "            accuracy_val = get_accuracy(logits_val, batch_y_val)\n",
        "            \n",
        "            if batch % 10 == 0:\n",
        "\n",
        "                      save_output_path = save_dir + 'TRAINING_TRANSLATION_OUTPUT_EPOCH_{}_STATE-SIZE_{}_NUM-LAYERS_{}_LEARNING-RATE{}_EMBEDDING-SIZE_{}.txt'.format(epoch, state_size, decoder_layers, lr, inembsize)\n",
        "                      text = generate_text(logits, batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE, y_idx_to_word)\n",
        "                      for _ in range(batch_size):\n",
        "                         print(text[_])\n",
        "    \n",
        "                      print('Epoch {} Batch {}/{} Training Loss: {}, Training Accuracy: {} '.format(epoch+1, batch+1, num_batches, batch_loss[0], accuracy))\n",
        "                      print('Epoch {} Batch {}/{} Validation Loss: {}, Validation Accuracy: {} '.format(epoch+1, batch+1, num_batches, batch_loss_val[0], accuracy_val))\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "            epoch_loss += batch_loss[0]\n",
        "            epoch_loss_val += batch_loss_val[0]\n",
        "            epoch_acc_agg += accuracy\n",
        "            epoch_acc_agg_val += accuracy_val\n",
        "            n_steps+=1\n",
        "            \n",
        "            \n",
        "            \n",
        "        print('Epoch {} Training Loss: {}, Training Accuracy: {} '.format(epoch+1, epoch_loss, epoch_acc_agg/n_steps))\n",
        "        print('Epoch {} Validation Loss: {}, Training Accuracy: {} '.format(epoch+1, epoch_loss_val, epoch_acc_agg_val/n_steps))\n",
        "        \n",
        "        if epoch == 0:\n",
        "            path = saver.save(sess, save_dir, epoch)\n",
        "        else:\n",
        "            path = saver.save(sess, save_dir, epoch, write_meta_graph=False)\n",
        "\n",
        "        print(\"Saved in: %s\" % path)\n",
        "        epoch+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DlWPXjIrjEKm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_path = save_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SErp96PEX8B2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_params(params):\n",
        "    with open('params.p', 'wb') as out_file:\n",
        "        pickle.dump(params, out_file)\n",
        "\n",
        "\n",
        "def load_params():\n",
        "    with open('params.p', mode='rb') as in_file:\n",
        "        return pickle.load(in_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGzmYMGORVPu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jpX4mdSWjDtD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save parameters for checkpoint\n",
        "import pickle\n",
        "save_params(save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5D-ExDDKjkou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "load_path = load_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CyiXP2T3jkyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Transliterate the word\n",
        "def word_to_seq(word):\n",
        "    results = []\n",
        "    for char in word.lower().split(\" \"):\n",
        "        if char in x_word_to_idx:\n",
        "            results.append(x_word_to_idx[char])\n",
        "        else:\n",
        "            results.append(x_word_to_idx['UK'])\n",
        "            \n",
        "    return results\n",
        "\n",
        "translate_word = \"A M E R I C A N\"\n",
        "\n",
        "translate_word = word_to_seq(translate_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7TJDxrO-jk2V",
        "colab_type": "code",
        "outputId": "cf215952-9875-43d9-8f04-c638621454a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "translate_word"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 16, 8, 5, 6, 20, 4, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "Vsk_3xselKPX",
        "colab_type": "code",
        "outputId": "5320b835-2a92-4627-85ea-68e031377b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "cell_type": "code",
      "source": [
        "loaded_graph = tf.Graph()\n",
        "counter = 0\n",
        "\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
        "    loader.restore(sess, load_path)\n",
        "       \n",
        "    encoder_x = loaded_graph.get_tensor_by_name('encoder_x:0')\n",
        "    decoder_x = loaded_graph.get_tensor_by_name('decoder_x:0')\n",
        "    y_sequence_length = loaded_graph.get_tensor_by_name('y_sequence_length:0')\n",
        "    \n",
        "    #target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
        "    #keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "    str_vec = []\n",
        "#     for i in range(len(test[\"ENG\"])):\n",
        "#       str_vec.append(\" \")\n",
        "#     xx = pd.DataFrame({'hin' : str_vec})\n",
        "    \n",
        "#     batch_loss_val = sess.run([total_cost], feed_dict={encoder_x:  batch_x_val, decoder_x:  batch_y_val, y_sequence_length: batch_y_sequence_length_val })     \n",
        "#     logits_val = sess.run([infer_output], feed_dict={encoder_x:  batch_x_val, decoder_x:  batch_y_val, y_sequence_length: batch_y_sequence_length_val })\n",
        "            \n",
        "    \n",
        "    for it, s in enumerate(test['ENG']):\n",
        "      translate_word = word_to_seq(s)\n",
        "      \n",
        "      translate_logits = sess.run(train_logits, {encoder_x: [translate_word]*batch_size,\n",
        "                                         y_sequence_length: [len(translate_word)*2]*batch_size\n",
        "                                         })[0]\n",
        "      translate_logits = \" \".join([y_idx_to_word[i] for i in translate_logits ])\n",
        "      \n",
        "#       xx['hin'][it] = str(translate_logits)\n",
        "#       if it == 0:\n",
        "#         final_trans = np.vstack([translate_logits, translate_logits])\n",
        "#       else:\n",
        "#         final_trans = np.vstack([final_trans, translate_logits])\n",
        "      str_vec.append(translate_logits)    \n",
        "    \n",
        "# print('Input')\n",
        "# print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
        "# print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
        "\n",
        "# print('\\nPrediction')\n",
        "# print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
        "# print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n",
        " "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a4a80baa8c83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaded_graph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Load saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1433\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[1;32m   1434\u001b[0m   return _import_meta_graph_with_return_elements(\n\u001b[0;32m-> 1435\u001b[0;31m       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n\u001b[0m\u001b[1;32m   1436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1446\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    631\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File drive/My Drive/data_PA3/.meta does not exist."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "L6UG4WLmRerd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFMY_AcClKSk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "translate_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "slk3tIazlKLh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "str_vec[1].split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqcMQXZnmzKo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train['HIN'][0].split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "geKRglrGmzIv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "str_vec1 = []\n",
        "for i in str_vec:\n",
        "  str_vec1.append(i.split('<')[0])\n",
        "\n",
        "for i in range(len(str_vec1)):\n",
        "  str_vec1[i] = str_vec1[i][:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0TVXe-nUmzGX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "str_vec1[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LbYO2rysmzDg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loaded_graph = tf_graph#tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
        "    loader.restore(sess, load_path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
        "#     keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "    translate_logits = sess.run(logits, {input_data: [translate_word]*batch_size,\n",
        "                                         target_sequence_length: [len(translate_word)*2]*batch_size,\n",
        "                                         keep_prob: 1.0})[0]\n",
        "\n",
        "print('Input')\n",
        "print('  Word Ids:      {}'.format([i for i in translate_word]))\n",
        "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_word]))\n",
        "\n",
        "print('\\nPrediction')\n",
        "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
        "print('  Hindi Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRpcd9BZmzAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(restore_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jQkSs6cMmy9H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N8EeqiOjE2Rs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tf_graph = tf.Graph()\n",
        "# with tf_graph.as_default():\n",
        "  \n",
        "#   encoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) #[batch_size, X_MAX_LENGTH]\n",
        "#   decoder_x = tf.placeholder(dtype=tf.int32, shape=[None, None]) #[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "#   y = tf.placeholder(dtype=tf.float32, shape=[None, None, None])#[batch_size, Y_MAX_LENGTH, Y_VOCAB_SIZE]\n",
        "  \n",
        "#   with tf.variable_scope('encoder_word_embeddings'):\n",
        "\n",
        "#       word_embeddings = tf.get_variable('encoder_word_embeddings', [X_VOCAB_SIZE, inembsize])\n",
        "#       encoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, encoder_x)\n",
        "#       encoder_embedded_word_ids = tf.reshape(encoder_embedded_word_ids, [-1, X_MAX_LENGTH, inembsize])\n",
        "\n",
        "#   with tf.variable_scope('encoder'):\n",
        "\n",
        "#       for n in range(encoder_layers): \n",
        "#                 encoder_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "#                 encoder_out, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
        "#                 cell_fw = encoder_cell,  ## initializer, reuse to be set\n",
        "#                 cell_bw = encoder_cell,  ## initializer, reuse to be set\n",
        "#                 inputs = encoder_embedded_word_ids,\n",
        "#                 dtype = tf.float32)\n",
        "\n",
        "#   #           (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
        "#   #               cell_fw = tf.nn.rnn_cell.LSTMCell(state_size//2, state_is_tuple=True),  ## initializer, reuse to be set\n",
        "#   #               cell_bw = tf.nn.rnn_cell.LSTMCell(state_size//2, state_is_tuple=True),  ## initializer, reuse to be set\n",
        "#   #               inputs = encoder_embedded_word_ids,\n",
        "\n",
        "#   #               sequence_length =  [61]* batch_size,\n",
        "#   #               dtype = tf.float32)\n",
        "\n",
        "#   #           encoder_embedded_word_ids = tf.concat((out_fw, out_bw), 2)\n",
        "\n",
        "#   #           bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n",
        "#   #           bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n",
        "#   #           bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
        "#   #           encoder_state = tuple([bi_lstm_state] * encoder_layers)\n",
        "\n",
        "#   #           encoder_state = tuple(encoder_state[-1] for _ in range(encoder_layers))\n",
        "\n",
        "\n",
        "#   with tf.variable_scope('decoder_word_embeddings'):\n",
        "\n",
        "#       word_embeddings = tf.get_variable('decoder_word_embeddings', [Y_VOCAB_SIZE, outembsize])\n",
        "#       decoder_embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, decoder_x)\n",
        "#       decoder_embedded_word_ids = tf.reshape(decoder_embedded_word_ids,  [-1, Y_MAX_LENGTH,  outembsize])\n",
        "\n",
        "\n",
        "\n",
        "#   with tf.variable_scope('decoder'):\n",
        "\n",
        "#       decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(state_size) for _ in range(decoder_layers)])\n",
        "#       dense_layer = tf.layers.Dense(Y_VOCAB_SIZE)\n",
        "\n",
        "#       #decoder for training data\n",
        "#       training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embedded_word_ids, sequence_length = [62]* batch_size)\n",
        "\n",
        "\n",
        "\n",
        "#       training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "#               cell = decoder_cell,\n",
        "#               helper = training_helper,\n",
        "#               initial_state = encoder_state,\n",
        "#               output_layer = dense_layer)\n",
        "\n",
        "#       training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "#               decoder = training_decoder,\n",
        "#               impute_finished = True,   ## impute finished make sure that weights causing correctly predicted words are not changed further\n",
        "#               maximum_iterations = Y_MAX_LENGTH)   \n",
        "      \n",
        "#       training_logits = training_decoder_output.rnn_output\n",
        "#       softmax_out = tf.nn.softmax(training_logits)\n",
        "      \n",
        "#       cost =  tf.nn.softmax_cross_entropy_with_logits(logits=training_logits, labels=y)\n",
        "\n",
        "#       total_cost = tf.reduce_mean(cost)\n",
        "#       optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost)\n",
        "\n",
        "#       y_t = tf.argmax(softmax_out,axis=2)\n",
        "#       y_t = tf.cast(y_t, tf.int32)\n",
        "\n",
        "#       #prediction = tf.boolean_mask(y_t, masks)\n",
        "#       prediction = y_t\n",
        "#       #mask_label = tf.boolean_mask(decoder_x, masks)\n",
        "#       mask_label = decoder_x\n",
        "\n",
        "#       correct_pred = tf.equal(prediction, mask_label)\n",
        "#       accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "           \n",
        "#   with tf.variable_scope('decoder', reuse=True):   \n",
        "#       #decoder for test data\n",
        "#       predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding = word_embeddings,\n",
        "#               start_tokens = tf.fill([batch_size], y_word_to_idx['GO']), end_token = y_word_to_idx['EOS'])\n",
        "                           \n",
        "#       predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "#               cell = decoder_cell,\n",
        "#               helper = predicting_helper,\n",
        "#               initial_state = encoder_state,\n",
        "#               output_layer = dense_layer)\n",
        "\n",
        "#       predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "#               decoder = predicting_decoder,\n",
        "#               impute_finished = True,\n",
        "#               maximum_iterations = 2 * (X_MAX_LENGTH))\n",
        "\n",
        "#       predicting_ids = predicting_decoder_output.sample_id\n",
        "      \n",
        "#       #predicting_ids = tf.cast(predicting_ids, tf.float32)\n",
        "#       #softmax_out_val = tf.nn.softmax(predicting_ids)\n",
        "# #       print(predicting_ids)\n",
        "      \n",
        "# #       cost_val =  tf.nn.softmax_cross_entropy_with_logits(logits=predicting_ids, labels=y)\n",
        "\n",
        "# #       total_cost_val = tf.reduce_mean(cost_val)\n",
        "#       #optimizer = tf.train.AdamOptimizer(lr).minimize(total_cost_val)\n",
        "      \n",
        "#       #y_t_val = tf.argmax(predicting_ids,axis=2)\n",
        "#       #y_t_val = tf.cast(y_t_val, tf.int32)\n",
        "\n",
        "#       #prediction = tf.boolean_mask(y_t, masks)\n",
        "# #       prediction_val = predicting_ids\n",
        "      \n",
        "# #       prediction_val = tf.cast(prediction_val, tf.int32)\n",
        "# #       #mask_label = tf.boolean_mask(decoder_x, masks)\n",
        "      \n",
        "# #       mask_label_val = decoder_x\n",
        "\n",
        "# #       correct_pred_val= tf.equal(prediction_val, mask_label_val)\n",
        "# #       accuracy_val = tf.reduce_mean(tf.cast(correct_pred_val, tf.float32))\n",
        "      \n",
        "      \n",
        "#       #masks = tf.sequence_mask(tf.count_nonzero(decoder_x, 1, dtype=tf.int32), Y_MAX_LENGTH, dtype=tf.float32)  \n",
        "\n",
        "#       #cost = tf.contrib.seq2seq.sequence_loss(logits = softmax_out,targets = decoder_x)\n",
        "\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJenXzc7e901",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}